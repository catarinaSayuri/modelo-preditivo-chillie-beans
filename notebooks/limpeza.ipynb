{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e870a0a1",
   "metadata": {},
   "source": [
    "# üìä Explora√ß√£o e Limpeza dos Dados\n",
    "Este notebook tem como prop√≥sito realizar a **explora√ß√£o inicial, pr√©-processamento e transforma√ß√£o dos dados** de nosso conjunto, garantindo sua qualidade para an√°lises futuras.  \n",
    "As etapas documentadas aqui incluem:\n",
    "\n",
    "- Explora√ß√£o das vari√°veis (estat√≠stica descritiva, gr√°ficos e identifica√ß√£o de colunas num√©ricas e categ√≥ricas);  \n",
    "- Tratamento de *missing values* e identifica√ß√£o/corre√ß√£o de *outliers*;  \n",
    "- Transforma√ß√µes (codifica√ß√£o de vari√°veis categ√≥ricas e normaliza√ß√£o de vari√°veis num√©ricas);  \n",
    "- Registro de decis√µes que ser√£o detalhadas na **Se√ß√£o 4.2 da documenta√ß√£o**.  \n",
    "\n",
    "Este material serve como evid√™ncia pr√°tica e complementa a documenta√ß√£o te√≥rica escrita na se√ß√£o 4.2 (*Compreens√£o dos Dados*) da Documenta√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a4155",
   "metadata": {},
   "source": [
    "## 0. Prepara√ß√£o do Ambiente\n",
    "\n",
    "- Instala√ß√£o e Importa√ß√£o das bibliotecas\n",
    "- Carregamento da Nova Base de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verifica√ß√£o das vers√µes das bibliotecas\n",
    "print(\"=== VERIFICA√á√ÉO DAS BIBLIOTECAS ===\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "\n",
    "# Carregamento da Base de Dados com padr√£o de nomes\n",
    "print(\"\\n=== CARREGAMENTO DOS DADOS ===\")\n",
    "df_raw = pd.read_csv('../database/dataset inicial/nova_base_vendas_chillie.csv')\n",
    "print(f\"df_raw shape: {df_raw.shape}\")\n",
    "\n",
    "# C√≥pia para trabalho tempor√°rio\n",
    "df = df_raw.copy()\n",
    "print(f\"df shape: {df.shape}\")\n",
    "print(\"‚úÖ Dados carregados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218c82b",
   "metadata": {},
   "source": [
    "## 1. Explora√ß√£o de Dados\n",
    "\n",
    "Nesta se√ß√£o, realizaremos uma an√°lise inicial do conjunto de dados para compreender suas caracter√≠sticas principais. As etapas incluem:\n",
    "\n",
    "- **Identifica√ß√£o de colunas num√©ricas e categ√≥ricas:** Classifica√ß√£o das vari√°veis para facilitar o tratamento e an√°lise posterior.\n",
    "- **Estat√≠sticas descritivas:** C√°lculo de m√©tricas como m√©dia, mediana, desvio padr√£o, valores m√≠nimos e m√°ximos para vari√°veis num√©ricas.\n",
    "Essas an√°lises iniciais s√£o fundamentais para orientar as etapas de pr√©-processamento e modelagem subsequentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95586c2",
   "metadata": {},
   "source": [
    "### 1.1 Identifica√ß√£o de Colunas (Num√©ricas e Categ√≥ricas)\n",
    "\n",
    "Nesta etapa, as colunas do dataset foram classificadas em diferentes categorias, como num√©ricas, categ√≥ricas, IDs e datas. Essa organiza√ß√£o facilita o tratamento e a an√°lise dos dados, permitindo aplicar estrat√©gias espec√≠ficas para cada tipo de vari√°vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82385d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica√ß√£o e Padroniza√ß√£o das Vari√°veis\n",
    "\n",
    "# Colunas de identifica√ß√£o e CEPs (n√£o participam de an√°lises estat√≠sticas)\n",
    "ID_COLUMNS = [\n",
    "    'ID_Loja', 'ID_Cliente', 'ID_Produto', 'ID_Deposito', \n",
    "    'Num_Vale', 'ID_SAP', 'Dim_Cliente.Cep_Cliente', 'Dim_Lojas.Cep_Emp'\n",
    "]\n",
    "\n",
    "# Colunas num√©ricas relacionadas a pre√ßos e valores monet√°rios\n",
    "NUMERIC_PRICE_COLUMNS = ['Preco_Custo', 'Valor_Total', 'Preco_Varejo', 'Desconto', \n",
    "                         'DESCONTO_CALCULADO', 'Total_Preco_Varejo', 'Total_Preco_Liquido']\n",
    "\n",
    "# Outras colunas num√©ricas (quantidades, m√©tricas, etc.)\n",
    "NUMERIC_OTHER_COLUMNS = ['Quantidade']\n",
    "\n",
    "# Colunas categ√≥ricas (texto/string que representam categorias)\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'Natureza_Operacao', 'Nome_Tabela_Preco', 'Dim_Cliente.Bairro_Cliente',\n",
    "    'Dim_Cliente.Cidade_cliente', 'Dim_Cliente.Uf_Cliente', 'Dim_Cliente.Pais',\n",
    "    'Dim_Cliente.Sexo', 'Dim_Cliente.Estado_Civil', 'Dim_Cliente.Regiao_Cliente',\n",
    "    'Dim_Lojas.Nome_Emp', 'Dim_Lojas.Bairro_Emp', 'Dim_Lojas.Cidade_Emp',\n",
    "    'Dim_Lojas.Estado_Emp', 'Dim_Lojas.Regiao', 'Dim_Lojas.Tipo_PDV',\n",
    "    'Dim_Lojas.CANAL_VENDA', 'Dim_Lojas.SAP_NOME', 'Dim_Lojas.REGIAO_CHILLI',\n",
    "    'Dim_Produtos.NomeDim_Produtos.Nome', 'Dim_Produtos.Grupo_Produto',\n",
    "    'Dim_Produtos.Sub_Grupo', 'Dim_Produtos.Cor1', 'Dim_Produtos.Cor2',\n",
    "    'Dim_Produtos.Material1', 'Dim_Produtos.Material2', 'Dim_Produtos.Segmentacao',\n",
    "    'Dim_Produtos.Shape', 'Dim_Produtos.Formato', 'Dim_Produtos.Sexo',\n",
    "    'Dim_Produtos.Griffe', 'Dim_Produtos.GRUPO_CHILLI'\n",
    "]\n",
    "\n",
    "# Colunas de data (para tratamento futuro espec√≠fico)\n",
    "DATE_COLUMNS = ['Dim_Cliente.Data_Cadastro', 'Dim_Cliente.Data_Nascimento']\n",
    "\n",
    "print(\"‚úÖ VARI√ÅVEIS DE TIPO PADRONIZADAS DEFINIDAS\")\n",
    "print(f\"üìã IDs/CEPs: {len(ID_COLUMNS)} colunas\")\n",
    "print(f\"üí∞ Pre√ßos/Valores: {len(NUMERIC_PRICE_COLUMNS)} colunas\") \n",
    "print(f\"üî¢ Num√©ricas outras: {len(NUMERIC_OTHER_COLUMNS)} colunas\")\n",
    "print(f\"üìù Categ√≥ricas: {len(CATEGORICAL_COLUMNS)} colunas\")\n",
    "print(f\"üìÖ Datas: {len(DATE_COLUMNS)} colunas\")\n",
    "print(f\"üéØ Total classificado: {len(ID_COLUMNS + NUMERIC_PRICE_COLUMNS + NUMERIC_OTHER_COLUMNS + CATEGORICAL_COLUMNS + DATE_COLUMNS)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15603e9",
   "metadata": {},
   "source": [
    "### 1.2 Estat√≠sticas B√°sicas\n",
    "\n",
    "Nesta se√ß√£o, s√£o realizadas an√°lises descritivas das vari√°veis do conjunto de dados. As estat√≠sticas b√°sicas incluem:\n",
    "\n",
    "- **Estat√≠sticas descritivas para vari√°veis num√©ricas:** c√°lculo de m√©tricas como m√©dia, mediana, desvio padr√£o, valores m√≠nimos e m√°ximos, al√©m de percentis. Essas informa√ß√µes ajudam a compreender a distribui√ß√£o e a variabilidade dos dados.\n",
    "- **Estat√≠sticas descritivas para vari√°veis categ√≥ricas:** identifica√ß√£o de valores √∫nicos, moda e frequ√™ncia de categorias, permitindo uma vis√£o geral das caracter√≠sticas qualitativas do dataset.\n",
    "- **Identifica√ß√£o de outliers:** an√°lise de valores extremos nas vari√°veis num√©ricas utilizando o m√©todo do IQR (Intervalo Interquart√≠lico), com c√°lculo de limites inferior e superior, al√©m da propor√ß√£o de outliers por vari√°vel.\n",
    "\n",
    "Essas an√°lises s√£o fundamentais para identificar padr√µes, inconsist√™ncias e poss√≠veis problemas nos dados, orientando as etapas subsequentes de limpeza e transforma√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caafc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estat√≠sticas Descritivas b√°sicas das colunas num√©ricas\n",
    "display(df.describe())\n",
    "\n",
    "# Estat√≠sticas Descritivas b√°sicas das colunas categ√≥ricas\n",
    "display(df.describe(include=['object', 'category']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51657147",
   "metadata": {},
   "source": [
    "## 2. Pr√©-Processamento dos Dados\n",
    "\n",
    "O pr√©-processamento √© uma etapa essencial para garantir a qualidade dos dados antes da aplica√ß√£o de modelos de aprendizado de m√°quina ou an√°lises mais profundas.  \n",
    "Nesta se√ß√£o, realizamos a√ß√µes que permitem tornar a base mais consistente, como a remo√ß√£o de conulas irrelevantes para a an√°lise, o tratamento de valores ausentes, a remo√ß√£o ou corre√ß√£o de outliers, a normaliza√ß√£o e a codifica√ß√£o de vari√°veis.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d5df1",
   "metadata": {},
   "source": [
    "#### 2.1 Exclus√£o de Colunas N√£o Relevantes\n",
    "\n",
    "Nesta etapa, removemos colunas que n√£o ser√£o utilizadas em nossas an√°lises ou transforma√ß√µes futuras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Exclus√£o de Colunas Irrelevantes\n",
    "print(\"=== INFORMA√á√ïES SOBRE VALORES NULOS (ANTES DA EXCLUS√ÉO) ===\")\n",
    "print(f\"Total de valores nulos: {df.isnull().sum().sum()}\")\n",
    "print(f\"Shape atual: {df.shape}\")\n",
    "\n",
    "# Colunas para serem exclu√≠das\n",
    "colunas_para_excluir = [\n",
    "    'Frete',\n",
    "    'Dim_Lojas.Data_Criacao_Emp',\n",
    "    'ID_Faturamento',\n",
    "    'ID_Vendedor',\n",
    "    'ID_Date',\n",
    "    'Documento',\n",
    "    'DOC_UNICO',\n",
    "    'Transacao',\n",
    "    'Dt_update',\n",
    "    'Dim_Cliente.Ativo',\n",
    "    'Dim_Lojas.Cod_Franqueado',\n",
    "    'Dim_Produtos.Cod_Auxiliar',\n",
    "    'Dim_Produtos.Referencia',\n",
    "    'Quantidade',\n",
    "    'Preco_Varejo',\n",
    "    'Num_Vale',\n",
    "    'DESCONTO_CALCULADO',\n",
    "    'ID_Deposito',\n",
    "    'Dim_Cliente.Sexo',\n",
    "    'Dim_Cliente.Data_Cadastro',\n",
    "    'Dim_Cliente.Estado_Civil',\n",
    "    'Dim_Produtos.Sub_Grupo',\n",
    "    'Dim_Produtos.Cor1',\n",
    "    'Dim_Produtos.Cor2',\n",
    "    'Dim_Produtos.Material1',\n",
    "    'Dim_Produtos.Material2',\n",
    "    'Dim_Produtos.Segmentacao',\n",
    "    'Dim_Produtos.Shape',\n",
    "    'Dim_Produtos.Formato',\n",
    "    'Dim_Produtos.Sexo',\n",
    "    'Dim_Produtos.Griffe',\n",
    "    'Natureza_Operacao',\n",
    "    'Preco_Custo',\n",
    "    'Valor_Total',\n",
    "    'Nome_Tabela_Preco',\n",
    "    'Dim_Cliente.Bairro_Cliente',\n",
    "    'Dim_Cliente.Cep_Cliente',\n",
    "    'Dim_Cliente.Cidade_cliente',\n",
    "    'Dim_Cliente.Uf_Cliente',\n",
    "    'Dim_Cliente.Pais',\n",
    "    'Dim_Lojas.ID_SAP',\n",
    "    'Dim_Lojas.SAP_NOME',\n",
    "    'Dim_Lojas.Cep_Emp',\n",
    "    'Dim_Produtos.NomeDim_Produtos.Nome'\n",
    "\n",
    "]\n",
    "\n",
    "# Verificar quais colunas existem antes de excluir\n",
    "colunas_existentes_para_excluir = [col for col in colunas_para_excluir if col in df.columns]\n",
    "colunas_inexistentes_para_excluir = [col for col in colunas_para_excluir if col not in df.columns]\n",
    "\n",
    "print(f\"\\nColunas encontradas para exclus√£o: {len(colunas_existentes_para_excluir)}\")\n",
    "print(f\"Colunas que ser√£o exclu√≠das: {colunas_existentes_para_excluir}\")\n",
    "\n",
    "if colunas_inexistentes_para_excluir:\n",
    "    print(f\"Colunas n√£o encontradas (n√£o ser√£o exclu√≠das): {colunas_inexistentes_para_excluir}\")\n",
    "\n",
    "# Excluir as colunas que existem\n",
    "if colunas_existentes_para_excluir:\n",
    "    df = df.drop(columns=colunas_existentes_para_excluir)\n",
    "    print(f\"\\n‚úÖ {len(colunas_existentes_para_excluir)} colunas foram exclu√≠das com sucesso!\")\n",
    "\n",
    "print(f\"\\n=== RESULTADO DA EXCLUS√ÉO ===\")\n",
    "print(f\"Shape ap√≥s exclus√£o: {df.shape}\")\n",
    "print(f\"Progress√£o: df_raw ({df_raw.shape}) -> df atual ({df.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee4c3f",
   "metadata": {},
   "source": [
    "### 2.2 Tratamento de Valores Ausentes  \n",
    "\n",
    "Nesta etapa foram identificados e tratados os valores ausentes presentes no conjunto de dados.  \n",
    "O processo teve como objetivo garantir a consist√™ncia da base e evitar distor√ß√µes nas an√°lises subsequentes.  \n",
    "\n",
    "As a√ß√µes realizadas inclu√≠ram:  \n",
    "\n",
    "- **Identifica√ß√£o de vari√°veis com dados faltantes;**  \n",
    "- **Avalia√ß√£o da propor√ß√£o de valores ausentes por coluna;**  \n",
    "- **Defini√ß√£o de estrat√©gias de tratamento,** que podem incluir remo√ß√£o de registros incompletos ou imputa√ß√£o de valores com base em medidas estat√≠sticas (m√©dia, mediana, moda) ou em crit√©rios definidos pelo contexto da an√°lise.  \n",
    "\n",
    "O resultado √© uma base de dados mais √≠ntegra, reduzindo impactos negativos em processos de transforma√ß√£o, detec√ß√£o de outliers e modelagem futura.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Tratamento de Valores Ausentes\n",
    "\n",
    "print(\"=== TRATAMENTO DE VALORES AUSENTES ===\")\n",
    "print(f\"Valores nulos antes do tratamento: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Identificar colunas categ√≥ricas (object ou category) e num√©ricas\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Preencher colunas categ√≥ricas com a moda (valor mais frequente)\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        moda = df[col].mode(dropna=True)\n",
    "        if not moda.empty:\n",
    "            df[col] = df[col].fillna(moda[0])\n",
    "\n",
    "# Preencher colunas num√©ricas com a mediana\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mediana = df[col].median()\n",
    "        df[col] = df[col].fillna(mediana)\n",
    "\n",
    "# Criar df_clean ap√≥s remo√ß√£o de colunas e imputa√ß√£o completa\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"\\n=== RESULTADO DO TRATAMENTO ===\")\n",
    "print(f\"Valores nulos ap√≥s tratamento: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"‚úÖ df_clean criado com sucesso!\")\n",
    "print(f\"df_clean shape: {df_clean.shape}\")\n",
    "print(f\"Progress√£o: df_raw ({df_raw.shape}) -> df_clean ({df_clean.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da32f97",
   "metadata": {},
   "source": [
    "### 2.2.1 Filtros de Limpeza Avan√ßada\n",
    "\n",
    "Nesta etapa, aplicamos filtros espec√≠ficos para limpar inconsist√™ncias nos dados:\n",
    "\n",
    "- **Corre√ß√£o de idades inv√°lidas:** Identifica√ß√£o e corre√ß√£o de datas de nascimento que resultem em idades menores que 18 anos ou maiores que 100 anos, substituindo-as pela idade m√©dia dos clientes v√°lidos.\n",
    "- **Remo√ß√£o de registros com pre√ßos inv√°lidos:** Exclus√£o de registros onde `Preco_Varejo` ou `Total_Preco_Varejo` sejam menores que 1, indicando dados inconsistentes.\n",
    "- Remo√ß√£o de registros que contenham \"RELOGIO\" na coluna \"Dim_Produtos.GRUPO_CHILLI\", indicando que produtos como rel√≥gios n√£o s√£o relevantes para a an√°lise.\n",
    "\n",
    "Estes filtros garantem maior qualidade e consist√™ncia dos dados para an√°lises posteriores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07026f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(\"=== FILTROS DE LIMPEZA AVAN√áADA ===\")\n",
    "print(f\"Shape antes dos filtros: {df_clean.shape}\")\n",
    "\n",
    "# 1. Corre√ß√£o de Idades Inv√°lidas\n",
    "print(\"\\n--- CORRE√á√ÉO DE IDADES INV√ÅLIDAS ---\")\n",
    "\n",
    "# Verificar se a coluna de data de nascimento existe\n",
    "if 'Dim_Cliente.Data_Nascimento' in df_clean.columns:\n",
    "    # Converter para datetime\n",
    "    df_clean['Dim_Cliente.Data_Nascimento'] = pd.to_datetime(df_clean['Dim_Cliente.Data_Nascimento'], errors='coerce')\n",
    "    \n",
    "    # Calcular idade atual\n",
    "    hoje = datetime.datetime.now()\n",
    "    df_clean['idade_calculada'] = (hoje - df_clean['Dim_Cliente.Data_Nascimento']).dt.days / 365.25\n",
    "    \n",
    "    # Identificar idades inv√°lidas (< 18 ou > 100)\n",
    "    idade_invalida = (df_clean['idade_calculada'] < 18) | (df_clean['idade_calculada'] > 100) | df_clean['idade_calculada'].isna()\n",
    "    n_idades_invalidas = idade_invalida.sum()\n",
    "    \n",
    "    print(f\"Idades inv√°lidas encontradas: {n_idades_invalidas}\")\n",
    "    \n",
    "    if n_idades_invalidas > 0:\n",
    "        # Calcular idade m√©dia dos clientes v√°lidos\n",
    "        idades_validas = df_clean[~idade_invalida]['idade_calculada']\n",
    "        idade_media = idades_validas.mean()\n",
    "        \n",
    "        # Calcular data de nascimento correspondente √† idade m√©dia\n",
    "        data_nascimento_media = hoje - datetime.timedelta(days=idade_media * 365.25)\n",
    "        \n",
    "        # Substituir datas inv√°lidas pela data m√©dia\n",
    "        df_clean.loc[idade_invalida, 'Dim_Cliente.Data_Nascimento'] = data_nascimento_media\n",
    "        \n",
    "        print(f\"‚úÖ {n_idades_invalidas} datas de nascimento corrigidas com idade m√©dia de {idade_media:.1f} anos\")\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhuma idade inv√°lida encontrada\")\n",
    "    \n",
    "    # Remover coluna tempor√°ria\n",
    "    df_clean = df_clean.drop('idade_calculada', axis=1)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Coluna 'Dim_Cliente.Data_Nascimento' n√£o encontrada\")\n",
    "\n",
    "# 2. Remo√ß√£o de Registros com Pre√ßos Inv√°lidos\n",
    "print(\"\\n--- REMO√á√ÉO DE PRE√áOS INV√ÅLIDOS ---\")\n",
    "\n",
    "colunas_preco = ['Preco_Varejo', 'Total_Preco_Varejo']\n",
    "colunas_preco_existentes = [col for col in colunas_preco if col in df_clean.columns]\n",
    "\n",
    "if colunas_preco_existentes:\n",
    "    # Identificar registros com pre√ßos baixos (< 1)\n",
    "    mask_preco_baixo = pd.Series(False, index=df_clean.index)\n",
    "    \n",
    "    for col in colunas_preco_existentes:\n",
    "        precos_baixos = (df_clean[col] < 1) | df_clean[col].isna()\n",
    "        n_precos_baixos = precos_baixos.sum()\n",
    "        print(f\"Registros com {col} < 1: {n_precos_baixos}\")\n",
    "        mask_preco_baixo = mask_preco_baixo | precos_baixos\n",
    "    \n",
    "    # Aplicar filtro - manter apenas registros com pre√ßos v√°lidos\n",
    "    df_clean = df_clean[~mask_preco_baixo].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ {mask_preco_baixo.sum()} registros removidos por pre√ßos inv√°lidos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Colunas de pre√ßo n√£o encontradas\")\n",
    "\n",
    "# Remo√ß√£o de registros que contenham \"RELOGIO e BAG\" na coluna \"Dim_Produtos.GRUPO_CHILLI\"\n",
    "if 'Dim_Produtos.GRUPO_CHILLI' in df_clean.columns:\n",
    "    mask_relogio = df_clean['Dim_Produtos.GRUPO_CHILLI'].astype(str).str.upper().str.contains('RELOGIO|BAG', na=False)\n",
    "    n_relogio = mask_relogio.sum()\n",
    "    df_clean = df_clean[~mask_relogio].reset_index(drop=True)\n",
    "    print(f\"‚úÖ {n_relogio} registros removidos contendo 'RELOGIO e BAG' em 'Dim_Produtos.GRUPO_CHILLI'\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Coluna 'Dim_Produtos.GRUPO_CHILLI' n√£o encontrada\")\n",
    "\n",
    "# Verificar duplicatas\n",
    "print(\"\\n--- VERIFICA√á√ÉO DE DUPLICATAS ---\")\n",
    "n_duplicatas = df_clean.duplicated().sum()\n",
    "if n_duplicatas > 0:\n",
    "    df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"‚úÖ {n_duplicatas} registros duplicados removidos\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhuma duplicata encontrada\")\n",
    "\n",
    "print(f\"\\n=== RESULTADO DOS FILTROS ===\")\n",
    "print(f\"Shape ap√≥s filtros: {df_clean.shape}\")\n",
    "print(f\"Registros removidos: {df.shape[0] - df_clean.shape[0]}\")\n",
    "print(f\"Progress√£o: df original ({df.shape}) -> df_clean filtrado ({df_clean.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e8db9",
   "metadata": {},
   "source": [
    "### 2.2.2 Identifica√ß√£o e Tratamento de Outliers (IQR)\n",
    "\n",
    "Nesta etapa, aplicamos o m√©todo do **IQR (Intervalo Interquartil)** para identificar e tratar outliers nas vari√°veis num√©ricas.\n",
    "\n",
    "O processo consiste em:\n",
    "- Calcular os quartis Q1 (25%) e Q3 (75%) para cada vari√°vel num√©rica\n",
    "- Definir os limites inferior e superior usando a f√≥rmula: Q1 - 1.5*IQR e Q3 + 1.5*IQR\n",
    "- Identificar valores que est√£o fora destes limites como outliers\n",
    "- Aplicar tratamento adequado (remo√ß√£o ou winsoriza√ß√£o) conforme a natureza da vari√°vel\n",
    "\n",
    "**Importante:** Vari√°veis de identifica√ß√£o (IDs) n√£o s√£o consideradas nesta an√°lise, pois n√£o possuem significado num√©rico interpret√°vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2 Identifica√ß√£o e Tratamento de Outliers (IQR)\n",
    "\n",
    "print(\"=== IDENTIFICA√á√ÉO E TRATAMENTO DE OUTLIERS (IQR) ===\")\n",
    "print(f\"Shape antes do tratamento de outliers: {df_clean.shape}\")\n",
    "\n",
    "# Usar apenas vari√°veis num√©ricas relevantes (excluindo IDs)\n",
    "numeric_cols_for_outliers = [col for col in NUMERIC_PRICE_COLUMNS + NUMERIC_OTHER_COLUMNS if col in df_clean.columns]\n",
    "print(f\"\\nVari√°veis num√©ricas para an√°lise de outliers: {len(numeric_cols_for_outliers)}\")\n",
    "print(f\"Colunas: {numeric_cols_for_outliers}\")\n",
    "\n",
    "# Fun√ß√£o para calcular limites IQR\n",
    "def calculate_iqr_bounds(series):\n",
    "    \"\"\"Calcula os limites inferior e superior usando o m√©todo IQR\"\"\"\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# An√°lise de outliers por vari√°vel\n",
    "outlier_analysis = []\n",
    "total_outliers_removed = 0\n",
    "\n",
    "print(f\"\\n--- AN√ÅLISE DE OUTLIERS POR VARI√ÅVEL ---\")\n",
    "\n",
    "for col in numeric_cols_for_outliers:\n",
    "    # Obter valores v√°lidos (n√£o nulos)\n",
    "    valid_values = df_clean[col].dropna()\n",
    "    \n",
    "    if len(valid_values) > 0:\n",
    "        # Calcular limites IQR\n",
    "        lower_bound, upper_bound = calculate_iqr_bounds(valid_values)\n",
    "        \n",
    "        # Identificar outliers\n",
    "        outlier_mask = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "        n_outliers = outlier_mask.sum()\n",
    "        perc_outliers = (n_outliers / len(df_clean)) * 100\n",
    "        \n",
    "        # Armazenar informa√ß√µes\n",
    "        outlier_info = {\n",
    "            'variavel': col,\n",
    "            'n_outliers': n_outliers,\n",
    "            'perc_outliers': round(perc_outliers, 2),\n",
    "            'limite_inferior': round(lower_bound, 2),\n",
    "            'limite_superior': round(upper_bound, 2),\n",
    "            'outlier_mask': outlier_mask\n",
    "        }\n",
    "        outlier_analysis.append(outlier_info)\n",
    "        \n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Outliers encontrados: {n_outliers} ({perc_outliers:.2f}%)\")\n",
    "        print(f\"  Limites: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "# Criar DataFrame com resumo dos outliers\n",
    "outlier_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Vari√°vel': info['variavel'],\n",
    "        'N¬∞ Outliers': info['n_outliers'],\n",
    "        '% Outliers': info['perc_outliers'],\n",
    "        'Limite Inferior': info['limite_inferior'],\n",
    "        'Limite Superior': info['limite_superior']\n",
    "    }\n",
    "    for info in outlier_analysis\n",
    "])\n",
    "\n",
    "print(f\"\\n--- RESUMO DE OUTLIERS ---\")\n",
    "outliers_found = outlier_summary[outlier_summary['N¬∞ Outliers'] > 0].sort_values('N¬∞ Outliers', ascending=False)\n",
    "\n",
    "if len(outliers_found) > 0:\n",
    "    print(f\"Vari√°veis com outliers: {len(outliers_found)}\")\n",
    "    display(outliers_found)\n",
    "    \n",
    "    # Tratamento de outliers - Remo√ß√£o de registros com outliers extremos\n",
    "    print(f\"\\n--- TRATAMENTO DE OUTLIERS ---\")\n",
    "    \n",
    "    # Identificar registros que s√£o outliers em m√∫ltiplas vari√°veis (mais propensos a serem erros)\n",
    "    combined_outlier_mask = pd.Series(False, index=df_clean.index)\n",
    "    \n",
    "    # Considerar para remo√ß√£o apenas outliers em vari√°veis de pre√ßo/valor\n",
    "    price_outlier_vars = [info for info in outlier_analysis \n",
    "                         if info['variavel'] in NUMERIC_PRICE_COLUMNS and info['n_outliers'] > 0]\n",
    "    \n",
    "    # Aplicar remo√ß√£o conservadora - apenas casos extremos\n",
    "    for info in price_outlier_vars:\n",
    "        # Remover apenas outliers extremos (al√©m de 3*IQR)\n",
    "        col = info['variavel']\n",
    "        series = df_clean[col].dropna()\n",
    "        q1, q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        extreme_lower = q1 - 3 * iqr\n",
    "        extreme_upper = q3 + 3 * iqr\n",
    "        \n",
    "        extreme_outliers = (df_clean[col] < extreme_lower) | (df_clean[col] > extreme_upper)\n",
    "        combined_outlier_mask = combined_outlier_mask | extreme_outliers\n",
    "        \n",
    "        if extreme_outliers.sum() > 0:\n",
    "            print(f\"{col}: {extreme_outliers.sum()} outliers extremos identificados para remo√ß√£o\")\n",
    "    \n",
    "    # Aplicar remo√ß√£o\n",
    "    if combined_outlier_mask.sum() > 0:\n",
    "        df_clean = df_clean[~combined_outlier_mask].reset_index(drop=True)\n",
    "        total_outliers_removed = combined_outlier_mask.sum()\n",
    "        print(f\"\\n‚úÖ {total_outliers_removed} registros com outliers extremos removidos\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Nenhum outlier extremo identificado para remo√ß√£o\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚úÖ Nenhum outlier detectado nas vari√°veis num√©ricas!\")\n",
    "\n",
    "print(f\"\\n=== RESULTADO DO TRATAMENTO DE OUTLIERS ===\")\n",
    "print(f\"Registros removidos por outliers extremos: {total_outliers_removed}\")\n",
    "print(f\"Shape final ap√≥s tratamento: {df_clean.shape}\")\n",
    "print(f\"Progress√£o total de limpeza: {df_raw.shape[0] - df_clean.shape[0]} registros removidos ({((df_raw.shape[0] - df_clean.shape[0]) / df_raw.shape[0] * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331e313",
   "metadata": {},
   "source": [
    "### 2.3 Transforma√ß√µes das Vari√°veis\n",
    "\n",
    "Ap√≥s a limpeza inicial da base de dados, √© necess√°rio aplicar transforma√ß√µes nas vari√°veis para padronizar seus formatos e possibilitar an√°lises e modelagens mais consistentes.  \n",
    "Nesta se√ß√£o, foram realizados os seguintes procedimentos: normaliza√ß√£o de vari√°veis num√©ricas e codifica√ß√£o de vari√°veis categ√≥ricas.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== USANDO VARI√ÅVEIS DE TIPO DEFINIDAS NO IN√çCIO ===\")\n",
    "print(f\"IDs/CEPs: {len(ID_COLUMNS)} colunas\")\n",
    "print(f\"Pre√ßos/Valores: {len(NUMERIC_PRICE_COLUMNS)} colunas\") \n",
    "print(f\"Num√©ricas outras: {len(NUMERIC_OTHER_COLUMNS)} colunas\")\n",
    "print(f\"Categ√≥ricas: {len(CATEGORICAL_COLUMNS)} colunas\")\n",
    "print(f\"Datas (p/ depois): {len(DATE_COLUMNS)} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4089e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a Classe de Tranforma√ß√£o\n",
    "class DataTransformer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.transformations_applied = {}\n",
    "\n",
    "    def clean_categorical_data(self):\n",
    "        \"\"\"Padroniza strings e substitui valores problem√°ticos\"\"\"\n",
    "        df_clean = self.df.copy()\n",
    "        for col in CATEGORICAL_COLUMNS:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].astype(str).str.strip().str.upper()\n",
    "                df_clean[col] = df_clean[col].replace(['NAN','NULL','NONE','','NaT'], np.nan)\n",
    "        self.transformations_applied['categorical_cleaning'] = CATEGORICAL_COLUMNS\n",
    "        return df_clean\n",
    "\n",
    "    def encode_categorical_variables(self, df_clean):\n",
    "        \"\"\"Aplica encoding categ√≥rico\"\"\"\n",
    "        df_encoded = df_clean.copy()\n",
    "        for col in CATEGORICAL_COLUMNS:\n",
    "            if col in df_encoded.columns:\n",
    "                unique_count = df_encoded[col].nunique()\n",
    "                if unique_count <= 10:\n",
    "                    dummies = pd.get_dummies(df_encoded[col], prefix=f\"{col}_\", drop_first=True)\n",
    "                    df_encoded = pd.concat([df_encoded.drop(col, axis=1), dummies], axis=1)\n",
    "                    self.encoders[col] = 'one_hot'\n",
    "                else:\n",
    "                    le = LabelEncoder()\n",
    "                    df_encoded[f\"{col}_encoded\"] = le.fit_transform(df_encoded[col])\n",
    "                    df_encoded = df_encoded.drop(col, axis=1)\n",
    "                    self.encoders[col] = le\n",
    "        self.transformations_applied['encoding'] = CATEGORICAL_COLUMNS\n",
    "        return df_encoded\n",
    "\n",
    "    def scale_numeric_variables(self, df_encoded):\n",
    "        \"\"\"Aplica normaliza√ß√£o nas vari√°veis num√©ricas\"\"\"\n",
    "        df_scaled = df_encoded.copy()\n",
    "        for col in NUMERIC_PRICE_COLUMNS:\n",
    "            if col in df_scaled.columns:\n",
    "                scaler = MinMaxScaler()\n",
    "                df_scaled[col] = scaler.fit_transform(df_scaled[[col]])\n",
    "                self.scalers[col] = scaler\n",
    "        for col in NUMERIC_OTHER_COLUMNS:\n",
    "            if col in df_scaled.columns:\n",
    "                scaler = StandardScaler()\n",
    "                df_scaled[col] = scaler.fit_transform(df_scaled[[col]])\n",
    "                self.scalers[col] = scaler\n",
    "        self.transformations_applied['scaling'] = NUMERIC_PRICE_COLUMNS + NUMERIC_OTHER_COLUMNS\n",
    "        return df_scaled\n",
    "\n",
    "    def transform_all(self):\n",
    "        df_clean = self.clean_categorical_data()\n",
    "        df_encoded = self.encode_categorical_variables(df_clean)\n",
    "        df_scaled = self.scale_numeric_variables(df_encoded)\n",
    "        return df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando as Transforma√ß√µes usando df_clean como entrada\n",
    "print(\"=== APLICANDO TRANSFORMA√á√ïES ===\")\n",
    "print(f\"Entrada: df_clean shape = {df_clean.shape}\")\n",
    "\n",
    "transformer = DataTransformer(df_clean)\n",
    "df_transformed = transformer.transform_all()\n",
    "\n",
    "print(f\"\\n=== RESULTADO DAS TRANSFORMA√á√ïES ===\")\n",
    "print(f\"Transforma√ß√µes aplicadas: {list(transformer.transformations_applied.keys())}\")\n",
    "print(f\"df_transformed shape: {df_transformed.shape}\")\n",
    "\n",
    "# Resumo da progress√£o completa\n",
    "print(f\"\\n=== PROGRESS√ÉO COMPLETA DOS DATAFRAMES ===\")\n",
    "print(f\"df_raw:         {df_raw.shape}\")\n",
    "print(f\"df_clean:       {df_clean.shape}\")\n",
    "print(f\"df_transformed: {df_transformed.shape}\")\n",
    "print(f\"Colunas adicionadas: {df_transformed.shape[1] - df_clean.shape[1]}\")\n",
    "\n",
    "print(f\"\\n=== VERIFICA√á√ïES DE QUALIDADE ===\")\n",
    "print(f\"Tipos de dados:\")\n",
    "print(df_transformed.dtypes.value_counts())\n",
    "\n",
    "null_counts = df_transformed.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è VALORES NULOS RESTANTES:\")\n",
    "    print(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhum valor nulo restante!\")\n",
    "\n",
    "print(f\"\\n=== AMOSTRA DO RESULTADO FINAL ===\")\n",
    "df_transformed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8f3e2",
   "metadata": {},
   "source": [
    "### 2.4 Salvamento das Bases de Dados  \n",
    "\n",
    "Ap√≥s as etapas de pr√©-processamento realizadas (remo√ß√£o de colunas irrelevantes, tratamento de valores ausentes, filtros de limpeza avan√ßada e transforma√ß√µes das vari√°veis), salvamos duas vers√µes da base de dados:\n",
    "\n",
    "1. **Dataset Limpo (`dataset_limpo.csv`):** Cont√©m os dados ap√≥s a limpeza e filtros, mas **antes das transforma√ß√µes** (normaliza√ß√£o e codifica√ß√£o). Esta vers√£o preserva os dados em formato original e √© ideal para an√°lises explorat√≥rias e visualiza√ß√µes.\n",
    "\n",
    "2. **Dataset Codificado (`dataset_codificado.csv`):** Cont√©m os dados **ap√≥s todas as transforma√ß√µes** aplicadas (normaliza√ß√£o de vari√°veis num√©ricas e codifica√ß√£o de vari√°veis categ√≥ricas). Esta vers√£o √© otimizada para algoritmos de Machine Learning e modelagem estat√≠stica.\n",
    "\n",
    "Ambas as vers√µes incluem os filtros de qualidade aplicados: corre√ß√£o de idades inv√°lidas, remo√ß√£o de registros com pre√ßos inconsistentes e elimina√ß√£o de duplicatas.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92210d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamento Final dos Datasets\n",
    "print(\"=== SALVAMENTO DOS DATASETS ===\")\n",
    "\n",
    "# 1. Salvamento do Dataset Limpo (df_clean)\n",
    "print(\"\\n--- SALVAMENTO DO DATASET LIMPO ---\")\n",
    "print(f\"Dataset limpo (df_clean):\")\n",
    "print(f\"  Shape: {df_clean.shape}\")\n",
    "print(f\"  Valores nulos: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"  Tipos de dados: {dict(df_clean.dtypes.value_counts())}\")\n",
    "\n",
    "output_path_clean = \"../database/dataset gerado/dataset_limpo.csv\"\n",
    "df_clean.to_csv(output_path_clean, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Dataset limpo exportado com sucesso!\")\n",
    "print(f\"Arquivo: {output_path_clean}\")\n",
    "\n",
    "# 2. Salvamento do Dataset Codificado (df_transformed)\n",
    "print(\"\\n--- SALVAMENTO DO DATASET CODIFICADO ---\")\n",
    "print(f\"Dataset codificado (df_transformed):\")\n",
    "print(f\"  Shape: {df_transformed.shape}\")\n",
    "print(f\"  Valores nulos: {df_transformed.isnull().sum().sum()}\")\n",
    "print(f\"  Tipos de dados: {dict(df_transformed.dtypes.value_counts())}\")\n",
    "\n",
    "output_path_transformed = \"../database/dataset gerado/dataset_codificado.csv\"\n",
    "df_transformed.to_csv(output_path_transformed, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Dataset codificado exportado com sucesso!\")\n",
    "print(f\"Arquivo: {output_path_transformed}\")\n",
    "\n",
    "# Resumo final da progress√£o\n",
    "print(f\"\\n=== RESUMO FINAL DA PROGRESS√ÉO ===\")\n",
    "print(f\"df_raw (original):     {df_raw.shape}\")\n",
    "print(f\"df_clean (p√≥s-limpeza): {df_clean.shape}\")\n",
    "print(f\"df_transformed (final): {df_transformed.shape}\")\n",
    "print(f\"\\nProcesso conclu√≠do: Dados limpos, transformados e salvos!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
