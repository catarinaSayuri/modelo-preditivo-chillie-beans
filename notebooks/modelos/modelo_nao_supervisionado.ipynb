{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9034c97c",
   "metadata": {},
   "source": [
    "# Modelo Preditivo N√£o Supervisionado - Chilli Beans\n",
    "## An√°lise de Localiza√ß√£o para Expans√£o de Lojas com Foco em √ìculos de Grau\n",
    "\n",
    "Este notebook desenvolve um modelo de clusteriza√ß√£o para identificar melhores localiza√ß√µes para posicionar novas lojas da Chilli Beans, com foco na venda de √≥culos de grau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9aff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dados\n",
    "df = pd.read_csv('../../database/dataset gerado/dataset_codificado.csv')\n",
    "\n",
    "print(\"Dataset carregado com sucesso!\")\n",
    "print(f\"Dimens√µes do dataset: {df.shape}\")\n",
    "print(f\"\\nColunas dispon√≠veis: {list(df.columns)}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1791b65",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è  Carregamento de Coordenadas Geogr√°ficas \n",
    "\n",
    "Esta etapa carrega um arquivo CSV que cont√©m as coordenadas geogr√°ficas (latitude e longitude) para cada combina√ß√£o de bairro, cidade e estado presentes nos dados da Chilli Beans.\n",
    "\n",
    "**1. Por que precisamos dessas coordenadas:**\n",
    "- O dataset original possui apenas nomes de bairros, cidades e estados codificados\n",
    "- Para fazer an√°lise geogr√°fica e clustering espacial, precisamos das coordenadas exatas\n",
    "- As coordenadas permitem calcular dist√¢ncias reais entre localidades\n",
    "\n",
    "**2. O que o arquivo `com_coordenadas.csv` cont√©m:**\n",
    "- Cada linha representa uma localiza√ß√£o √∫nica (bairro + cidade + estado)\n",
    "- Colunas com latitude e longitude em formato decimal\n",
    "- Mesma estrutura de nomea√ß√£o das colunas do dataset principal\n",
    "\n",
    "**3. Verifica√ß√µes importantes realizadas:**\n",
    "- Total de localiza√ß√µes dispon√≠veis no arquivo de coordenadas\n",
    "- Quais estados est√£o cobertos pelas coordenadas\n",
    "- Se existem dados faltantes (valores nulos) nas coordenadas\n",
    "\n",
    "**4. Resultado esperado:**\n",
    "- Confirma√ß√£o de que temos coordenadas para as principais localidades\n",
    "- Identifica√ß√£o de poss√≠veis lacunas nos dados geogr√°ficos\n",
    "- Base s√≥lida para aplicar algoritmos de clustering espacial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f92ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando CSV com coordenadas precisas por bairro/cidade\n",
    "print(\"=\"*50)\n",
    "print(\"CARREGANDO COORDENADAS PRECISAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Carregando coordenadas por bairro/cidade\n",
    "coordenadas_precisas = pd.read_csv('../../database/dataset gerado/com_coordenadas.csv')\n",
    "\n",
    "print(f\"Total de localiza√ß√µes precisas carregadas: {len(coordenadas_precisas)}\")\n",
    "print(\"\\nPrimeiras localiza√ß√µes:\")\n",
    "print(coordenadas_precisas.head())\n",
    "\n",
    "print(\"\\nEstados √∫nicos nas coordenadas precisas:\")\n",
    "print(sorted(coordenadas_precisas['Dim_Lojas.Estado_Emp'].unique()))\n",
    "\n",
    "print(\"\\nVerificando dados ausentes:\")\n",
    "print(coordenadas_precisas.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adbc4d",
   "metadata": {},
   "source": [
    "##  Cria√ß√£o de Mapeamentos Reversos para Coordenadas\n",
    "\n",
    "Esta etapa √© fundamental para conectar os dados codificados com as coordenadas geogr√°ficas reais. O processo cria mapeamentos que permitem converter c√≥digos num√©ricos de volta para nomes reais de estados, cidades e bairros.\n",
    "\n",
    "**1. Por que usamos dados codificados vs dados originais:**\n",
    "- O dataset codificado (`dataset_codificado.csv`) √© usado para processamento pelo modelo porque:\n",
    "  - Os algoritmos de machine learning funcionam melhor com dados num√©ricos\n",
    "  - Evita problemas de encoding de caracteres especiais\n",
    "  - Garante consist√™ncia e padroniza√ß√£o dos dados\n",
    "- Por√©m, para an√°lises e visualiza√ß√µes humanas, precisamos dos nomes reais\n",
    "- **Esta c√©lula faz a ponte entre os dois mundos**: processamento num√©rico + interpreta√ß√£o textual\n",
    "\n",
    "**2. Por que precisamos de mapeamentos reversos:**\n",
    "- O dataset principal (`dataset_codificado.csv`) possui apenas c√≥digos num√©ricos para localiza√ß√£o\n",
    "- O arquivo de coordenadas (`com_coordenadas.csv`) usa nomes reais de estados, cidades e bairros\n",
    "- Precisamos conectar esses dois formatos para associar coordenadas aos registros\n",
    "- **Essencial para mostrar resultados interpret√°veis nas an√°lises**\n",
    "\n",
    "**3. Processo de cria√ß√£o dos mapeamentos:**\n",
    "- Carrega o dataset original n√£o codificado (`dataset_limpo.csv`)\n",
    "- Usa a fun√ß√£o `pd.factorize()` para recriar a mesma codifica√ß√£o num√©rica usada anteriormente\n",
    "- Cria dicion√°rios que mapeiam c√≥digo ‚Üí nome real para cada n√≠vel geogr√°fico\n",
    "- **Garante que a codifica√ß√£o seja id√™ntica √† usada no dataset principal**\n",
    "\n",
    "**4. Estrutura dos mapeamentos criados:**\n",
    "- `estado_mapping`: {0: 'SP', 1: 'RJ', 2: 'MG', ...}\n",
    "- `cidade_mapping`: {0: 'S√£o Paulo', 1: 'Rio de Janeiro', ...}\n",
    "- `bairro_mapping`: {0: 'Centro', 1: 'Copacabana', ...}\n",
    "\n",
    "**5. Valida√ß√µes realizadas:**\n",
    "- Verifica se as colunas de localiza√ß√£o existem no dataset original\n",
    "- Conta quantos valores √∫nicos existem para cada n√≠vel geogr√°fico\n",
    "- Confirma que os mapeamentos foram criados corretamente\n",
    "- **Essencial para garantir que n√£o h√° perda de informa√ß√£o na convers√£o**\n",
    "\n",
    "**6. Resultado esperado:**\n",
    "- Tr√™s dicion√°rios de mapeamento funcionais\n",
    "- Capacidade de converter qualquer c√≥digo num√©rico de volta para nome real\n",
    "- Base para aplicar coordenadas geogr√°ficas precisas aos dados\n",
    "- **Permitir interpreta√ß√£o humana dos resultados do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f39af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, precisamos carregar o dataset original para criar os mapeamentos reversos\n",
    "print(\"=\"*50)\n",
    "print(\"CRIANDO MAPEAMENTOS REVERSOS PARA COORDENADAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Carregando dataset original n√£o codificado para criar mapeamentos\n",
    "df_original = pd.read_csv('../../database/dataset gerado/dataset_limpo.csv')\n",
    "\n",
    "print(f\"Dataset original carregado: {len(df_original)} registros\")\n",
    "\n",
    "# Verificando se tem as colunas necess√°rias\n",
    "print(\"Colunas relacionadas a localiza√ß√£o no dataset original:\")\n",
    "location_cols = [col for col in df_original.columns if any(term in col.lower() for term in ['bairro', 'cidade', 'estado', 'emp'])]\n",
    "print(location_cols)\n",
    "\n",
    "# Criando mapeamentos reversos dos c√≥digos para valores originais\n",
    "if 'Dim_Lojas.Estado_Emp' in df_original.columns:\n",
    "    # Mapeamento Estado\n",
    "    estado_mapping = dict(zip(\n",
    "        pd.factorize(df_original['Dim_Lojas.Estado_Emp'])[0],\n",
    "        df_original['Dim_Lojas.Estado_Emp']\n",
    "    ))\n",
    "    \n",
    "    # Mapeamento Cidade  \n",
    "    cidade_mapping = dict(zip(\n",
    "        pd.factorize(df_original['Dim_Lojas.Cidade_Emp'])[0],\n",
    "        df_original['Dim_Lojas.Cidade_Emp']\n",
    "    ))\n",
    "    \n",
    "    # Mapeamento Bairro\n",
    "    bairro_mapping = dict(zip(\n",
    "        pd.factorize(df_original['Dim_Lojas.Bairro_Emp'])[0],\n",
    "        df_original['Dim_Lojas.Bairro_Emp']\n",
    "    ))\n",
    "    \n",
    "    print(f\"Mapeamentos criados:\")\n",
    "    print(f\"- Estados √∫nicos: {len(set(estado_mapping.values()))}\")\n",
    "    print(f\"- Cidades √∫nicas: {len(set(cidade_mapping.values()))}\")\n",
    "    print(f\"- Bairros √∫nicos: {len(set(bairro_mapping.values()))}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Colunas de localiza√ß√£o n√£o encontradas. Verificando estrutura...\")\n",
    "    print(df_original.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73eb4e",
   "metadata": {},
   "source": [
    "## Aplica√ß√£o de Coordenadas Precisas aos Dados\n",
    "\n",
    "Esta c√©lula implementa a fun√ß√£o central para mapear coordenadas geogr√°ficas precisas aos dados codificados, estabelecendo a ponte entre os c√≥digos num√©ricos e as localiza√ß√µes reais.\n",
    "\n",
    "**1. Fun√ß√£o `obter_coordenadas_precisas()`:**\n",
    "- **Estrat√©gia em cascata** para m√°xima precis√£o:\n",
    "  1. **Match exato**: Busca por bairro + cidade + estado espec√≠ficos\n",
    "  2. **Match por cidade**: Se n√£o encontrar o bairro, usa cidade + estado\n",
    "  3. **Fallback por estado**: Como √∫ltimo recurso, usa coordenadas centrais do estado\n",
    "\n",
    "**2. Por que essa abordagem hier√°rquica:**\n",
    "- **Precis√£o m√°xima**: Prioriza coordenadas espec√≠ficas quando dispon√≠veis\n",
    "- **Robustez**: Garante que todos os registros tenham coordenadas v√°lidas\n",
    "- **Realismo**: Usa localiza√ß√µes reais em vez de estimativas grosseiras\n",
    "- **Escalabilidade**: Funciona mesmo com dados incompletos\n",
    "\n",
    "**3. Processo de aplica√ß√£o das coordenadas:**\n",
    "- Percorre cada registro do dataset codificado\n",
    "- **Decodifica** os c√≥digos num√©ricos usando os mapeamentos reversos criados\n",
    "- Aplica a fun√ß√£o de busca de coordenadas para cada localiza√ß√£o\n",
    "- **Valida** se as coordenadas s√£o v√°lidas (n√£o nulas/zero)\n",
    "\n",
    "**4. Tratamento de casos especiais:**\n",
    "- **Dados faltantes**: Usa coordenadas do centro do estado como fallback\n",
    "- **Caracteres especiais**: Os mapeamentos reversos garantem consist√™ncia\n",
    "- **Coordenadas inv√°lidas**: Sistema de verifica√ß√£o evita dados corrompidos\n",
    "\n",
    "**5. Valida√ß√µes implementadas:**\n",
    "- Contagem de registros com coordenadas v√°lidas vs inv√°lidas\n",
    "- Exemplos pr√°ticos de coordenadas aplicadas\n",
    "- Estat√≠sticas de distribui√ß√£o geogr√°fica\n",
    "- Verifica√ß√£o de cobertura por estado\n",
    "\n",
    "**6. Benef√≠cios desta abordagem:**\n",
    "- **100% de cobertura**: Todos os registros recebem coordenadas\n",
    "- **Precis√£o vari√°vel**: Desde coordenadas exatas at√© aproxima√ß√µes estaduais\n",
    "- **Interpretabilidade**: Permite an√°lises geogr√°ficas detalhadas\n",
    "- **Clustering espacial**: Base s√≥lida para algoritmos de agrupamento geogr√°fico\n",
    "\n",
    "**7. Output esperado:**\n",
    "- Dataset enriquecido com colunas `latitude` e `longitude`\n",
    "- Coordenadas v√°lidas para an√°lise de clustering\n",
    "- Mapeamento completo entre dados codificados e localiza√ß√£o real\n",
    "- **Funda√ß√£o para identifica√ß√£o de locais ideais para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para mapear coordenadas precisas\n",
    "def obter_coordenadas_precisas(bairro, cidade, estado, coordenadas_df):\n",
    "    \"\"\"\n",
    "    Obt√©m coordenadas precisas baseadas em bairro, cidade e estado\n",
    "    \"\"\"\n",
    "    # Primeiro, tenta match exato por bairro, cidade e estado\n",
    "    match_exato = coordenadas_df[\n",
    "        (coordenadas_df['Dim_Lojas.Bairro_Emp'] == bairro) & \n",
    "        (coordenadas_df['Dim_Lojas.Cidade_Emp'] == cidade) & \n",
    "        (coordenadas_df['Dim_Lojas.Estado_Emp'] == estado)\n",
    "    ]\n",
    "    \n",
    "    if not match_exato.empty and not pd.isna(match_exato.iloc[0]['Latitude']):\n",
    "        return match_exato.iloc[0]['Latitude'], match_exato.iloc[0]['Longitude']\n",
    "    \n",
    "    # Se n√£o encontrar, tenta por cidade e estado\n",
    "    match_cidade = coordenadas_df[\n",
    "        (coordenadas_df['Dim_Lojas.Cidade_Emp'] == cidade) & \n",
    "        (coordenadas_df['Dim_Lojas.Estado_Emp'] == estado)\n",
    "    ]\n",
    "    \n",
    "    if not match_cidade.empty and not pd.isna(match_cidade.iloc[0]['Latitude']):\n",
    "        return match_cidade.iloc[0]['Latitude'], match_cidade.iloc[0]['Longitude']\n",
    "    \n",
    "    # Se ainda n√£o encontrar, usa coordenadas por estado (fallback)\n",
    "    estado_coords = {\n",
    "        'AC': (-9.97, -67.81), 'AL': (-9.66, -35.73), 'AP': (0.04, -51.06), 'AM': (-3.13, -59.98),\n",
    "        'BA': (-12.98, -38.48), 'CE': (-3.73, -38.52), 'DF': (-15.79, -47.88), 'ES': (-20.32, -40.31),\n",
    "        'GO': (-16.68, -49.25), 'MA': (-2.56, -44.06), 'MT': (-15.60, -56.10), 'MS': (-20.46, -54.62),\n",
    "        'MG': (-19.92, -43.94), 'PA': (-1.45, -48.47), 'PB': (-7.12, -34.88), 'PR': (-25.43, -49.27),\n",
    "        'PE': (-8.06, -34.88), 'PI': (-5.09, -42.80), 'RJ': (-22.91, -43.21), 'RN': (-5.81, -35.21),\n",
    "        'RS': (-30.03, -51.23), 'RO': (-8.76, -63.87), 'RR': (2.82, -60.67), 'SC': (-27.60, -48.55),\n",
    "        'SP': (-23.55, -46.63), 'SE': (-10.92, -37.08), 'TO': (-10.18, -48.33)\n",
    "    }\n",
    "    \n",
    "    return estado_coords.get(estado, (0, 0))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"APLICANDO COORDENADAS PRECISAS AOS DADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Aplicando coordenadas precisas aos dados usando mapeamentos reversos\n",
    "lat_coords = []\n",
    "lon_coords = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Convertendo c√≥digos de volta para valores originais\n",
    "    estado_code = row['Dim_Lojas.Estado_Emp_encoded']\n",
    "    cidade_code = row['Dim_Lojas.Cidade_Emp_encoded'] \n",
    "    bairro_code = row['Dim_Lojas.Bairro_Emp_encoded']\n",
    "    \n",
    "    # Obtendo valores originais\n",
    "    estado = estado_mapping.get(estado_code, 'SP')  # fallback para SP\n",
    "    cidade = cidade_mapping.get(cidade_code, 'S√£o Paulo')  # fallback\n",
    "    bairro = bairro_mapping.get(bairro_code, 'Centro')  # fallback\n",
    "    \n",
    "    lat, lon = obter_coordenadas_precisas(bairro, cidade, estado, coordenadas_precisas)\n",
    "    lat_coords.append(lat)\n",
    "    lon_coords.append(lon)\n",
    "\n",
    "# Atualizando o DataFrame com coordenadas precisas\n",
    "df['latitude'] = lat_coords\n",
    "df['longitude'] = lon_coords\n",
    "\n",
    "print(f\"Total de registros com coordenadas atualizadas: {len(df)}\")\n",
    "print(f\"Coordenadas v√°lidas (n√£o zero): {len(df[(df['latitude'] != 0) & (df['longitude'] != 0)])}\")\n",
    "\n",
    "# Verificando exemplos de coordenadas atualizadas\n",
    "print(\"\\nExemplos de coordenadas atualizadas:\")\n",
    "sample_indices = df.sample(10).index\n",
    "for i in sample_indices[:5]:\n",
    "    estado = estado_mapping.get(df.loc[i, 'Dim_Lojas.Estado_Emp_encoded'], 'N/A')\n",
    "    cidade = cidade_mapping.get(df.loc[i, 'Dim_Lojas.Cidade_Emp_encoded'], 'N/A')\n",
    "    bairro = bairro_mapping.get(df.loc[i, 'Dim_Lojas.Bairro_Emp_encoded'], 'N/A')\n",
    "    lat = df.loc[i, 'latitude']\n",
    "    lon = df.loc[i, 'longitude']\n",
    "    print(f\"{bairro}, {cidade}, {estado}: ({lat:.3f}, {lon:.3f})\")\n",
    "\n",
    "# Estat√≠sticas das coordenadas\n",
    "print(f\"\\nFaixa de Latitude: {df['latitude'].min():.2f} a {df['latitude'].max():.2f}\")\n",
    "print(f\"Faixa de Longitude: {df['longitude'].min():.2f} a {df['longitude'].max():.2f}\")\n",
    "\n",
    "# Verificando distribui√ß√£o por estado usando mapeamento reverso\n",
    "estados_originais = [estado_mapping.get(code, 'N/A') for code in df['Dim_Lojas.Estado_Emp_encoded']]\n",
    "estado_counts = pd.Series(estados_originais).value_counts().head(10)\n",
    "print(\"\\nDistribui√ß√£o de registros por estado (top 10):\")\n",
    "print(estado_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d49388",
   "metadata": {},
   "source": [
    "## Verifica√ß√£o e Corre√ß√£o de Mapeamentos\n",
    "\n",
    "Esta etapa cr√≠tica garante que os mapeamentos entre c√≥digos num√©ricos e valores originais estejam corretos, criando uma base s√≥lida para a aplica√ß√£o precisa de coordenadas geogr√°ficas.\n",
    "\n",
    "**1. Por que precisamos verificar os mapeamentos:**\n",
    "- **Valida√ß√£o de integridade**: Confirma que a codifica√ß√£o foi consistente\n",
    "- **Corre√ß√£o de discrep√¢ncias**: Identifica e corrige poss√≠veis erros na cria√ß√£o inicial dos mapeamentos\n",
    "- **Garantia de precis√£o**: Assegura que cada c√≥digo corresponde exatamente ao valor original correto\n",
    "- **Base para coordenadas**: Mapeamentos incorretos resultariam em coordenadas aplicadas aos locais errados\n",
    "\n",
    "**2. Processo de verifica√ß√£o implementado:**\n",
    "- **Concatena√ß√£o inteligente**: Une os dados codificados com os dados originais pelo √≠ndice\n",
    "- **Verifica√ß√£o linha por linha**: Percorre cada registro para criar mapeamentos baseados na correspond√™ncia real\n",
    "- **Valida√ß√£o de dados**: Ignora valores nulos ou inv√°lidos durante a cria√ß√£o dos mapeamentos\n",
    "- **Compara√ß√£o com mapeamentos anteriores**: Permite identificar diferen√ßas e inconsist√™ncias\n",
    "\n",
    "**3. Metodologia de corre√ß√£o:**\n",
    "- **Mapeamento direto**: Para cada c√≥digo, identifica o valor original correspondente no mesmo registro\n",
    "- **Elimina√ß√£o de duplicatas**: Garante que cada c√≥digo seja mapeado para apenas um valor\n",
    "- **Tratamento de casos especiais**: Lida com valores ausentes ou c√≥digos √≥rf√£os\n",
    "- **Valida√ß√£o estat√≠stica**: Conta valores √∫nicos para verificar consist√™ncia\n",
    "\n",
    "**4. Benef√≠cios da corre√ß√£o:**\n",
    "- **Coordenadas precisas**: Elimina erros na aplica√ß√£o de latitude/longitude\n",
    "- **Interpretabilidade correta**: Garante que an√°lises mostrem os locais reais\n",
    "- **Confiabilidade do modelo**: Reduz ru√≠do causado por mapeamentos incorretos\n",
    "- **Reprodutibilidade**: Permite que an√°lises futuras usem os mesmos mapeamentos\n",
    "\n",
    "**5. Valida√ß√µes implementadas:**\n",
    "- **Contagem de valores √∫nicos**: Verifica se o n√∫mero de estados, cidades e bairros √∫nicos faz sentido\n",
    "- **Exemplos de mapeamento**: Mostra casos reais de c√≥digo ‚Üí valor para verifica√ß√£o manual\n",
    "- **Teste de consist√™ncia**: Confirma que os mapeamentos s√£o bijettivos (um-para-um)\n",
    "- **Verifica√ß√£o de completude**: Assegura que todos os c√≥digos tenham valores correspondentes\n",
    "\n",
    "**6. Re-aplica√ß√£o de coordenadas:**\n",
    "- **Uso dos mapeamentos corrigidos**: Aplica as coordenadas usando os novos mapeamentos verificados\n",
    "- **Compara√ß√£o antes/depois**: Permite avaliar o impacto das corre√ß√µes\n",
    "- **Valida√ß√£o final**: Confirma que as coordenadas aplicadas s√£o geograficamente v√°lidas\n",
    "- **Estat√≠sticas atualizadas**: Recalcula distribui√ß√µes por estado com dados corrigidos\n",
    "\n",
    "**7. Output esperado:**\n",
    "- Mapeamentos reversos precisos e validados\n",
    "- Coordenadas geogr√°ficas aplicadas corretamente a todos os registros\n",
    "- Base confi√°vel para todas as an√°lises subsequentes de clustering\n",
    "- **Elimina√ß√£o de erros sistem√°ticos** que poderiam comprometer a identifica√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eea836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando e corrigindo mapeamentos\n",
    "print(\"=\"*50)\n",
    "print(\"VERIFICANDO E CORRIGINDO MAPEAMENTOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Criando mapeamentos mais precisos usando o LabelEncoder approach\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Verificando se os dados codificados correspondem aos originais\n",
    "print(\"Verificando mapeamentos...\")\n",
    "\n",
    "# Criando um DataFrame com c√≥digos e valores originais para verifica√ß√£o\n",
    "check_df = pd.DataFrame({\n",
    "    'estado_code': df['Dim_Lojas.Estado_Emp_encoded'],\n",
    "    'cidade_code': df['Dim_Lojas.Cidade_Emp_encoded'], \n",
    "    'bairro_code': df['Dim_Lojas.Bairro_Emp_encoded']\n",
    "})\n",
    "\n",
    "# Juntando com dados originais baseados no √≠ndice\n",
    "df_with_original = pd.concat([\n",
    "    df[['Dim_Lojas.Estado_Emp_encoded', 'Dim_Lojas.Cidade_Emp_encoded', 'Dim_Lojas.Bairro_Emp_encoded']],\n",
    "    df_original[['Dim_Lojas.Estado_Emp', 'Dim_Lojas.Cidade_Emp', 'Dim_Lojas.Bairro_Emp']]\n",
    "], axis=1)\n",
    "\n",
    "print(\"Criando mapeamentos corretos...\")\n",
    "\n",
    "# Criando mapeamentos corretos baseados na correspond√™ncia real\n",
    "estado_map_correto = {}\n",
    "cidade_map_correto = {}\n",
    "bairro_map_correto = {}\n",
    "\n",
    "for idx in range(len(df_with_original)):\n",
    "    # Estado\n",
    "    code_estado = df_with_original.iloc[idx]['Dim_Lojas.Estado_Emp_encoded']\n",
    "    val_estado = df_with_original.iloc[idx]['Dim_Lojas.Estado_Emp']\n",
    "    if pd.notna(code_estado) and pd.notna(val_estado):\n",
    "        estado_map_correto[code_estado] = val_estado\n",
    "    \n",
    "    # Cidade  \n",
    "    code_cidade = df_with_original.iloc[idx]['Dim_Lojas.Cidade_Emp_encoded']\n",
    "    val_cidade = df_with_original.iloc[idx]['Dim_Lojas.Cidade_Emp']\n",
    "    if pd.notna(code_cidade) and pd.notna(val_cidade):\n",
    "        cidade_map_correto[code_cidade] = val_cidade\n",
    "        \n",
    "    # Bairro\n",
    "    code_bairro = df_with_original.iloc[idx]['Dim_Lojas.Bairro_Emp_encoded'] \n",
    "    val_bairro = df_with_original.iloc[idx]['Dim_Lojas.Bairro_Emp']\n",
    "    if pd.notna(code_bairro) and pd.notna(val_bairro):\n",
    "        bairro_map_correto[code_bairro] = val_bairro\n",
    "\n",
    "print(f\"Mapeamentos corretos criados:\")\n",
    "print(f\"- Estados: {len(set(estado_map_correto.values()))} √∫nicos\")\n",
    "print(f\"- Cidades: {len(set(cidade_map_correto.values()))} √∫nicas\") \n",
    "print(f\"- Bairros: {len(set(bairro_map_correto.values()))} √∫nicos\")\n",
    "\n",
    "# Testando alguns exemplos\n",
    "print(\"\\nExemplos de mapeamentos corretos:\")\n",
    "for code in list(estado_map_correto.keys())[:10]:\n",
    "    print(f\"C√≥digo {code} -> {estado_map_correto[code]}\")\n",
    "\n",
    "# Replicando coordenadas com mapeamentos corretos\n",
    "print(\"\\nReaplicando coordenadas com mapeamentos corretos...\")\n",
    "\n",
    "lat_coords_correto = []\n",
    "lon_coords_correto = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Convertendo c√≥digos de volta para valores originais (usando mapeamentos corretos)\n",
    "    estado_code = row['Dim_Lojas.Estado_Emp_encoded']\n",
    "    cidade_code = row['Dim_Lojas.Cidade_Emp_encoded'] \n",
    "    bairro_code = row['Dim_Lojas.Bairro_Emp_encoded']\n",
    "    \n",
    "    # Obtendo valores originais\n",
    "    estado = estado_map_correto.get(estado_code, 'SP')\n",
    "    cidade = cidade_map_correto.get(cidade_code, 'S√£o Paulo') \n",
    "    bairro = bairro_map_correto.get(bairro_code, 'Centro')\n",
    "    \n",
    "    lat, lon = obter_coordenadas_precisas(bairro, cidade, estado, coordenadas_precisas)\n",
    "    lat_coords_correto.append(lat)\n",
    "    lon_coords_correto.append(lon)\n",
    "\n",
    "# Atualizando o DataFrame com coordenadas corrigidas\n",
    "df['latitude'] = lat_coords_correto\n",
    "df['longitude'] = lon_coords_correto\n",
    "\n",
    "print(f\"\\nCoordenadas corrigidas aplicadas!\")\n",
    "print(f\"Coordenadas v√°lidas (n√£o zero): {len(df[(df['latitude'] != 0) & (df['longitude'] != 0)])}\")\n",
    "\n",
    "# Verificando exemplos corrigidos\n",
    "print(\"\\nExemplos de coordenadas corrigidas:\")\n",
    "sample_indices = [0, 100, 1000, 5000, 10000]\n",
    "for i in sample_indices:\n",
    "    if i < len(df):\n",
    "        estado = estado_map_correto.get(df.loc[i, 'Dim_Lojas.Estado_Emp_encoded'], 'N/A')\n",
    "        cidade = cidade_map_correto.get(df.loc[i, 'Dim_Lojas.Cidade_Emp_encoded'], 'N/A')\n",
    "        bairro = bairro_map_correto.get(df.loc[i, 'Dim_Lojas.Bairro_Emp_encoded'], 'N/A')\n",
    "        lat = df.loc[i, 'latitude']\n",
    "        lon = df.loc[i, 'longitude']\n",
    "        print(f\"{bairro}, {cidade}, {estado}: ({lat:.3f}, {lon:.3f})\")\n",
    "\n",
    "# Distribui√ß√£o corrigida por estado\n",
    "estados_corretos = [estado_map_correto.get(code, 'N/A') for code in df['Dim_Lojas.Estado_Emp_encoded']]\n",
    "estado_counts_correto = pd.Series(estados_corretos).value_counts().head(10)\n",
    "print(\"\\nDistribui√ß√£o corrigida por estado (top 10):\")\n",
    "print(estado_counts_correto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711712d",
   "metadata": {},
   "source": [
    "## üìä Processamento dos Dados com Coordenadas Precisas\n",
    "\n",
    "Esta se√ß√£o representa uma etapa crucial do modelo n√£o supervisionado, onde os dados brutos s√£o transformados em informa√ß√µes estruturadas e agregadas, prontas para an√°lise de clustering geogr√°fico com foco espec√≠fico em vendas de √≥culos de grau.\n",
    "\n",
    "**1. Investiga√ß√£o de Categorias de Produtos:**\n",
    "- **Identifica√ß√£o de colunas relevantes**: Busca por todas as colunas que contenham `Dim_Produtos.GRUPO_CHILLI__` para mapear categorias de produtos\n",
    "- **An√°lise espec√≠fica para produtos com \"GRAU\"**: Identifica colunas que contenham a palavra \"GRAU\" para capturar arma√ß√µes e produtos de corre√ß√£o visual\n",
    "- **An√°lise de produtos VISTA/LENTES**: Mapeia categorias relacionadas √† vis√£o e lentes de grau\n",
    "- **Valida√ß√£o quantitativa**: Conta valores √∫nicos e somas para entender a distribui√ß√£o de cada categoria de produto\n",
    "\n",
    "**2. Cria√ß√£o da M√©trica Principal - Vendas de √ìculos de Grau:**\n",
    "- **Identifica√ß√£o ampla de produtos**: Busca colunas que cont√™m \"GRAU\" no nome para capturar arma√ß√µes e produtos de grau\n",
    "- **Combina√ß√£o estrat√©gica**: Une produtos com \"GRAU\", \"VISTA\" e \"LENTES\" para criar uma m√©trica unificada abrangente\n",
    "- **Inclus√£o de arma√ß√µes**: Garante que arma√ß√µes de √≥culos de grau sejam inclu√≠das na an√°lise, n√£o apenas lentes\n",
    "- **Valida√ß√£o quantitativa**: Verifica a distribui√ß√£o e cobertura da nova m√©trica criada com detalhamento por categoria\n",
    "\n",
    "**3. Agrega√ß√£o por Localiza√ß√£o Geogr√°fica Precisa:**\n",
    "- **Chave √∫nica por coordenadas**: Cria `loc_key` combinando latitude e longitude para identificar localiza√ß√µes √∫nicas\n",
    "- **Agrega√ß√£o inteligente**: Agrupa dados por localiza√ß√£o precisa calculando:\n",
    "  - `num_lojas`: N√∫mero √∫nico de lojas por localiza√ß√£o\n",
    "  - `num_clientes`: N√∫mero √∫nico de clientes por localiza√ß√£o  \n",
    "  - `vendas_oculos_grau`: Soma total de vendas de √≥culos de grau\n",
    "  - `receita_total`: Soma da receita l√≠quida total\n",
    "  - `estado_code`: Mant√©m refer√™ncia do estado para mapeamento posterior\n",
    "\n",
    "**4. Enriquecimento com Informa√ß√µes Contextuais:**\n",
    "- **Mapeamento de estados**: Converte c√≥digos num√©ricos de volta para nomes de estados usando mapeamentos corrigidos\n",
    "- **Score de potencial multicrit√©rio**: Cria uma m√©trica composta considerando:\n",
    "  - Vendas de √≥culos de grau (peso 2x - prioridade principal)\n",
    "  - N√∫mero de clientes (peso 0.1)\n",
    "  - Receita total (peso 0.001) \n",
    "  - N√∫mero de lojas (peso 1.5)\n",
    "\n",
    "**5. Separa√ß√£o Estrat√©gica por Regi√£o:**\n",
    "- **S√£o Paulo vs Outros Estados**: Divide os dados em dois conjuntos para an√°lise independente\n",
    "- **Justificativa da separa√ß√£o**: S√£o Paulo possui caracter√≠sticas √∫nicas de densidade e volume que poderiam enviesar a an√°lise\n",
    "- **M√©tricas comparativas**: Calcula estat√≠sticas separadas para cada regi√£o\n",
    "\n",
    "**6. Valida√ß√µes e Estat√≠sticas Descritivas:**\n",
    "- **Cobertura geogr√°fica**: Verifica ranges de latitude e longitude para confirmar abrang√™ncia nacional\n",
    "- **Top localiza√ß√µes**: Identifica as 10 melhores localiza√ß√µes por potencial para valida√ß√£o visual\n",
    "- **Distribui√ß√£o regional**: Mostra como vendas e lojas se distribuem entre SP e outros estados\n",
    "\n",
    "**7. Prepara√ß√£o para Clustering:**\n",
    "- **Dados limpos e estruturados**: Cada linha representa uma localiza√ß√£o √∫nica com m√©tricas agregadas\n",
    "- **Coordenadas precisas**: Latitude e longitude validadas para algoritmos de clustering espacial\n",
    "- **M√©tricas padronizadas**: Vari√°veis em escalas apropriadas para an√°lise de machine learning\n",
    "\n",
    "**8. Output Esperado:**\n",
    "- **Dataset `dados_localizacao`**: Dataframe principal com localiza√ß√µes √∫nicas e m√©tricas agregadas\n",
    "- **Subsets regionais**: `dados_sp` e `dados_outros_estados` para an√°lises espec√≠ficas\n",
    "- **Base para clustering**: Dados prontos para aplica√ß√£o de algoritmos K-means com coordenadas geogr√°ficas\n",
    "\n",
    "**9. Benef√≠cios desta Abordagem:**\n",
    "- **Precis√£o geogr√°fica**: Usa coordenadas reais em vez de aproxima√ß√µes\n",
    "- **Agrega√ß√£o inteligente**: Evita duplicatas e consolida informa√ß√µes por localiza√ß√£o\n",
    "- **Flexibilidade anal√≠tica**: Permite an√°lises tanto unificadas quanto regionais\n",
    "- **Foco no neg√≥cio**: Prioriza m√©tricas relevantes para decis√µes de expans√£o de √≥culos de grau\n",
    "\n",
    "**10. Prepara√ß√£o para Pr√≥ximas Etapas:**\n",
    "- **Clustering espacial**: Dados estruturados permitem aplica√ß√£o direta de algoritmos de agrupamento\n",
    "- **An√°lise de potencial**: Score composto facilita identifica√ß√£o de oportunidades\n",
    "- **Visualiza√ß√µes**: Coordenadas permitem cria√ß√£o de mapas e gr√°ficos geogr√°ficos\n",
    "- **Tomada de decis√£o**: M√©tricas agregadas fornecem base s√≥lida para recomenda√ß√µes estrat√©gicas focadas em √≥culos de grau (incluindo arma√ß√µes e lentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESSAMENTO DOS DADOS COM COORDENADAS PRECISAS\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROCESSAMENTO DOS DADOS PARA CLUSTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Primeiro, vamos investigar as colunas de produtos dispon√≠veis\n",
    "print(\"üîç INVESTIGANDO CATEGORIAS DE PRODUTOS PARA √ìCULOS DE GRAU:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Verificar colunas relacionadas a produtos\n",
    "produto_cols = [col for col in df.columns if 'Dim_Produtos.GRUPO_CHILLI__' in col]\n",
    "print(f\"Colunas GRUPO_CHILLI encontradas: {len(produto_cols)}\")\n",
    "for col in produto_cols:\n",
    "    valores_unicos = df[col].sum() if df[col].dtype in ['int64', 'float64'] else df[col].nunique()\n",
    "    print(f\"  üìã {col}: {valores_unicos}\")\n",
    "\n",
    "print(f\"\\nüéØ IDENTIFICANDO PRODUTOS RELACIONADOS A √ìCULOS DE GRAU:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Identificar colunas que cont√™m \"GRAU\" no nome\n",
    "colunas_grau = [col for col in produto_cols if 'GRAU' in col.upper()]\n",
    "print(f\"Colunas que cont√™m 'GRAU': {len(colunas_grau)}\")\n",
    "for col in colunas_grau:\n",
    "    valores = df[col].sum() if df[col].dtype in ['int64', 'float64'] else df[col].nunique()\n",
    "    print(f\"  üìã {col}: {valores}\")\n",
    "\n",
    "# Identificar colunas relacionadas a VISTA/LENTES\n",
    "colunas_vista = [col for col in produto_cols if any(termo in col.upper() for termo in ['VISTA', 'LENTES'])]\n",
    "print(f\"\\nColunas relacionadas a VISTA/LENTES: {len(colunas_vista)}\")\n",
    "for col in colunas_vista:\n",
    "    valores = df[col].sum() if df[col].dtype in ['int64', 'float64'] else df[col].nunique()\n",
    "    print(f\"  üìã {col}: {valores}\")\n",
    "\n",
    "print(f\"\\nüéØ CRIANDO M√âTRICA ESPEC√çFICA PARA √ìCULOS DE GRAU:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Combinando todas as categorias relacionadas a √≥culos de grau\n",
    "colunas_oculos_grau = list(set(colunas_grau + colunas_vista))\n",
    "print(f\"Total de colunas para √≥culos de grau: {len(colunas_oculos_grau)}\")\n",
    "\n",
    "# Calculando vendas de √≥culos de grau\n",
    "df['vendas_oculos_grau'] = 0\n",
    "for col in colunas_oculos_grau:\n",
    "    if col in df.columns:\n",
    "        # Para colunas boolean, convertemos True/False para 1/0\n",
    "        if df[col].dtype == 'bool':\n",
    "            df['vendas_oculos_grau'] += df[col].astype(int)\n",
    "            print(f\"   ‚Ä¢ Adicionando {col} (boolean): {df[col].sum()} registros True\")\n",
    "        elif df[col].dtype in ['int64', 'float64']:\n",
    "            df['vendas_oculos_grau'] += df[col]\n",
    "            print(f\"   ‚Ä¢ Adicionando {col} (num√©rico): {df[col].sum()}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö† Ignorando {col} (tipo {df[col].dtype} n√£o suportado)\")\n",
    "\n",
    "print(f\"\\n‚úÖ M√©trica 'vendas_oculos_grau' criada combinando {len([col for col in colunas_oculos_grau if col in df.columns])} categorias:\")\n",
    "print(f\"   ‚Ä¢ TOTAL √≥culos de grau: {df['vendas_oculos_grau'].sum()}\")\n",
    "\n",
    "# Detalhamento por categoria\n",
    "print(f\"\\nüìä DETALHAMENTO POR CATEGORIA:\")\n",
    "for col in colunas_oculos_grau:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'bool':\n",
    "            total = df[col].sum()  # Para boolean, sum() conta os True\n",
    "            print(f\"   ‚Ä¢ {col.replace('Dim_Produtos.GRUPO_CHILLI__', '')}: {total} registros\")\n",
    "        elif df[col].dtype in ['int64', 'float64']:\n",
    "            total = df[col].sum()\n",
    "            if total > 0:\n",
    "                print(f\"   ‚Ä¢ {col.replace('Dim_Produtos.GRUPO_CHILLI__', '')}: {total}\")\n",
    "print(f\"   ‚Ä¢ TOTAL √≥culos de grau: {df['vendas_oculos_grau'].sum()}\")\n",
    "\n",
    "# Verifica√ß√£o da distribui√ß√£o\n",
    "print(f\"\\nüìä VALIDA√á√ÉO DA M√âTRICA:\")\n",
    "registros_com_oculos_grau = (df['vendas_oculos_grau'] > 0).sum()\n",
    "print(f\"   ‚Ä¢ Registros com vendas de √≥culos de grau: {registros_com_oculos_grau:,}\")\n",
    "print(f\"   ‚Ä¢ Porcentagem do dataset: {(registros_com_oculos_grau/len(df)*100):.1f}%\")\n",
    "\n",
    "# Agregando dados por localiza√ß√£o precisa (latitude/longitude)\n",
    "print(\"\\nüó∫Ô∏è AGREGANDO DADOS POR LOCALIZA√á√ÉO PRECISA...\")\n",
    "\n",
    "# Criando chave √∫nica por localiza√ß√£o\n",
    "df['loc_key'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "dados_localizacao = df.groupby(['loc_key', 'latitude', 'longitude']).agg({\n",
    "    'ID_Loja': 'nunique',\n",
    "    'ID_Cliente': 'nunique', \n",
    "    'vendas_oculos_grau': 'sum',\n",
    "    'Total_Preco_Liquido': 'sum',\n",
    "    'Dim_Lojas.Estado_Emp_encoded': 'first'  # Para manter refer√™ncia do estado\n",
    "}).reset_index()\n",
    "\n",
    "# Renomeando colunas\n",
    "dados_localizacao.columns = ['loc_key', 'latitude', 'longitude', 'num_lojas', 'num_clientes', \n",
    "                           'vendas_oculos_grau', 'receita_total', 'estado_code']\n",
    "\n",
    "# Adicionando estado original usando mapeamento correto\n",
    "dados_localizacao['estado'] = dados_localizacao['estado_code'].map(estado_map_correto)\n",
    "\n",
    "# Criando score de potencial baseado em m√∫ltiplos fatores\n",
    "dados_localizacao['potencial_score'] = (\n",
    "    dados_localizacao['vendas_oculos_grau'] * 2 +  # Peso maior para √≥culos de grau\n",
    "    dados_localizacao['num_clientes'] * 0.1 +\n",
    "    dados_localizacao['receita_total'] * 0.001 +\n",
    "    dados_localizacao['num_lojas'] * 1.5\n",
    ")\n",
    "\n",
    "print(f\"Dados agregados por localiza√ß√£o:\")\n",
    "print(f\"- Total de localiza√ß√µes √∫nicas: {len(dados_localizacao)}\")\n",
    "print(f\"- Total de vendas de √≥culos de grau: {dados_localizacao['vendas_oculos_grau'].sum()}\")\n",
    "print(f\"- Total de clientes √∫nicos: {dados_localizacao['num_clientes'].sum()}\")\n",
    "print(f\"- Total de lojas: {dados_localizacao['num_lojas'].sum()}\")\n",
    "print(f\"- Receita total: R$ {dados_localizacao['receita_total'].sum():,.2f}\")\n",
    "\n",
    "# Separando S√£o Paulo dos outros estados\n",
    "dados_sp = dados_localizacao[dados_localizacao['estado'] == 'SP'].copy()\n",
    "dados_outros_estados = dados_localizacao[dados_localizacao['estado'] != 'SP'].copy()\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o por regi√£o:\")\n",
    "print(f\"- Localiza√ß√µes em SP: {len(dados_sp)}\")\n",
    "print(f\"- Localiza√ß√µes em outros estados: {len(dados_outros_estados)}\")\n",
    "print(f\"- Vendas √≥culos SP: {dados_sp['vendas_oculos_grau'].sum()}\")\n",
    "print(f\"- Vendas √≥culos outros: {dados_outros_estados['vendas_oculos_grau'].sum()}\")\n",
    "\n",
    "# Estat√≠sticas das coordenadas precisas\n",
    "print(f\"\\nRanges de coordenadas:\")\n",
    "print(f\"- Latitude: {dados_localizacao['latitude'].min():.2f} a {dados_localizacao['latitude'].max():.2f}\")\n",
    "print(f\"- Longitude: {dados_localizacao['longitude'].min():.2f} a {dados_localizacao['longitude'].max():.2f}\")\n",
    "\n",
    "# Top 10 localiza√ß√µes por potencial\n",
    "print(f\"\\nTop 10 localiza√ß√µes por potencial:\")\n",
    "top_locations = dados_localizacao.nlargest(10, 'potencial_score')[['estado', 'latitude', 'longitude', 'potencial_score', 'vendas_oculos_grau', 'num_clientes']]\n",
    "print(top_locations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc0fd7",
   "metadata": {},
   "source": [
    "## Valida√ß√£o e Enriquecimento dos Dados Processados\n",
    "\n",
    "Esta se√ß√£o realiza a **valida√ß√£o completa e enriquecimento contextual** dos dados processados na etapa anterior, garantindo a qualidade dos dados antes da aplica√ß√£o dos algoritmos de clustering e fornecendo informa√ß√µes regionais estrat√©gicas.\n",
    "\n",
    "**1. Valida√ß√£o da Estrutura de Dados:**\n",
    "- **Inspe√ß√£o de colunas**: Verifica se todas as colunas esperadas foram criadas corretamente ap√≥s o processamento\n",
    "- **An√°lise das primeiras linhas**: Valida visualmente se os dados agregados est√£o consistentes e bem formatados\n",
    "- **Verifica√ß√£o de tipos**: Confirma que as m√©tricas num√©ricas est√£o nos formatos corretos para an√°lise\n",
    "\n",
    "**2. An√°lise Estat√≠stica Descritiva:**\n",
    "- **Resumo estat√≠stico**: Calcula m√©dias, medianas, quartis e desvios para todas as m√©tricas num√©ricas\n",
    "- **Identifica√ß√£o de outliers**: Detecta valores extremos que podem impactar a an√°lise de clustering\n",
    "- **Distribui√ß√£o geogr√°fica**: Verifica se h√° concentra√ß√£o excessiva em determinadas regi√µes\n",
    "\n",
    "**3. Mapeamento Geogr√°fico e Regional:**\n",
    "- **Cria√ß√£o de c√≥digos √∫nicos**: Gera identificadores sequenciais para cada localiza√ß√£o processada\n",
    "- **Classifica√ß√£o regional**: Mapeia estados para regi√µes da estrat√©gia Chilli Beans (S√£o Paulo, Sudeste, Sul, Nordeste, Centro-Oeste, Norte)\n",
    "- **Valida√ß√£o da cobertura**: Confirma presen√ßa de dados em todas as regi√µes do pa√≠s\n",
    "\n",
    "**4. An√°lise de Distribui√ß√£o por Estado:**\n",
    "- **Ranking estadual**: Identifica quais estados t√™m maior concentra√ß√£o de localiza√ß√µes processadas\n",
    "- **Balanceamento regional**: Verifica se h√° representatividade adequada de diferentes estados\n",
    "- **Identifica√ß√£o de gaps**: Detecta poss√≠veis lacunas geogr√°ficas na cobertura de dados\n",
    "\n",
    "**5. Separa√ß√£o Estrat√©gica S√£o Paulo vs Outros Estados:**\n",
    "- **Justificativa da separa√ß√£o**: S√£o Paulo possui caracter√≠sticas √∫nicas de densidade populacional e mercado\n",
    "- **Compara√ß√£o quantitativa**: Mostra diferen√ßas de volume entre SP e demais estados\n",
    "- **Prepara√ß√£o para clustering**: Valida se ambos os subconjuntos t√™m dados suficientes para an√°lise independente\n",
    "\n",
    "**6. Identifica√ß√£o de Localiza√ß√µes de Alto Potencial:**\n",
    "- **Top 5 SP**: Lista as melhores localiza√ß√µes em S√£o Paulo baseadas no score de potencial\n",
    "- **Top 5 outros estados**: Identifica oportunidades fora de S√£o Paulo\n",
    "- **M√©tricas de valida√ß√£o**: Mostra vendas de √≥culos de grau e n√∫mero de clientes para cada localiza√ß√£o\n",
    "\n",
    "**7. Valida√ß√µes de Qualidade:**\n",
    "- **Consist√™ncia de dados**: Verifica se m√©tricas agregadas fazem sentido matem√°tico\n",
    "- **Completude geogr√°fica**: Confirma que coordenadas est√£o dentro dos limites geogr√°ficos do Brasil\n",
    "- **Integridade referencial**: Valida se c√≥digos de estados correspondem aos nomes corretos\n",
    "\n",
    "**8. Prepara√ß√£o para An√°lises Avan√ßadas:**\n",
    "- **Estrutura otimizada**: Dados organizados e limpos para aplica√ß√£o de algoritmos de machine learning\n",
    "- **Contexto regional**: Informa√ß√µes geogr√°ficas enriquecidas para interpreta√ß√£o dos resultados\n",
    "- **M√©tricas padronizadas**: Indicadores harmonizados para compara√ß√£o entre diferentes regi√µes\n",
    "\n",
    "**9. Outputs de Valida√ß√£o:**\n",
    "- **Relat√≥rios estat√≠sticos**: Resumos num√©ricos para valida√ß√£o da qualidade dos dados\n",
    "- **Listas de top performers**: Identifica√ß√£o pr√©via de localiza√ß√µes promissoras\n",
    "- **Distribui√ß√µes regionais**: Vis√£o geral da cobertura geogr√°fica dos dados\n",
    "\n",
    "**10. Benef√≠cios desta Etapa:**\n",
    "- **Confiabilidade**: Garante que dados est√£o corretos antes de an√°lises complexas\n",
    "- **Interpretabilidade**: Adiciona contexto geogr√°fico e regional aos dados num√©ricos\n",
    "- **Efici√™ncia**: Identifica problemas cedo, evitando reprocessamento posterior\n",
    "- **Insights preliminares**: Fornece primeira vis√£o das oportunidades de expans√£o mais promissoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16612202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando os dados processados com coordenadas precisas\n",
    "print(\"=\"*80)\n",
    "print(\"DADOS PROCESSADOS COM COORDENADAS PRECISAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Estrutura dos dados de localiza√ß√£o:\")\n",
    "print(f\"Colunas: {dados_localizacao.columns.tolist()}\")\n",
    "print(f\"\\nPrimeiras 5 localiza√ß√µes:\")\n",
    "print(dados_localizacao.head())\n",
    "\n",
    "print(f\"\\nResumo estat√≠stico:\")\n",
    "print(dados_localizacao.describe())\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o por estado:\")\n",
    "estado_distribution = dados_localizacao['estado'].value_counts()\n",
    "print(estado_distribution)\n",
    "\n",
    "# Criando vari√°vel para c√≥digos de cidade baseada na localiza√ß√£o\n",
    "dados_localizacao['cod_cidade'] = range(len(dados_localizacao))\n",
    "\n",
    "# Adicionando informa√ß√µes regionais\n",
    "dados_localizacao['regiao_chilli'] = dados_localizacao['estado'].map({\n",
    "    'SP': 'S√ÉO PAULO',\n",
    "    'RJ': 'SUDESTE', 'MG': 'SUDESTE', 'ES': 'SUDESTE',\n",
    "    'RS': 'SUL', 'SC': 'SUL', 'PR': 'SUL',\n",
    "    'BA': 'NORDESTE', 'PE': 'NORDESTE', 'CE': 'NORDESTE', 'AL': 'NORDESTE', \n",
    "    'PB': 'NORDESTE', 'RN': 'NORDESTE', 'SE': 'NORDESTE', 'PI': 'NORDESTE', 'MA': 'NORDESTE',\n",
    "    'GO': 'CENTRO-OESTE', 'MT': 'CENTRO-OESTE', 'MS': 'CENTRO-OESTE', 'DF': 'CENTRO-OESTE',\n",
    "    'PA': 'NORTE', 'AM': 'NORTE', 'TO': 'NORTE', 'RO': 'NORTE', 'AC': 'NORTE', 'RR': 'NORTE', 'AP': 'NORTE'\n",
    "})\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o por regi√£o Chilli:\")\n",
    "regiao_distribution = dados_localizacao['regiao_chilli'].value_counts()\n",
    "print(regiao_distribution)\n",
    "\n",
    "# Separando S√£o Paulo dos outros estados (j√° feito anteriormente)\n",
    "print(f\"\\nSepara√ß√£o SP vs Outros Estados:\")\n",
    "print(f\"- SP: {len(dados_sp)} localiza√ß√µes\")\n",
    "print(f\"- Outros estados: {len(dados_outros_estados)} localiza√ß√µes\")\n",
    "\n",
    "# Top localiza√ß√µes por potencial em SP\n",
    "print(f\"\\nTop 5 localiza√ß√µes em SP por potencial:\")\n",
    "if len(dados_sp) > 0:\n",
    "    top_sp = dados_sp.nlargest(5, 'potencial_score')[['latitude', 'longitude', 'potencial_score', 'vendas_oculos_grau', 'num_clientes']]\n",
    "    print(top_sp)\n",
    "\n",
    "# Top localiza√ß√µes por potencial em outros estados\n",
    "print(f\"\\nTop 5 localiza√ß√µes em outros estados por potencial:\")\n",
    "if len(dados_outros_estados) > 0:\n",
    "    top_outros = dados_outros_estados.nlargest(5, 'potencial_score')[['estado', 'latitude', 'longitude', 'potencial_score', 'vendas_oculos_grau', 'num_clientes']]\n",
    "    print(top_outros)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce4aa1",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o Inicial dos Dados Geogr√°ficos\n",
    "\n",
    "Esta se√ß√£o cria **visualiza√ß√µes geogr√°ficas abrangentes** para explorar a distribui√ß√£o espacial das lojas Chilli Beans, destacando padr√µes de vendas de √≥culos de grau, diferen√ßas regionais e oportunidades de expans√£o atrav√©s de m√∫ltiplas perspectivas anal√≠ticas.\n",
    "\n",
    "**1. Dashboard Geogr√°fico Multidimensional:**\n",
    "- **Layout 2x2**: Organiza quatro visualiza√ß√µes complementares em uma √∫nica figura para an√°lise comparativa\n",
    "- **Coordenadas reais**: Utiliza latitude e longitude precisas para mapeamento geogr√°fico fidedigno\n",
    "- **Escalas consistentes**: Mant√©m propor√ß√µes geogr√°ficas corretas para interpreta√ß√£o espacial acurada\n",
    "\n",
    "**2. Gr√°fico 1 - Distribui√ß√£o Geral de Vendas:**\n",
    "- **Mapeamento de cores**: Representa intensidade de vendas de √≥culos de grau atrav√©s do colormap 'viridis'\n",
    "- **Tamanho proporcional**: Pontos dimensionados pelo n√∫mero de lojas (√ó10) para mostrar densidade de opera√ß√µes\n",
    "- **Identifica√ß√£o de hotspots**: Facilita visualiza√ß√£o de regi√µes com alta concentra√ß√£o de vendas\n",
    "\n",
    "**3. Gr√°fico 2 - Comparativo SP vs Outros Estados:**\n",
    "- **Segmenta√ß√£o estrat√©gica**: Diferencia S√£o Paulo (vermelho) de outros estados (azul) para an√°lise regional\n",
    "- **Tamanho por vendas**: Pontos dimensionados pelas vendas de √≥culos de grau (√ó2) para destacar performance\n",
    "- **An√°lise competitiva**: Permite compara√ß√£o direta entre as duas principais regi√µes de atua√ß√£o\n",
    "\n",
    "**4. Gr√°fico 3 - An√°lise de Receita e Base de Clientes:**\n",
    "- **Colormap de receita**: Utiliza 'plasma' para representar receita total por localiza√ß√£o\n",
    "- **Tamanho por clientes**: Pontos dimensionados pelo n√∫mero de clientes (√∑10) para mostrar penetra√ß√£o de mercado\n",
    "- **Correla√ß√£o visual**: Permite identificar rela√ß√£o entre receita e base de clientes\n",
    "\n",
    "**5. Gr√°fico 4 - Ranking Estadual:**\n",
    "- **Top 10 estados**: Identifica os maiores mercados por vendas de √≥culos de grau\n",
    "- **Destaque para SP**: Usa cor vermelha para S√£o Paulo e azul para outros estados\n",
    "- **Valores absolutos**: Exibe n√∫meros exatos de vendas sobre cada barra para precis√£o quantitativa\n",
    "\n",
    "**6. Funcionalidades Avan√ßadas de Visualiza√ß√£o:**\n",
    "- **Legendas informativas**: Explica significado de cores e tamanhos em cada gr√°fico\n",
    "- **Grid e formata√ß√£o**: Adiciona elementos visuais para melhor leitura dos dados\n",
    "- **Rota√ß√£o de labels**: Otimiza legibilidade dos nomes de estados no gr√°fico de barras\n",
    "\n",
    "**7. Exporta√ß√£o e Persist√™ncia:**\n",
    "- **Alta resolu√ß√£o**: Salva gr√°fico em 300 DPI para qualidade profissional\n",
    "- **Formato PNG**: Compat√≠vel com documentos e apresenta√ß√µes\n",
    "- **Ajuste autom√°tico**: Layout responsivo que se adapta ao conte√∫do\n",
    "\n",
    "**8. Estat√≠sticas Complementares:**\n",
    "- **Resumo quantitativo**: Fornece n√∫meros consolidados por regi√£o (SP vs outros)\n",
    "- **M√©tricas chave**: Total de cidades, vendas, receita e lojas por regi√£o\n",
    "- **Valida√ß√£o visual**: Confirma padr√µes observados nos gr√°ficos com dados num√©ricos\n",
    "\n",
    "**9. Insights Estrat√©gicos Facilitados:**\n",
    "- **Identifica√ß√£o de gaps**: Visualiza regi√µes com baixa penetra√ß√£o de mercado\n",
    "- **Oportunidades de expans√£o**: Destaca √°reas com alto potencial n√£o explorado\n",
    "- **Balanceamento regional**: Mostra distribui√ß√£o atual vs distribui√ß√£o ideal\n",
    "\n",
    "**10. Prepara√ß√£o para Clustering:**\n",
    "- **Padr√µes espaciais**: Revela agrupamentos naturais de localiza√ß√µes de alta performance\n",
    "- **Densidade geogr√°fica**: Identifica regi√µes com concentra√ß√£o adequada para clustering\n",
    "- **Valida√ß√£o de dados**: Confirma qualidade dos dados geogr√°ficos antes da an√°lise de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZA√á√ÉO INICIAL\n",
    "# Plotando distribui√ß√£o geogr√°fica das lojas\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Gr√°fico 1: Todas as localiza√ß√µes\n",
    "ax1 = axes[0, 0]\n",
    "scatter1 = ax1.scatter(dados_localizacao['longitude'], dados_localizacao['latitude'], \n",
    "                      c=dados_localizacao['vendas_oculos_grau'], \n",
    "                      s=dados_localizacao['num_lojas']*10,\n",
    "                      alpha=0.7, cmap='viridis')\n",
    "ax1.set_xlabel('Longitude (¬∞)')\n",
    "ax1.set_ylabel('Latitude (¬∞)')\n",
    "ax1.set_title('Distribui√ß√£o Geogr√°fica - Vendas de √ìculos de Grau\\n(Tamanho = N√∫mero de Lojas)')\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1, label='Vendas √ìculos de Grau')\n",
    "cbar1.set_label('Quantidade de Vendas de √ìculos de Grau', fontsize=10)\n",
    "# Adicionar legenda para tamanho dos pontos\n",
    "ax1.text(0.02, 0.98, 'Tamanho do ponto = N√∫mero de lojas', \n",
    "         transform=ax1.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Gr√°fico 2: Separando S√£o Paulo vs Outros Estados\n",
    "ax2 = axes[0, 1]\n",
    "scatter_sp = ax2.scatter(dados_sp['longitude'], dados_sp['latitude'], \n",
    "           c='red', s=dados_sp['vendas_oculos_grau']*2, alpha=0.7, label='S√£o Paulo')\n",
    "scatter_outros = ax2.scatter(dados_outros_estados['longitude'], dados_outros_estados['latitude'], \n",
    "           c='blue', s=dados_outros_estados['vendas_oculos_grau']*2, alpha=0.7, label='Outros Estados')\n",
    "ax2.set_xlabel('Longitude (¬∞)')\n",
    "ax2.set_ylabel('Latitude (¬∞)')\n",
    "ax2.set_title('Distribui√ß√£o: S√£o Paulo vs Outros Estados\\n(Tamanho = Vendas √ìculos de Grau)')\n",
    "ax2.legend(loc='upper right')\n",
    "# Adicionar explica√ß√£o do tamanho\n",
    "ax2.text(0.02, 0.02, 'Tamanho proporcional √†s vendas de √≥culos de grau', \n",
    "         transform=ax2.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Gr√°fico 3: Receita por localiza√ß√£o\n",
    "ax3 = axes[1, 0]\n",
    "scatter3 = ax3.scatter(dados_localizacao['longitude'], dados_localizacao['latitude'], \n",
    "                      c=dados_localizacao['receita_total'], \n",
    "                      s=dados_localizacao['num_clientes']/10,\n",
    "                      alpha=0.7, cmap='plasma')\n",
    "ax3.set_xlabel('Longitude (¬∞)')\n",
    "ax3.set_ylabel('Latitude (¬∞)')\n",
    "ax3.set_title('Receita Total por Localiza√ß√£o\\n(Tamanho = N√∫mero de Clientes)')\n",
    "cbar3 = plt.colorbar(scatter3, ax=ax3, label='Receita Total (R$)')\n",
    "cbar3.set_label('Receita Total (R$)', fontsize=10)\n",
    "# Adicionar legenda para tamanho dos pontos\n",
    "ax3.text(0.02, 0.98, 'Tamanho do ponto = N√∫mero de clientes √∑ 10', \n",
    "         transform=ax3.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Gr√°fico 4: Densidade de lojas por estado\n",
    "ax4 = axes[1, 1]\n",
    "densidade_estado = dados_localizacao.groupby('estado').agg({\n",
    "    'num_lojas': 'sum',\n",
    "    'vendas_oculos_grau': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Top 10 estados por vendas de √≥culos de grau\n",
    "top_estados = densidade_estado.nlargest(10, 'vendas_oculos_grau')\n",
    "bars = ax4.bar(range(len(top_estados)), top_estados['vendas_oculos_grau'], \n",
    "               color=['red' if estado == 'S√£o Paulo' else 'steelblue' for estado in top_estados['estado']])\n",
    "ax4.set_xticks(range(len(top_estados)))\n",
    "ax4.set_xticklabels(top_estados['estado'], rotation=45, ha='right')\n",
    "ax4.set_ylabel('Vendas √ìculos de Grau (unidades)')\n",
    "ax4.set_title('Top 10 Estados - Vendas de √ìculos de Grau')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (bar, valor) in enumerate(zip(bars, top_estados['vendas_oculos_grau'])):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5, \n",
    "             f'{valor}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Legenda para cores\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', label='S√£o Paulo'),\n",
    "                   Patch(facecolor='steelblue', label='Outros Estados')]\n",
    "ax4.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig((\"../../assets/gr√°ficos/estatisticas_regiao.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas descritivas\n",
    "print(\"ESTAT√çSTICAS POR REGI√ÉO:\")\n",
    "print(\"\\nS√£o Paulo:\")\n",
    "print(f\"- Total de cidades: {len(dados_sp)}\")\n",
    "print(f\"- Vendas √≥culos de grau: {dados_sp['vendas_oculos_grau'].sum()}\")\n",
    "print(f\"- Receita total: R$ {dados_sp['receita_total'].sum():,.2f}\")\n",
    "print(f\"- N√∫mero de lojas: {dados_sp['num_lojas'].sum()}\")\n",
    "\n",
    "print(\"\\nOutros Estados:\")\n",
    "print(f\"- Total de cidades: {len(dados_outros_estados)}\")\n",
    "print(f\"- Vendas √≥culos de grau: {dados_outros_estados['vendas_oculos_grau'].sum()}\")\n",
    "print(f\"- Receita total: R$ {dados_outros_estados['receita_total'].sum():,.2f}\")\n",
    "print(f\"- N√∫mero de lojas: {dados_outros_estados['num_lojas'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfc5e6",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de C√°lculo de Dist√¢ncia Geogr√°fica\n",
    "\n",
    "Esta se√ß√£o implementa **fun√ß√µes essenciais para c√°lculos de dist√¢ncia geogr√°fica** que s√£o fundamentais para a an√°lise de clustering espacial e identifica√ß√£o de localiza√ß√µes ideais para novas lojas.\n",
    "\n",
    "**1. Fun√ß√£o `haversine_distance()`:**\n",
    "- **Prop√≥sito**: Calcula a dist√¢ncia real em quil√¥metros entre dois pontos na superf√≠cie da Terra\n",
    "- **F√≥rmula Haversine**: M√©todo matem√°tico preciso que considera a curvatura da Terra\n",
    "- **Par√¢metros de entrada**: Latitude e longitude de dois pontos em graus decimais\n",
    "- **Sa√≠da**: Dist√¢ncia em quil√¥metros com alta precis√£o\n",
    "\n",
    "**2. Por que usar a F√≥rmula Haversine:**\n",
    "- **Precis√£o geogr√°fica**: Considera que a Terra √© esf√©rica, n√£o plana\n",
    "- **Aplica√ß√£o real**: Fornece dist√¢ncias \"em linha reta\" reais entre coordenadas\n",
    "- **Padr√£o da ind√∫stria**: Amplamente usado em sistemas de geolocaliza√ß√£o\n",
    "- **Efici√™ncia computacional**: R√°pida para grandes volumes de dados\n",
    "\n",
    "**3. Processo de C√°lculo Implementado:**\n",
    "- **Convers√£o para radianos**: Converte graus para radianos (unidade matem√°tica padr√£o)\n",
    "- **C√°lculo de diferen√ßas**: Determina diferen√ßas de latitude e longitude\n",
    "- **Aplica√ß√£o da f√≥rmula**: Usa fun√ß√µes trigonom√©tricas (seno, cosseno, arco-seno)\n",
    "- **Multiplica√ß√£o pelo raio**: Usa raio da Terra (6.371 km) para obter dist√¢ncia final\n",
    "\n",
    "**4. Fun√ß√£o `encontrar_cidade_mais_proxima()`:**\n",
    "- **Prop√≥sito**: Identifica qual localiza√ß√£o real est√° mais pr√≥xima de um centr√≥ide de cluster\n",
    "- **Metodologia**: Calcula dist√¢ncia Haversine entre centr√≥ide e todas as localiza√ß√µes\n",
    "- **Retorna**: A localiza√ß√£o mais pr√≥xima e a dist√¢ncia em quil√¥metros\n",
    "- **Aplica√ß√£o**: Traduz centr√≥ides abstratos em recomenda√ß√µes de locais reais\n",
    "\n",
    "**5. Import√¢ncia para o Modelo de Clustering:**\n",
    "- **Valida√ß√£o de clusters**: Permite verificar se centr√≥ides fazem sentido geograficamente\n",
    "- **Recomenda√ß√µes pr√°ticas**: Converte resultados matem√°ticos em sugest√µes acion√°veis\n",
    "- **Interpretabilidade**: Facilita comunica√ß√£o dos resultados para stakeholders\n",
    "- **Tomada de decis√£o**: Fornece dist√¢ncias concretas para planejamento log√≠stico\n",
    "\n",
    "**6. Aplica√ß√µes Espec√≠ficas no Projeto:**\n",
    "- **An√°lise de clusters SP**: Identifica cidades representativas de cada cluster em S√£o Paulo\n",
    "- **An√°lise nacional**: Encontra regi√µes ideais em outros estados\n",
    "- **Valida√ß√£o de centr√≥ides**: Confirma que locais sugeridos s√£o geograficamente vi√°veis\n",
    "- **M√©tricas de qualidade**: Mede qu√£o bem os clusters representam localiza√ß√µes reais\n",
    "\n",
    "**7. Benef√≠cios T√©cnicos:**\n",
    "- **Efici√™ncia**: Fun√ß√£o vetorizada que processa m√∫ltiplas coordenadas rapidamente\n",
    "- **Robustez**: Lida com casos extremos (coordenadas inv√°lidas, DataFrames vazios)\n",
    "- **Reutiliza√ß√£o**: Fun√ß√µes modulares que podem ser aplicadas em diferentes contextos\n",
    "- **Precis√£o**: C√°lculos matematicamente corretos para decis√µes estrat√©gicas\n",
    "\n",
    "**8. Valida√ß√µes Implementadas:**\n",
    "- **Verifica√ß√£o de dados**: Confirma que coordenadas s√£o v√°lidas antes do c√°lculo\n",
    "- **Tratamento de erros**: Lida graciosamente com dados faltantes ou inv√°lidos\n",
    "- **Otimiza√ß√£o**: Usa opera√ß√µes vetorizadas do pandas para performance\n",
    "- **Teste de consist√™ncia**: Garante que dist√¢ncias calculadas s√£o realistas\n",
    "\n",
    "**9. Output Esperado:**\n",
    "- **Dist√¢ncias precisas**: Valores em quil√¥metros para planejamento real\n",
    "- **Localiza√ß√µes identificadas**: Cidades/regi√µes espec√≠ficas para cada cluster\n",
    "- **M√©tricas de proximidade**: Qu√£o bem centr√≥ides representam dados reais\n",
    "- **Base para recomenda√ß√µes**: Funda√ß√£o s√≥lida para sugest√µes de expans√£o\n",
    "\n",
    "**10. Prepara√ß√£o para An√°lises Subsequentes:**\n",
    "- **Clustering de SP**: Identifica√ß√£o de lojas representativas por cluster\n",
    "- **Clustering nacional**: Mapeamento de regi√µes ideais por estado\n",
    "- **An√°lise de potencial**: Conex√£o entre centr√≥ides matem√°ticos e oportunidades reais\n",
    "- **Exporta√ß√£o de sugest√µes**: Convers√£o de resultados em coordenadas espec√≠ficas para o modelo supervisionado\n",
    "\n",
    "Esta implementa√ß√£o garante que todas as an√°lises geogr√°ficas subsequentes sejam baseadas em **c√°lculos matematicamente precisos e geograficamente realistas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para calcular dist√¢ncia Haversine entre dois pontos\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcula a dist√¢ncia Haversine entre dois pontos na Terra\n",
    "    \"\"\"\n",
    "    R = 6371  # Raio da Terra em km\n",
    "    \n",
    "    # Convertendo para radianos\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Diferen√ßas\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # F√≥rmula Haversine\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def encontrar_cidade_mais_proxima(centroide_lat, centroide_lon, dataframe):\n",
    "    \"\"\"\n",
    "    Encontra a cidade mais pr√≥xima de um centr√≥ide\n",
    "    \"\"\"\n",
    "    distancias = dataframe.apply(\n",
    "        lambda row: haversine_distance(centroide_lat, centroide_lon, \n",
    "                                     row['latitude'], row['longitude']), \n",
    "        axis=1\n",
    "    )\n",
    "    idx_mais_proxima = distancias.idxmin()\n",
    "    return dataframe.loc[idx_mais_proxima], distancias.min()\n",
    "\n",
    "print(\"Fun√ß√µes de dist√¢ncia criadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15aa5a6",
   "metadata": {},
   "source": [
    "## Clusteriza√ß√£o de Lojas Existentes - S√£o Paulo\n",
    "\n",
    "Esta se√ß√£o implementa a **an√°lise de clustering espec√≠fica para o estado de S√£o Paulo**, utilizando algoritmos de machine learning para identificar padr√µes geogr√°ficos e operacionais das lojas existentes, com foco em vendas de √≥culos de grau.\n",
    "\n",
    "**1. Por que An√°lise Separada para S√£o Paulo:**\n",
    "- **Densidade √∫nica**: S√£o Paulo possui concentra√ß√£o muito maior de lojas que outros estados\n",
    "- **Caracter√≠sticas espec√≠ficas**: Mercado metropolitano com din√¢micas pr√≥prias\n",
    "- **Evitar vi√©s**: Impedir que SP domine a an√°lise nacional devido ao volume\n",
    "- **Estrat√©gias diferenciadas**: Permitir recomenda√ß√µes espec√≠ficas para o maior mercado\n",
    "\n",
    "**2. Prepara√ß√£o dos Dados:**\n",
    "- **Dataset focado**: Filtra apenas localiza√ß√µes do estado de S√£o Paulo\n",
    "- **Matriz de coordenadas**: Cria `X_sp` com longitude e latitude para algoritmos espaciais\n",
    "- **Pesos estrat√©gicos**: Usa `vendas_oculos_grau + 1` como peso para cada localiza√ß√£o\n",
    "- **Evita bias zero**: Adiciona 1 para garantir que todas as localiza√ß√µes tenham peso m√≠nimo\n",
    "\n",
    "**3. Metodologia de Teste de Clusters:**\n",
    "- **Range de k**: Testa de 1 a 6 clusters para encontrar configura√ß√£o √≥tima\n",
    "- **Algoritmo K-Means**: Usa coordenadas geogr√°ficas como features principais\n",
    "- **Sample weights**: Prioriza localiza√ß√µes com mais vendas de √≥culos de grau\n",
    "- **Par√¢metros fixos**: `random_state=42` garante reprodutibilidade\n",
    "\n",
    "**4. Layout de Visualiza√ß√£o (Grid 2x3 + Grid 1x2):**\n",
    "- **Primeira figura**: 6 gr√°ficos de clustering organizados em 2 linhas √ó 3 colunas\n",
    "- **Segunda figura**: An√°lises de qualidade lado a lado (Elbow + Silhouette)\n",
    "- **Consist√™ncia visual**: Mesmo padr√£o usado para an√°lise de outros estados\n",
    "- **Tamanhos otimizados**: 20√ó12 para clustering, 16√ó6 para an√°lises\n",
    "\n",
    "**5. M√©tricas de Qualidade Implementadas:**\n",
    "- **In√©rcia (WCSS)**: Mede compacta√ß√£o dos clusters (menor = melhor)\n",
    "- **Silhouette Score**: Avalia separa√ß√£o entre clusters (maior = melhor)\n",
    "- **Valida√ß√£o m√∫ltipla**: Compara diferentes valores de k para otimiza√ß√£o\n",
    "- **Tratamento de exce√ß√µes**: Lida com casos onde silhouette n√£o pode ser calculado\n",
    "\n",
    "**6. Identifica√ß√£o de Lojas Representativas:**\n",
    "- **Fun√ß√£o Haversine**: Calcula dist√¢ncias reais entre centr√≥ides e lojas\n",
    "- **Loja mais pr√≥xima**: Identifica localiza√ß√£o real mais pr√≥xima de cada centr√≥ide\n",
    "- **M√©tricas contextuais**: Mostra vendas e n√∫mero de lojas para cada representante\n",
    "- **Interpretabilidade**: Converte centr√≥ides abstratos em recomenda√ß√µes pr√°ticas\n",
    "\n",
    "**7. An√°lises Detalhadas:**\n",
    "- **k=3**: Configura√ß√£o intermedi√°ria com an√°lise de clusters representativos\n",
    "- **k=6**: Configura√ß√£o mais granular para segmenta√ß√£o detalhada\n",
    "- **Tabela comparativa**: Resume todos os valores de k com interpreta√ß√µes\n",
    "- **Melhor k identificado**: Baseado no maior Silhouette Score\n",
    "\n",
    "**8. Outputs Gerados:**\n",
    "- **clusters_sp.png**: Visualiza√ß√£o dos 6 gr√°ficos de clustering\n",
    "- **analise_clusters_sp.png**: Gr√°ficos de Elbow e Silhouette lado a lado\n",
    "- **Estat√≠sticas consolidadas**: Vendas, lojas, receita e melhor configura√ß√£o\n",
    "- **Recomenda√ß√µes espec√≠ficas**: Clusters representativos para cada k\n",
    "\n",
    "**9. Interpreta√ß√£o dos Resultados:**\n",
    "- **Clusters identificados**: Agrupamentos naturais de lojas em SP\n",
    "- **Representantes**: Localiza√ß√µes-chave que exemplificam cada cluster\n",
    "- **Qualidade validada**: Silhouette Score confirma separa√ß√£o adequada\n",
    "- **Base para estrat√©gia**: Insights espec√≠ficos para mercado paulista\n",
    "\n",
    "**10. Integra√ß√£o com An√°lise Nacional:**\n",
    "- **Compara√ß√£o facilitada**: Layout id√™ntico ao de outros estados\n",
    "- **Estrat√©gias complementares**: SP requer abordagem diferenciada\n",
    "- **Dados consolidados**: Contribui para vis√£o geral do modelo\n",
    "- **Pr√≥ximas etapas**: Prepara para an√°lise de expans√£o direcionada\n",
    "\n",
    "Esta an√°lise garante que as **particularidades do mercado paulista** sejam adequadamente capturadas e que as recomenda√ß√µes de expans√£o considerem as din√¢micas espec√≠ficas da regi√£o mais importante para a Chilli Beans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERIZA√á√ÉO DE LOJAS EXISTENTES - S√ÉO PAULO\n",
    "print(\"\\n=== AN√ÅLISE DE LOJAS EXISTENTES - S√ÉO PAULO ===\")\n",
    "\n",
    "# Preparando dados de SP para clusteriza√ß√£o\n",
    "X_sp = dados_sp[['longitude', 'latitude']].values\n",
    "weights_sp = dados_sp['vendas_oculos_grau'].values + 1\n",
    "\n",
    "# Testando diferentes n√∫meros de clusters (1 a 6) - ATUALIZADO PARA CONSIST√äNCIA\n",
    "k_values_sp = range(1, 7)\n",
    "inertias_sp = []\n",
    "silhouette_scores_sp = []\n",
    "\n",
    "# Primeira figura: Grid 2x3 para os 6 gr√°ficos de clustering\n",
    "fig1, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig1.suptitle('Clusteriza√ß√£o de Lojas Existentes - S√£o Paulo (K=1 a K=6)', fontsize=16)\n",
    "\n",
    "for i, k in enumerate(k_values_sp):\n",
    "    # KMeans com peso das vendas de √≥culos de grau\n",
    "    kmeans_sp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters_sp = kmeans_sp.fit_predict(X_sp, sample_weight=weights_sp)\n",
    "    \n",
    "    # M√©tricas\n",
    "    inertias_sp.append(kmeans_sp.inertia_)\n",
    "    if k > 1 and len(np.unique(clusters_sp)) > 1:\n",
    "        try:\n",
    "            silhouette_avg = silhouette_score(X_sp, clusters_sp)\n",
    "            silhouette_scores_sp.append(silhouette_avg)\n",
    "        except:\n",
    "            silhouette_scores_sp.append(0)\n",
    "    elif k > 1:\n",
    "        silhouette_scores_sp.append(0)\n",
    "    \n",
    "    # Plotando clusters em grid 2x3\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    scatter = ax.scatter(dados_sp['longitude'], dados_sp['latitude'], \n",
    "                       c=clusters_sp, s=dados_sp['vendas_oculos_grau']*2, \n",
    "                       alpha=0.7, cmap='tab10')\n",
    "    \n",
    "    # Plotando centr√≥ides\n",
    "    centroids = kmeans_sp.cluster_centers_\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], \n",
    "              c='red', marker='x', s=200, linewidths=3, label='Centr√≥ides')\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title(f'K = {k} clusters')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Encontrando lojas representativas de cada cluster para k=3\n",
    "    if k == 3:\n",
    "        print(f\"\\nDetalhamento para K = {k} (clusters representativos):\")\n",
    "        for j, (cent_lon, cent_lat) in enumerate(centroids):\n",
    "            loja_proxima, distancia = encontrar_cidade_mais_proxima(cent_lat, cent_lon, dados_sp)\n",
    "            print(f\"  Cluster {j+1}: {loja_proxima['estado']} (Lat: {loja_proxima['latitude']:.2f}, Lon: {loja_proxima['longitude']:.2f}) - Dist√¢ncia: {distancia:.2f} km\")\n",
    "            print(f\"    Vendas √≥culos de grau: {loja_proxima['vendas_oculos_grau']}, Lojas: {loja_proxima['num_lojas']}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../assets/gr√°ficos/clusters_sp.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Segunda figura: Grid 1x2 para Elbow e Silhouette lado a lado\n",
    "fig2, (ax_elbow, ax_silhouette) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig2.suptitle('An√°lise de Qualidade dos Clusters - S√£o Paulo', fontsize=16)\n",
    "\n",
    "# Gr√°fico da curva do cotovelo\n",
    "ax_elbow.plot(k_values_sp, inertias_sp, 'bo-', linewidth=2, markersize=8)\n",
    "ax_elbow.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax_elbow.set_ylabel('In√©rcia (WCSS)')\n",
    "ax_elbow.set_title('Curva do Cotovelo')\n",
    "ax_elbow.grid(True, alpha=0.3)\n",
    "# Adicionar anota√ß√£o para identificar o cotovelo\n",
    "ax_elbow.text(0.02, 0.98, 'Procure o \"cotovelo\" onde\\na in√©rcia para de diminuir\\nsignificativamente', \n",
    "             transform=ax_elbow.transAxes, fontsize=10, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "\n",
    "# Gr√°fico de Silhouette Score\n",
    "if silhouette_scores_sp:\n",
    "    ax_silhouette.plot(k_values_sp[1:], silhouette_scores_sp, 'ro-', linewidth=2, markersize=8)\n",
    "    ax_silhouette.set_xlabel('N√∫mero de Clusters (k)')\n",
    "    ax_silhouette.set_ylabel('Silhouette Score')\n",
    "    ax_silhouette.set_title('Silhouette Score')\n",
    "    ax_silhouette.grid(True, alpha=0.3)\n",
    "    ax_silhouette.set_ylim(0, max(silhouette_scores_sp) + 0.1)\n",
    "    \n",
    "    # Adicionar linha de refer√™ncia para qualidade boa (>0.5)\n",
    "    ax_silhouette.axhline(y=0.5, color='green', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    ax_silhouette.text(0.02, 0.98, 'Score > 0.5 indica\\nboa separa√ß√£o\\ndos clusters', \n",
    "                      transform=ax_silhouette.transAxes, fontsize=10, \n",
    "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8),\n",
    "                      verticalalignment='top')\n",
    "    \n",
    "    best_k_sp = k_values_sp[1:][np.argmax(silhouette_scores_sp)]\n",
    "    ax_silhouette.scatter([best_k_sp], [max(silhouette_scores_sp)], \n",
    "                         color='red', s=100, zorder=5, label=f'Melhor k = {best_k_sp}')\n",
    "    ax_silhouette.legend()\n",
    "    \n",
    "    print(f\"\\nMelhor Silhouette Score S√£o Paulo: {max(silhouette_scores_sp):.3f} com k = {best_k_sp}\")\n",
    "else:\n",
    "    ax_silhouette.text(0.5, 0.5, 'Silhouette Score\\nn√£o calculado', \n",
    "                      ha='center', va='center', transform=ax_silhouette.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../assets/gr√°ficos/analise_clusters_sp.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# An√°lise adicional para k=6\n",
    "if len(k_values_sp) >= 6:\n",
    "    print(f\"\\nAn√°lise detalhada para K = 6:\")\n",
    "    kmeans_6 = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "    clusters_6 = kmeans_6.fit_predict(X_sp, sample_weight=weights_sp)\n",
    "    centroids_6 = kmeans_6.cluster_centers_\n",
    "    \n",
    "    for j, (cent_lon, cent_lat) in enumerate(centroids_6):\n",
    "        loja_proxima, distancia = encontrar_cidade_mais_proxima(cent_lat, cent_lon, dados_sp)\n",
    "        print(f\"  Cluster {j+1}: {loja_proxima['estado']} (Lat: {loja_proxima['latitude']:.2f}, Lon: {loja_proxima['longitude']:.2f}) - Dist√¢ncia: {distancia:.2f} km\")\n",
    "        print(f\"    Vendas √≥culos de grau: {loja_proxima['vendas_oculos_grau']}, Lojas: {loja_proxima['num_lojas']}\")\n",
    "\n",
    "print(f\"\\nAn√°lise das lojas em S√£o Paulo:\")\n",
    "print(f\"- S√£o Paulo concentra {dados_sp['vendas_oculos_grau'].sum()} vendas de √≥culos de grau\")\n",
    "print(f\"- Distribu√≠das em {dados_sp['num_lojas'].sum()} lojas\")\n",
    "print(f\"- Receita total: R$ {dados_sp['receita_total'].sum():.2f}\")\n",
    "print(f\"- Melhor configura√ß√£o: {best_k_sp} clusters regionais\")\n",
    "\n",
    "# Tabela comparativa dos diferentes valores de k\n",
    "print(f\"\\nüìä COMPARA√á√ÉO DOS DIFERENTES VALORES DE K:\")\n",
    "print(f\"{'K':<3} {'In√©rcia':<12} {'Silhouette':<12} {'Interpreta√ß√£o'}\")\n",
    "print(f\"-\" * 50)\n",
    "for i, k in enumerate(k_values_sp):\n",
    "    inercia = inertias_sp[i]\n",
    "    silhouette = silhouette_scores_sp[i-1] if i > 0 and i-1 < len(silhouette_scores_sp) else \"N/A\"\n",
    "    \n",
    "    if k == 1:\n",
    "        interpretacao = \"Sem clusters\"\n",
    "    elif k == best_k_sp:\n",
    "        interpretacao = \"‚≠ê √ìTIMO\"\n",
    "    elif isinstance(silhouette, float) and silhouette > 0.5:\n",
    "        interpretacao = \"Boa qualidade\"\n",
    "    elif isinstance(silhouette, float) and silhouette > 0.3:\n",
    "        interpretacao = \"Qualidade m√©dia\"\n",
    "    else:\n",
    "        interpretacao = \"Baixa qualidade\"\n",
    "    \n",
    "    silhouette_str = f\"{silhouette:.3f}\" if isinstance(silhouette, float) else str(silhouette)\n",
    "    print(f\"{k:<3} {inercia:<12.2f} {silhouette_str:<12} {interpretacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413f208",
   "metadata": {},
   "source": [
    "## Clusteriza√ß√£o de Lojas Existentes - Outros Estados\n",
    "\n",
    "Esta se√ß√£o implementa a **an√°lise de clustering para todos os estados brasileiros exceto S√£o Paulo**, aplicando machine learning para identificar padr√µes regionais e oportunidades de otimiza√ß√£o da rede de lojas existente da Chilli Beans.\n",
    "\n",
    "**1. Escopo da An√°lise - Brasil Menos S√£o Paulo:**\n",
    "- **Abrang√™ncia nacional**: Inclui todos os 26 estados + DF, exceto S√£o Paulo\n",
    "- **Diversidade geogr√°fica**: Contempla diferentes regi√µes, climas e culturas\n",
    "- **Heterogeneidade econ√¥mica**: Mercados com diferentes potenciais e caracter√≠sticas\n",
    "- **Complementaridade**: An√°lise que se soma √† espec√≠fica de S√£o Paulo\n",
    "\n",
    "**2. Prepara√ß√£o dos Dados Regionais:**\n",
    "- **Dataset nacional**: `dados_outros_estados` cont√©m todas as localiza√ß√µes n√£o-SP\n",
    "- **Matriz espacial**: `X_outros` com coordenadas longitude/latitude\n",
    "- **Pondera√ß√£o estrat√©gica**: `vendas_oculos_grau + 1` como peso de cada localiza√ß√£o\n",
    "- **Normaliza√ß√£o**: Garante que localiza√ß√µes com vendas zero ainda sejam consideradas\n",
    "\n",
    "**3. Metodologia de Clustering Abrangente:**\n",
    "- **Range expandido**: Testa k=1 a k=6 para maior granularidade\n",
    "- **Algoritmo robusto**: K-Means com sample weights para considerar performance\n",
    "- **Valida√ß√£o dupla**: In√©rcia (compacta√ß√£o) + Silhouette (separa√ß√£o)\n",
    "- **Reprodutibilidade**: `random_state=42` para resultados consistentes\n",
    "\n",
    "**4. Estrutura de Visualiza√ß√£o Padronizada:**\n",
    "- **Grid 2√ó3**: Seis gr√°ficos de clustering para compara√ß√£o visual direta\n",
    "- **Grid 1√ó2 separado**: Elbow e Silhouette lado a lado para an√°lise de qualidade\n",
    "- **Layout consistente**: Id√™ntico √† an√°lise de S√£o Paulo para facilitar compara√ß√£o\n",
    "- **Tamanhos otimizados**: 20√ó12 e 16√ó6 para visualiza√ß√£o clara\n",
    "\n",
    "**5. An√°lise de Qualidade dos Clusters:**\n",
    "- **Curva do Cotovelo**: Identifica ponto √≥timo onde in√©rcia estabiliza\n",
    "- **Silhouette Score**: Mede qualidade da separa√ß√£o (>0.5 = boa qualidade)\n",
    "- **Linha de refer√™ncia**: Marca visual no gr√°fico para interpreta√ß√£o f√°cil\n",
    "- **Melhor k destacado**: Identifica√ß√£o autom√°tica da configura√ß√£o √≥tima\n",
    "\n",
    "**6. Identifica√ß√£o de Regi√µes Representativas:**\n",
    "- **Centr√≥ides calculados**: Pontos m√©dios matem√°ticos de cada cluster\n",
    "- **Mapeamento para realidade**: Fun√ß√£o Haversine encontra cidade mais pr√≥xima\n",
    "- **Contexto regional**: Cada cluster representado por localiza√ß√£o real espec√≠fica\n",
    "- **M√©tricas de proximidade**: Dist√¢ncia em km entre centr√≥ide e representante\n",
    "\n",
    "**7. An√°lises Detalhadas Multi-k:**\n",
    "- **k=4**: Configura√ß√£o intermedi√°ria com an√°lise detalhada de representantes\n",
    "- **k=6**: M√°xima granularidade para segmenta√ß√£o regional espec√≠fica\n",
    "- **Tabela comparativa completa**: Resumo de in√©rcia, silhouette e interpreta√ß√£o\n",
    "- **Classifica√ß√£o qualitativa**: √ìtimo, boa, m√©dia ou baixa qualidade\n",
    "\n",
    "**8. Outputs Informativos:**\n",
    "- **clusters_br.png**: Visualiza√ß√£o dos 6 gr√°ficos de clustering nacional\n",
    "- **analise_clusters_br.png**: Gr√°ficos de qualidade lado a lado\n",
    "- **Estat√≠sticas consolidadas**: Total de vendas, lojas e receita nacional\n",
    "- **Configura√ß√£o recomendada**: Melhor k identificado automaticamente\n",
    "\n",
    "**9. Insights Regionais Estrat√©gicos:**\n",
    "- **Padr√µes geogr√°ficos**: Clusters naturais baseados em proximidade e performance\n",
    "- **Representatividade**: Cada cluster tem uma \"capital\" identificada\n",
    "- **Diversidade capturada**: Diferentes regi√µes com caracter√≠sticas √∫nicas\n",
    "- **Base para expans√£o**: Identifica gaps e oportunidades por regi√£o\n",
    "\n",
    "**10. Integra√ß√£o com Modelo Global:**\n",
    "- **Complementa S√£o Paulo**: Vis√£o nacional completa quando combinado\n",
    "- **Consist√™ncia metodol√≥gica**: Mesma abordagem, diferentes escopos\n",
    "- **Dados para supervisionado**: Clusters informam pr√≥ximas etapas do modelo\n",
    "- **Estrat√©gia diferenciada**: Permite abordagens espec√≠ficas por regi√£o\n",
    "\n",
    "**11. Valida√ß√£o e Robustez:**\n",
    "- **M√∫ltiplas m√©tricas**: In√©rcia + Silhouette para valida√ß√£o cruzada\n",
    "- **Tratamento de exce√ß√µes**: C√≥digo robusto para casos extremos\n",
    "- **Interpretabilidade**: Resultados traduzidos em recomenda√ß√µes pr√°ticas\n",
    "- **Reprodutibilidade**: Par√¢metros fixos garantem consist√™ncia\n",
    "\n",
    "**12. Prepara√ß√£o para Pr√≥ximas Etapas:**\n",
    "- **Clustering √≥timo identificado**: Base s√≥lida para an√°lise de expans√£o\n",
    "- **Regi√µes mapeadas**: Cada cluster representa oportunidade espec√≠fica\n",
    "- **Dados estruturados**: Prontos para algoritmos supervisionados\n",
    "- **Insights acion√°veis**: Recomenda√ß√µes diretas para tomada de decis√£o\n",
    "\n",
    "Esta an√°lise fornece uma **vis√£o abrangente e sistem√°tica** do mercado nacional (exceto SP), identificando padr√µes regionais naturais e oportunidades de otimiza√ß√£o que complementam perfeitamente a an√°lise espec√≠fica de S√£o Paulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERIZA√á√ÉO DE LOJAS EXISTENTES - OUTROS ESTADOS\n",
    "print(\"\\n=== AN√ÅLISE DE LOJAS EXISTENTES - OUTROS ESTADOS ===\")\n",
    "\n",
    "# Preparando dados de outros estados para clusteriza√ß√£o\n",
    "X_outros = dados_outros_estados[['longitude', 'latitude']].values\n",
    "weights_outros = dados_outros_estados['vendas_oculos_grau'].values + 1\n",
    "\n",
    "# Testando diferentes n√∫meros de clusters (1 a 6)\n",
    "k_values_outros = range(1, 7)\n",
    "inertias_outros = []\n",
    "silhouette_scores_outros = []\n",
    "\n",
    "# Primeira figura: Grid 2x3 para os 6 gr√°ficos de clustering\n",
    "fig1, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig1.suptitle('Clusteriza√ß√£o de Lojas Existentes - Outros Estados (K=1 a K=6)', fontsize=16)\n",
    "\n",
    "for i, k in enumerate(k_values_outros):\n",
    "    # KMeans com peso das vendas de √≥culos de grau\n",
    "    kmeans_outros = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters_outros = kmeans_outros.fit_predict(X_outros, sample_weight=weights_outros)\n",
    "    \n",
    "    # M√©tricas\n",
    "    inertias_outros.append(kmeans_outros.inertia_)\n",
    "    if k > 1 and len(np.unique(clusters_outros)) > 1:\n",
    "        try:\n",
    "            silhouette_avg = silhouette_score(X_outros, clusters_outros)\n",
    "            silhouette_scores_outros.append(silhouette_avg)\n",
    "        except:\n",
    "            silhouette_scores_outros.append(0)\n",
    "    elif k > 1:\n",
    "        silhouette_scores_outros.append(0)\n",
    "    \n",
    "    # Plotando clusters em grid 2x3\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    scatter = ax.scatter(dados_outros_estados['longitude'], dados_outros_estados['latitude'], \n",
    "                       c=clusters_outros, s=dados_outros_estados['vendas_oculos_grau']*0.5, \n",
    "                       alpha=0.7, cmap='tab10')\n",
    "    \n",
    "    # Plotando centr√≥ides\n",
    "    centroids = kmeans_outros.cluster_centers_\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], \n",
    "              c='red', marker='x', s=200, linewidths=3, label='Centr√≥ides')\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title(f'K = {k} clusters')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Encontrando regi√µes representativas de cada cluster para k=4\n",
    "    if k == 4:\n",
    "        print(f\"\\nDetalhamento para K = {k} (clusters representativos):\")\n",
    "        for j, (cent_lon, cent_lat) in enumerate(centroids):\n",
    "            regiao_proxima, distancia = encontrar_cidade_mais_proxima(cent_lat, cent_lon, dados_outros_estados)\n",
    "            print(f\"  Cluster {j+1}: {regiao_proxima['estado']} (Lat: {regiao_proxima['latitude']:.2f}, Lon: {regiao_proxima['longitude']:.2f}) - Dist√¢ncia: {distancia:.2f} km\")\n",
    "            print(f\"    Vendas √≥culos de grau: {regiao_proxima['vendas_oculos_grau']}, Lojas: {regiao_proxima['num_lojas']}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../assets/gr√°ficos/clusters_br.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Segunda figura: Grid 1x2 para Elbow e Silhouette lado a lado\n",
    "fig2, (ax_elbow, ax_silhouette) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig2.suptitle('An√°lise de Qualidade dos Clusters - Outros Estados', fontsize=16)\n",
    "\n",
    "# Gr√°fico da curva do cotovelo\n",
    "ax_elbow.plot(k_values_outros, inertias_outros, 'bo-', linewidth=2, markersize=8)\n",
    "ax_elbow.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax_elbow.set_ylabel('In√©rcia (WCSS)')\n",
    "ax_elbow.set_title('Curva do Cotovelo')\n",
    "ax_elbow.grid(True, alpha=0.3)\n",
    "# Adicionar anota√ß√£o para identificar o cotovelo\n",
    "ax_elbow.text(0.02, 0.98, 'Procure o \"cotovelo\" onde\\na in√©rcia para de diminuir\\nsignificativamente', \n",
    "             transform=ax_elbow.transAxes, fontsize=10, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "\n",
    "# Gr√°fico de Silhouette Score\n",
    "if silhouette_scores_outros:\n",
    "    ax_silhouette.plot(k_values_outros[1:], silhouette_scores_outros, 'ro-', linewidth=2, markersize=8)\n",
    "    ax_silhouette.set_xlabel('N√∫mero de Clusters (k)')\n",
    "    ax_silhouette.set_ylabel('Silhouette Score')\n",
    "    ax_silhouette.set_title('Silhouette Score')\n",
    "    ax_silhouette.grid(True, alpha=0.3)\n",
    "    ax_silhouette.set_ylim(0, max(silhouette_scores_outros) + 0.1)\n",
    "    \n",
    "    # Adicionar linha de refer√™ncia para qualidade boa (>0.5)\n",
    "    ax_silhouette.axhline(y=0.5, color='green', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    ax_silhouette.text(0.02, 0.98, 'Score > 0.5 indica\\nboa separa√ß√£o\\ndos clusters', \n",
    "                      transform=ax_silhouette.transAxes, fontsize=10, \n",
    "                      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8),\n",
    "                      verticalalignment='top')\n",
    "    \n",
    "    best_k_outros = k_values_outros[1:][np.argmax(silhouette_scores_outros)]\n",
    "    ax_silhouette.scatter([best_k_outros], [max(silhouette_scores_outros)], \n",
    "                         color='red', s=100, zorder=5, label=f'Melhor k = {best_k_outros}')\n",
    "    ax_silhouette.legend()\n",
    "    \n",
    "    print(f\"\\nMelhor Silhouette Score Outros Estados: {max(silhouette_scores_outros):.3f} com k = {best_k_outros}\")\n",
    "else:\n",
    "    ax_silhouette.text(0.5, 0.5, 'Silhouette Score\\nn√£o calculado', \n",
    "                      ha='center', va='center', transform=ax_silhouette.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../assets/gr√°ficos/analise_clusters_br.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# An√°lise adicional para k=6\n",
    "if len(k_values_outros) >= 6:\n",
    "    print(f\"\\nAn√°lise detalhada para K = 6:\")\n",
    "    kmeans_6 = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "    clusters_6 = kmeans_6.fit_predict(X_outros, sample_weight=weights_outros)\n",
    "    centroids_6 = kmeans_6.cluster_centers_\n",
    "    \n",
    "    for j, (cent_lon, cent_lat) in enumerate(centroids_6):\n",
    "        regiao_proxima, distancia = encontrar_cidade_mais_proxima(cent_lat, cent_lon, dados_outros_estados)\n",
    "        print(f\"  Cluster {j+1}: {regiao_proxima['estado']} (Lat: {regiao_proxima['latitude']:.2f}, Lon: {regiao_proxima['longitude']:.2f}) - Dist√¢ncia: {distancia:.2f} km\")\n",
    "        print(f\"    Vendas √≥culos de grau: {regiao_proxima['vendas_oculos_grau']}, Lojas: {regiao_proxima['num_lojas']}\")\n",
    "\n",
    "print(f\"\\nAn√°lise das lojas nos outros estados:\")\n",
    "print(f\"- Outros estados concentram {dados_outros_estados['vendas_oculos_grau'].sum()} vendas de √≥culos de grau\")\n",
    "print(f\"- Distribu√≠das em {dados_outros_estados['num_lojas'].sum()} lojas\")\n",
    "print(f\"- Receita total: R$ {dados_outros_estados['receita_total'].sum():.2f}\")\n",
    "print(f\"- Melhor configura√ß√£o: {best_k_outros} clusters regionais\")\n",
    "\n",
    "# Tabela comparativa dos diferentes valores de k\n",
    "print(f\"\\nüìä COMPARA√á√ÉO DOS DIFERENTES VALORES DE K:\")\n",
    "print(f\"{'K':<3} {'In√©rcia':<12} {'Silhouette':<12} {'Interpreta√ß√£o'}\")\n",
    "print(f\"-\" * 50)\n",
    "for i, k in enumerate(k_values_outros):\n",
    "    inercia = inertias_outros[i]\n",
    "    silhouette = silhouette_scores_outros[i-1] if i > 0 and i-1 < len(silhouette_scores_outros) else \"N/A\"\n",
    "    \n",
    "    if k == 1:\n",
    "        interpretacao = \"Sem clusters\"\n",
    "    elif k == best_k_outros:\n",
    "        interpretacao = \"‚≠ê √ìTIMO\"\n",
    "    elif isinstance(silhouette, float) and silhouette > 0.5:\n",
    "        interpretacao = \"Boa qualidade\"\n",
    "    elif isinstance(silhouette, float) and silhouette > 0.3:\n",
    "        interpretacao = \"Qualidade m√©dia\"\n",
    "    else:\n",
    "        interpretacao = \"Baixa qualidade\"\n",
    "    \n",
    "    silhouette_str = f\"{silhouette:.3f}\" if isinstance(silhouette, float) else str(silhouette)\n",
    "    print(f\"{k:<3} {inercia:<12.2f} {silhouette_str:<12} {interpretacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c35516",
   "metadata": {},
   "source": [
    "## Identifica√ß√£o de Locais Ideais para Novas Lojas\n",
    "\n",
    "Esta se√ß√£o representa o **n√∫cleo estrat√©gico do modelo n√£o supervisionado**, onde os insights das an√°lises anteriores s√£o consolidados para identificar localiza√ß√µes espec√≠ficas e acion√°veis para expans√£o da rede Chilli Beans, com foco particular em √≥culos de grau.\n",
    "\n",
    "**1. Cria√ß√£o do Score de Potencial Multicrit√©rio:**\n",
    "- **F√≥rmula ponderada estrat√©gica**: Combina 4 m√©tricas essenciais com pesos otimizados\n",
    "- **40% vendas de √≥culos de grau**: Prioridade m√°xima para o produto-foco da an√°lise\n",
    "- **30% n√∫mero de clientes**: Indica penetra√ß√£o e potencial de mercado\n",
    "- **20% receita total**: Representa poder de compra da regi√£o\n",
    "- **10% n√∫mero de lojas**: Identifica satura√ß√£o vs oportunidade (peso menor para n√£o penalizar regi√µes sub-exploradas)\n",
    "\n",
    "**2. Estrat√©gia de Separa√ß√£o Regional Mantida:**\n",
    "- **S√£o Paulo isolado**: Continua an√°lise independente devido √†s caracter√≠sticas √∫nicas\n",
    "- **Outros estados priorizados**: Foco principal na an√°lise de expans√£o para evitar vi√©s metropolitano\n",
    "- **Complementaridade**: Ambas as an√°lises juntas fornecem vis√£o completa nacional\n",
    "\n",
    "**3. Algoritmo K-Means Aplicado para Expans√£o:**\n",
    "- **k=5 clusters**: Configura√ß√£o √≥tima baseada nas an√°lises de silhouette anteriores\n",
    "- **Coordenadas como features**: Longitude e latitude para clustering espacial\n",
    "- **Sample weights**: Prioriza regi√µes com maior score de potencial\n",
    "- **Centr√≥ides como locais ideais**: Pontos matem√°ticos √≥timos traduzidos em recomenda√ß√µes geogr√°ficas\n",
    "\n",
    "**4. Dashboard Visual Multidimensional (2√ó2):**\n",
    "- **Gr√°fico 1 - Clusters por Potencial**: Visualiza agrupamentos com tamanho proporcional ao score\n",
    "- **Gr√°fico 2 - Vendas de √ìculos de Grau**: Foca especificamente no produto-alvo da an√°lise\n",
    "- **Gr√°fico 3 - Base de Clientes vs Receita**: Correlaciona penetra√ß√£o de mercado com poder de compra\n",
    "- **Gr√°fico 4 - Ranking de Clusters**: Identifica hierarquia de prioridades com escala de cores\n",
    "\n",
    "**5. An√°lise Estat√≠stica dos Clusters:**\n",
    "- **M√©tricas agregadas**: Soma e m√©dia de indicadores por cluster\n",
    "- **Centr√≥ides precisos**: Coordenadas exatas dos pontos ideais para novas lojas\n",
    "- **Ranking por potencial**: Ordena√ß√£o autom√°tica dos clusters por atratividade\n",
    "- **Estat√≠sticas descritivas**: N√∫mero de regi√µes, vendas totais, clientes e receita por cluster\n",
    "\n",
    "**6. Sistema de Recomenda√ß√µes Hier√°rquicas:**\n",
    "- **Prioridade por cluster**: Do maior ao menor potencial m√©dio\n",
    "- **Top 3 regi√µes por cluster**: Localiza√ß√µes espec√≠ficas mais promissoras dentro de cada agrupamento\n",
    "- **Dist√¢ncia dos centr√≥ides**: M√©tricas Haversine para validar proximidade geogr√°fica\n",
    "- **Contextualiza√ß√£o detalhada**: Score, vendas, lojas atuais e base de clientes para cada recomenda√ß√£o\n",
    "\n",
    "**7. Exporta√ß√£o de Insights Acion√°veis:**\n",
    "- **Coordenadas espec√≠ficas**: Latitude e longitude para cada local recomendado\n",
    "- **M√©tricas de potencial**: Scores quantitativos para prioriza√ß√£o de investimentos\n",
    "- **Valida√ß√£o geogr√°fica**: Dist√¢ncias reais em quil√¥metros para planejamento log√≠stico\n",
    "- **Resumo executivo**: Tabela consolidada com ranking de clusters\n",
    "\n",
    "**8. Integra√ß√£o com Modelo Supervisionado:**\n",
    "- **Base s√≥lida**: Recomenda√ß√µes fundamentadas em dados reais e algoritmos validados\n",
    "- **Coordenadas precisas**: Input direto para modelos preditivos subsequentes\n",
    "- **M√©tricas padronizadas**: Indicadores consistentes para compara√ß√£o e valida√ß√£o\n",
    "- **Escalabilidade**: Framework replic√°vel para futuras expans√µes\n",
    "\n",
    "**9. Valida√ß√µes de Qualidade Implementadas:**\n",
    "- **Centr√≥ides geograficamente vi√°veis**: Pontos dentro do territ√≥rio nacional\n",
    "- **Diversidade regional**: Clusters distribu√≠dos em diferentes estados/regi√µes\n",
    "- **Consist√™ncia estat√≠stica**: Scores de potencial coerentes com dados hist√≥ricos\n",
    "- **Interpretabilidade**: Resultados traduz√≠veis em decis√µes estrat√©gicas\n",
    "\n",
    "**10. Outputs Estrat√©gicos Gerados:**\n",
    "- **Arquivo de visualiza√ß√£o**: `potencial_clusters.png` com dashboard completo\n",
    "- **Lista priorizada**: Ranking de 5 clusters com recomenda√ß√µes espec√≠ficas\n",
    "- **Coordenadas acion√°veis**: Localiza√ß√µes precisas para prospec√ß√£o imediata\n",
    "- **M√©tricas de valida√ß√£o**: Estat√≠sticas que justificam cada recomenda√ß√£o\n",
    "\n",
    "**11. Benef√≠cios para Tomada de Decis√£o:**\n",
    "- **Redu√ß√£o de riscos**: Recomenda√ß√µes baseadas em dados hist√≥ricos reais\n",
    "- **Otimiza√ß√£o de investimentos**: Prioriza√ß√£o clara dos locais mais promissores\n",
    "- **Escalabilidade**: Metodologia replic√°vel para expans√µes futuras\n",
    "- **ROI maximizado**: Foco em regi√µes com maior potencial de retorno\n",
    "\n",
    "**12. Prepara√ß√£o para Implementa√ß√£o:**\n",
    "- **Coordenadas espec√≠ficas**: Prontas para sistemas de GPS e mapeamento\n",
    "- **Contexto regional**: Informa√ß√µes sobre caracter√≠sticas locais de cada cluster\n",
    "- **M√©tricas de acompanhamento**: Indicadores para monitoramento p√≥s-implementa√ß√£o\n",
    "- **Base para modelo supervisionado**: Dados estruturados para pr√≥ximas etapas anal√≠ticas\n",
    "\n",
    "Esta an√°lise transforma **dados brutos em recomenda√ß√µes acion√°veis**, fornecendo √† Chilli Beans um roadmap claro e fundamentado para expans√£o estrat√©gica com foco em √≥culos de grau, maximizando o potencial de sucesso de cada nova loja atrav√©s de localiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFICA√á√ÉO DE LOCAIS IDEAIS PARA NOVAS LOJAS\n",
    "print(\"=== AN√ÅLISE PARA IDENTIFICA√á√ÉO DE NOVOS LOCAIS ===\")\n",
    "\n",
    "# Criando score de potencial baseado em m√∫ltiplos fatores para identificar locais ideais\n",
    "dados_localizacao['potencial_score'] = (\n",
    "    dados_localizacao['vendas_oculos_grau'] * 0.4 +  # 40% peso vendas atuais de √≥culos de grau\n",
    "    dados_localizacao['num_clientes'] * 0.3 +        # 30% peso n√∫mero de clientes\n",
    "    dados_localizacao['receita_total'] * 0.2 +       # 20% peso receita\n",
    "    dados_localizacao['num_lojas'] * 0.1             # 10% peso lojas existentes (para identificar satura√ß√£o)\n",
    ")\n",
    "\n",
    "# Separando novamente SP e outros estados para an√°lise independente\n",
    "dados_expansao_sp = dados_localizacao[dados_localizacao['estado'] == 'S√£o Paulo'].copy()\n",
    "dados_expansao_outros = dados_localizacao[dados_localizacao['estado'] != 'S√£o Paulo'].copy()\n",
    "\n",
    "# AN√ÅLISE PRINCIPAL: OUTROS ESTADOS (excluindo SP para evitar vi√©s)\n",
    "print(\"\\n=== IDENTIFICA√á√ÉO DE NOVOS LOCAIS - OUTROS ESTADOS ===\")\n",
    "\n",
    "X_expansao = dados_expansao_outros[['longitude', 'latitude']].values\n",
    "weights_expansao = dados_expansao_outros['potencial_score'].values + 1\n",
    "\n",
    "# Usando k=5 como √≥timo baseado na an√°lise anterior\n",
    "k_otimo = 5\n",
    "kmeans_expansao = KMeans(n_clusters=k_otimo, random_state=42, n_init=10)\n",
    "clusters_expansao = kmeans_expansao.fit_predict(X_expansao, sample_weight=weights_expansao)\n",
    "\n",
    "# Visualiza√ß√£o da clusteriza√ß√£o para identifica√ß√£o de novos locais\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Gr√°fico 1: Clusters baseados no potencial de mercado\n",
    "ax1 = axes[0, 0]\n",
    "scatter1 = ax1.scatter(dados_expansao_outros['longitude'], dados_expansao_outros['latitude'], \n",
    "                      c=clusters_expansao, s=dados_expansao_outros['potencial_score']*2, \n",
    "                      alpha=0.7, cmap='tab10')\n",
    "centroids = kmeans_expansao.cluster_centers_\n",
    "ax1.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           c='red', marker='x', s=300, linewidths=4, label='Locais Ideais (Centr√≥ides)')\n",
    "ax1.set_xlabel('Longitude (¬∞)')\n",
    "ax1.set_ylabel('Latitude (¬∞)')\n",
    "ax1.set_title('Clusters por Potencial de Mercado\\n(Tamanho = Potencial Score)')\n",
    "ax1.legend(loc='upper right')\n",
    "# Adicionar explica√ß√£o do tamanho e cores\n",
    "ax1.text(0.02, 0.02, 'Cores = Clusters diferentes\\nTamanho = Score de potencial √ó 2', \n",
    "         transform=ax1.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Gr√°fico 2: Foco espec√≠fico em vendas de √≥culos de grau\n",
    "ax2 = axes[0, 1]\n",
    "scatter2 = ax2.scatter(dados_expansao_outros['longitude'], dados_expansao_outros['latitude'], \n",
    "                      c=dados_expansao_outros['vendas_oculos_grau'], \n",
    "                      s=dados_expansao_outros['num_lojas']*10, \n",
    "                      alpha=0.7, cmap='plasma')\n",
    "ax2.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           c='black', marker='x', s=300, linewidths=4, label='Locais Ideais')\n",
    "ax2.set_xlabel('Longitude (¬∞)')\n",
    "ax2.set_ylabel('Latitude (¬∞)')\n",
    "ax2.set_title('Vendas Atuais de √ìculos de Grau\\n(Tamanho = N√∫mero de Lojas)')\n",
    "cbar2 = plt.colorbar(scatter2, ax=ax2, label='Vendas √ìculos de Grau')\n",
    "cbar2.set_label('Quantidade de Vendas de √ìculos de Grau', fontsize=10)\n",
    "ax2.legend(loc='upper right')\n",
    "# Adicionar explica√ß√£o do tamanho\n",
    "ax2.text(0.02, 0.02, 'Tamanho = N√∫mero de lojas √ó 10', \n",
    "         transform=ax2.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Gr√°fico 3: Densidade de clientes vs receita\n",
    "ax3 = axes[1, 0]\n",
    "scatter3 = ax3.scatter(dados_expansao_outros['longitude'], dados_expansao_outros['latitude'], \n",
    "                      c=dados_expansao_outros['num_clientes'], \n",
    "                      s=dados_expansao_outros['receita_total']*10, \n",
    "                      alpha=0.7, cmap='viridis')\n",
    "ax3.scatter(centroids[:, 0], centroids[:, 1], \n",
    "           c='white', marker='x', s=300, linewidths=4, label='Locais Ideais', \n",
    "           edgecolors='black')\n",
    "ax3.set_xlabel('Longitude (¬∞)')\n",
    "ax3.set_ylabel('Latitude (¬∞)')\n",
    "ax3.set_title('Base de Clientes\\n(Tamanho = Receita Total)')\n",
    "cbar3 = plt.colorbar(scatter3, ax=ax3, label='N√∫mero de Clientes')\n",
    "cbar3.set_label('Quantidade de Clientes', fontsize=10)\n",
    "ax3.legend(loc='upper right')\n",
    "# Adicionar explica√ß√£o do tamanho\n",
    "ax3.text(0.02, 0.02, 'Tamanho = Receita total √ó 10', \n",
    "         transform=ax3.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Gr√°fico 4: Ranking de potencial por cluster\n",
    "cluster_stats = []\n",
    "for i in range(k_otimo):\n",
    "    mask = clusters_expansao == i\n",
    "    cluster_data = dados_expansao_outros[mask]\n",
    "    \n",
    "    stats = {\n",
    "        'cluster': i+1,\n",
    "        'num_regioes': len(cluster_data),\n",
    "        'vendas_oculos_grau_total': cluster_data['vendas_oculos_grau'].sum(),\n",
    "        'num_clientes_total': cluster_data['num_clientes'].sum(),\n",
    "        'receita_total': cluster_data['receita_total'].sum(),\n",
    "        'potencial_medio': cluster_data['potencial_score'].mean(),\n",
    "        'potencial_maximo': cluster_data['potencial_score'].max(),\n",
    "        'centro_lat': centroids[i][1],\n",
    "        'centro_lon': centroids[i][0]\n",
    "    }\n",
    "    cluster_stats.append(stats)\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_stats)\n",
    "cluster_df = cluster_df.sort_values('potencial_medio', ascending=False)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(cluster_df)))\n",
    "bars = ax4.bar(cluster_df['cluster'], cluster_df['potencial_medio'], color=colors)\n",
    "ax4.set_xlabel('N√∫mero do Cluster')\n",
    "ax4.set_ylabel('Score de Potencial M√©dio')\n",
    "ax4.set_title('Ranking de Potencial por Cluster\\n(Verde = Maior Potencial)')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionando valores nas barras\n",
    "for i, (bar, valor) in enumerate(zip(bars, cluster_df['potencial_medio'])):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.2, \n",
    "             f'{valor:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Adicionar escala de cores como legenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=plt.cm.RdYlGn(0.9), label='Alto Potencial (>15)'),\n",
    "    Patch(facecolor=plt.cm.RdYlGn(0.6), label='M√©dio Potencial (12-15)'),\n",
    "    Patch(facecolor=plt.cm.RdYlGn(0.3), label='Baixo Potencial (<12)')\n",
    "]\n",
    "ax4.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig((\"../../assets/gr√°ficos/potencial_clusters.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# RECOMENDA√á√ïES DETALHADAS PARA NOVOS LOCAIS\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RECOMENDA√á√ïES DE LOCAIS IDEAIS PARA NOVAS LOJAS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Ordenando clusters por potencial\n",
    "cluster_df_ordenado = cluster_df.sort_values('potencial_medio', ascending=False)\n",
    "\n",
    "for idx, cluster_info in cluster_df_ordenado.iterrows():\n",
    "    cluster_id = int(cluster_info['cluster'] - 1)  # Ajuste do √≠ndice\n",
    "    print(f\"\\nüè™ PRIORIDADE {idx+1} - CLUSTER {cluster_info['cluster']}:\")\n",
    "    print(f\"   üìç Localiza√ß√£o ideal: ({cluster_info['centro_lat']:.2f}, {cluster_info['centro_lon']:.2f})\")\n",
    "    print(f\"   üìä Potencial m√©dio: {cluster_info['potencial_medio']:.2f}\")\n",
    "    print(f\"   üè¢ Regi√µes no cluster: {cluster_info['num_regioes']}\")\n",
    "    print(f\"   üëì Vendas √≥culos de grau: {cluster_info['vendas_oculos_grau_total']}\")\n",
    "    print(f\"   üë• Base de clientes: {cluster_info['num_clientes_total']:,}\")\n",
    "    print(f\"   üí∞ Receita total: R$ {cluster_info['receita_total']:,.2f}\")\n",
    "    \n",
    "    # Encontrando as 3 melhores regi√µes dentro do cluster\n",
    "    cluster_data = dados_expansao_outros[clusters_expansao == cluster_id]\n",
    "    top_regioes = cluster_data.nlargest(3, 'potencial_score')\n",
    "    \n",
    "    print(f\"   üéØ TOP 3 REGI√ïES RECOMENDADAS:\")\n",
    "    for j, (region_idx, regiao) in enumerate(top_regioes.iterrows()):\n",
    "        print(f\"      {j+1}. {regiao['estado']} (Regi√£o {regiao['cod_cidade']}):\")\n",
    "        print(f\"         - Score: {regiao['potencial_score']:.2f}\")\n",
    "        print(f\"         - Vendas √≥culos: {regiao['vendas_oculos_grau']}\")\n",
    "        print(f\"         - Lojas atuais: {regiao['num_lojas']}\")\n",
    "        print(f\"         - Clientes: {regiao['num_clientes']:,}\")\n",
    "        \n",
    "        # Calculando dist√¢ncia do centr√≥ide\n",
    "        distancia_centroide = haversine_distance(\n",
    "            cluster_info['centro_lat'], cluster_info['centro_lon'],\n",
    "            regiao['latitude'], regiao['longitude']\n",
    "        )\n",
    "        print(f\"         - Dist√¢ncia do ponto ideal: {distancia_centroide:.1f} km\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RESUMO EXECUTIVO:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(cluster_df_ordenado[['cluster', 'potencial_medio', 'vendas_oculos_grau_total', \n",
    "                         'num_clientes_total', 'receita_total']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21132dc6",
   "metadata": {},
   "source": [
    "## Exporta√ß√£o de Sugest√µes para o Modelo Supervisionado\n",
    "\n",
    "Esta se√ß√£o representa a **fase final e cr√≠tica do modelo n√£o supervisionado**, onde todos os insights e an√°lises anteriores s√£o consolidados e transformados em um dataset estruturado e acion√°vel para alimentar o modelo supervisionado subsequente.\n",
    "\n",
    "**1. Determina√ß√£o dos Valores K Ideais:**\n",
    "- **Valida√ß√£o dos par√¢metros √≥timos**: Confirma os melhores valores de k identificados nas an√°lises de silhouette\n",
    "- **S√£o Paulo (k=2)**: Configura√ß√£o otimizada para caracter√≠sticas metropolitanas espec√≠ficas\n",
    "- **Outros Estados (k=3)**: Segmenta√ß√£o nacional balanceada para diversidade regional\n",
    "- **Fallback inteligente**: Sistema de recupera√ß√£o caso vari√°veis n√£o estejam dispon√≠veis\n",
    "\n",
    "**2. Recalculagem de Clusters com Par√¢metros Otimizados:**\n",
    "- **Clustering final de SP**: Aplica√ß√£o do K-Means com k=2 usando sample weights baseados em vendas de √≥culos de grau\n",
    "- **Clustering nacional**: Segmenta√ß√£o de outros estados com k=3 priorizando score de potencial\n",
    "- **Centr√≥ides precisos**: Identifica√ß√£o matem√°tica dos pontos ideais para cada cluster\n",
    "- **Valida√ß√£o de qualidade**: Confirma√ß√£o da cria√ß√£o bem-sucedida de todos os clusters\n",
    "\n",
    "**3. Sistema de Gera√ß√£o de Sugest√µes Inteligente:**\n",
    "- **Processamento por escopo**: An√°lise separada e especializada para SP vs outros estados\n",
    "- **Mapeamento centr√≥ide-realidade**: Convers√£o de pontos matem√°ticos abstratos em localiza√ß√µes geogr√°ficas reais\n",
    "- **Fun√ß√£o Haversine integrada**: C√°lculo de dist√¢ncias reais entre centr√≥ides e regi√µes existentes\n",
    "- **Prote√ß√£o contra dados vazios**: Valida√ß√µes robustas para evitar erros em clusters sem dados\n",
    "\n",
    "**4. M√©tricas Abrangentes por Sugest√£o:**\n",
    "- **Coordenadas duais**: Tanto centr√≥ides te√≥ricos quanto localiza√ß√µes reais sugeridas\n",
    "- **Score de potencial**: M√©dio e m√°ximo por cluster para prioriza√ß√£o estrat√©gica\n",
    "- **M√©tricas de performance**: Vendas de √≥culos de grau, clientes, receita e n√∫mero de lojas\n",
    "- **Indicadores normalizados**: M√©tricas por loja para compara√ß√£o justa entre regi√µes\n",
    "\n",
    "**5. Estrutura de Dados Padronizada:**\n",
    "- **Campos consistentes**: Esquema uniforme para todas as sugest√µes independente do escopo\n",
    "- **Tipos de dados otimizados**: Convers√µes apropriadas (int, float, string) para efici√™ncia\n",
    "- **Informa√ß√µes contextuais**: Estado, regi√£o Chilli Beans e classifica√ß√£o de escopo\n",
    "- **Dist√¢ncias validadas**: Proximidade em quil√¥metros entre centr√≥ides e sugest√µes\n",
    "\n",
    "**6. Sistema de Prioriza√ß√£o Hier√°rquica:**\n",
    "- **Ranking por escopo**: Ordena√ß√£o dentro de S√£o Paulo e outros estados separadamente\n",
    "- **Score como crit√©rio**: Potencial m√©dio do cluster determina ordem de prioridade\n",
    "- **Numera√ß√£o autom√°tica**: Prioridades de 1 a N dentro de cada escopo geogr√°fico\n",
    "- **Flexibilidade estrat√©gica**: Permite abordagens diferenciadas por regi√£o\n",
    "\n",
    "**7. Valida√ß√£o e Prote√ß√£o de Dados:**\n",
    "- **Verifica√ß√£o de exist√™ncia**: Confirma disponibilidade de todas as vari√°veis necess√°rias\n",
    "- **Tratamento de divis√£o por zero**: Prote√ß√£o em normaliza√ß√µes por n√∫mero de lojas\n",
    "- **Dados ausentes**: Fallbacks apropriados para cen√°rios de dados incompletos\n",
    "- **Consist√™ncia de tipos**: Garantia de que todos os campos est√£o nos formatos corretos\n",
    "\n",
    "**8. Exporta√ß√£o Estruturada para CSV:**\n",
    "- **Arquivo padronizado**: `sugestoes_expansao_para_supervisionado.csv` com esquema definido\n",
    "- **Compatibilidade garantida**: Formato ideal para ingest√£o por modelos de machine learning\n",
    "- **Metadados inclu√≠dos**: Informa√ß√µes suficientes para interpreta√ß√£o independente\n",
    "- **Versionamento impl√≠cito**: Dados datados e reproduz√≠veis para auditoria\n",
    "\n",
    "**9. Campos Exportados Detalhados:**\n",
    "- **Identifica√ß√£o**: escopo, cluster_id, prioridade_no_escopo\n",
    "- **Coordenadas**: latitude/longitude de centr√≥ides e sugest√µes\n",
    "- **Geografia**: estado, regi√£o Chilli Beans\n",
    "- **Performance**: vendas, clientes, receita (total e por loja)\n",
    "- **Qualidade**: potencial score, dist√¢ncia do centr√≥ide\n",
    "\n",
    "**10. Estat√≠sticas de Valida√ß√£o:**\n",
    "- **Contagem por escopo**: N√∫mero de sugest√µes geradas para SP vs outros estados\n",
    "- **Potencial m√©dio**: Score agregado para validar qualidade das recomenda√ß√µes\n",
    "- **Vendas totais**: Soma das vendas de √≥culos de grau nas regi√µes sugeridas\n",
    "- **Shape do dataset**: Dimens√µes finais para confirma√ß√£o de completude\n",
    "\n",
    "**11. Integra√ß√£o com Modelo Supervisionado:**\n",
    "- **Features prontas**: Vari√°veis num√©ricas e categ√≥ricas estruturadas para ML\n",
    "- **Target impl√≠cito**: Score de potencial pode servir como vari√°vel de interesse\n",
    "- **Coordenadas para geocoding**: Dados geogr√°ficos para an√°lises espaciais avan√ßadas\n",
    "- **Contexto de neg√≥cio**: Informa√ß√µes interpret√°veis para stakeholders\n",
    "\n",
    "**12. Benef√≠cios da Abordagem:**\n",
    "- **Redu√ß√£o de processamento**: Modelo supervisionado recebe dados pr√©-processados e validados\n",
    "- **Reprodutibilidade**: Mesmos dados de entrada geram mesmas sugest√µes\n",
    "- **Escalabilidade**: Framework aplic√°vel a futuras expans√µes ou atualiza√ß√µes\n",
    "- **Auditabilidade**: Rastro completo desde dados brutos at√© recomenda√ß√µes finais\n",
    "\n",
    "**13. Outputs de Qualidade:**\n",
    "- **Dataset limpo**: Sem valores nulos ou inconsist√™ncias\n",
    "- **Distribui√ß√£o balanceada**: Sugest√µes cobrindo diferentes regi√µes e potenciais\n",
    "- **M√©tricas validadas**: Todos os indicadores dentro de ranges esperados\n",
    "- **Pronto para produ√ß√£o**: Formato diretamente utiliz√°vel por sistemas downstream\n",
    "\n",
    "**14. Prepara√ß√£o para Decis√µes Estrat√©gicas:**\n",
    "- **Prioriza√ß√£o clara**: Ordem de investimento baseada em dados objetivos\n",
    "- **ROI estimado**: M√©tricas de potencial fundamentam expectativas de retorno\n",
    "- **Diversifica√ß√£o geogr√°fica**: Mix de oportunidades em diferentes mercados\n",
    "- **Implementa√ß√£o faseada**: Permite execu√ß√£o gradual baseada em prioridades\n",
    "\n",
    "Esta etapa **converte an√°lises explorat√≥rias em a√ß√µes concretas**, fornecendo ao modelo supervisionado uma base s√≥lida de localiza√ß√µes pr√©-qualificadas e priorizadas, maximizando a efici√™ncia e precis√£o das pr√≥ximas fases do projeto de expans√£o da Chilli Beans com foco em √≥culos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVALIA√á√ÉO E INTERPRETA√á√ÉO DOS RESULTADOS\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"M√âTRICAS DE QUALIDADE DA CLUSTERIZA√á√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Avalia√ß√£o das m√©tricas para diferentes valores de k\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# Compara√ß√£o de in√©rcia entre SP e outros estados\n",
    "ax1 = axes[0, 0]\n",
    "line_sp = ax1.plot(k_values_sp, inertias_sp, 'ro-', label='S√£o Paulo', linewidth=2, markersize=6)\n",
    "line_outros = ax1.plot(k_values_outros[:len(inertias_outros)], inertias_outros, 'bo-', label='Outros Estados', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax1.set_ylabel('In√©rcia (WCSS)')\n",
    "ax1.set_title('Compara√ß√£o da In√©rcia: SP vs Outros Estados\\n(Menor in√©rcia = Melhor agrupamento)')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "# Adicionar anota√ß√£o explicativa\n",
    "ax1.text(0.02, 0.98, 'In√©rcia mede a compacta√ß√£o dos clusters\\n(valores menores s√£o melhores)', \n",
    "         transform=ax1.transAxes, fontsize=9, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.8),\n",
    "         verticalalignment='top')\n",
    "\n",
    "# Compara√ß√£o de Silhouette Score\n",
    "ax2 = axes[0, 1]\n",
    "if silhouette_scores_sp:\n",
    "    ax2.plot(k_values_sp[1:], silhouette_scores_sp, 'ro-', label='S√£o Paulo', linewidth=2, markersize=6)\n",
    "if silhouette_scores_outros:\n",
    "    ax2.plot(k_values_outros[1:len(silhouette_scores_outros)+1], silhouette_scores_outros, 'bo-', label='Outros Estados', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('N√∫mero de Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Compara√ß√£o do Silhouette Score\\n(Maior score = Melhor separa√ß√£o)')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, max(max(silhouette_scores_outros) if silhouette_scores_outros else 0, \n",
    "                   max(silhouette_scores_sp) if silhouette_scores_sp else 0) + 0.1)\n",
    "# Adicionar linha de refer√™ncia para qualidade boa (>0.5)\n",
    "ax2.axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Boa qualidade (>0.5)')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "# Distribui√ß√£o de vendas por regi√£o\n",
    "ax3 = axes[1, 0]\n",
    "regioes = ['S√£o Paulo', 'Outros Estados']\n",
    "vendas_por_regiao = [dados_sp['vendas_oculos_grau'].sum(), dados_outros_estados['vendas_oculos_grau'].sum()]\n",
    "colors = ['red', 'blue']\n",
    "wedges, texts, autotexts = ax3.pie(vendas_por_regiao, labels=regioes, colors=colors, \n",
    "                                   autopct='%1.1f%%', startangle=90, \n",
    "                                   explode=(0.05, 0))  # Destacar S√£o Paulo\n",
    "ax3.set_title('Distribui√ß√£o de Vendas de √ìculos de Grau por Regi√£o')\n",
    "# Melhorar formata√ß√£o dos textos\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(12)\n",
    "# Adicionar valores absolutos\n",
    "total_vendas = sum(vendas_por_regiao)\n",
    "ax3.text(0.02, -1.3, f'Total de vendas: {total_vendas:,} unidades', \n",
    "         transform=ax3.transData, fontsize=10, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "# Efici√™ncia do modelo - Rela√ß√£o lojas vs vendas\n",
    "ax4 = axes[1, 1]\n",
    "lojas_por_regiao = [dados_sp['num_lojas'].sum(), dados_outros_estados['num_lojas'].sum()]\n",
    "eficiencia = [v/l for v, l in zip(vendas_por_regiao, lojas_por_regiao)]\n",
    "bars = ax4.bar(regioes, eficiencia, color=colors, alpha=0.7, width=0.6)\n",
    "ax4.set_ylabel('Vendas de √ìculos por Loja (unidades)')\n",
    "ax4.set_title('Efici√™ncia de Vendas por Regi√£o\\n(Vendas de √≥culos por loja)')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "ax4.set_ylim(0, max(eficiencia) * 1.2)\n",
    "\n",
    "# Adicionando valores nas barras com mais detalhes\n",
    "for i, (bar, valor, vendas, lojas) in enumerate(zip(bars, eficiencia, vendas_por_regiao, lojas_por_regiao)):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05, \n",
    "             f'{valor:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    # Adicionar informa√ß√µes adicionais\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., -0.15, \n",
    "             f'{vendas} vendas\\n{lojas} lojas', ha='center', va='top', fontsize=9,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=colors[i], alpha=0.3))\n",
    "\n",
    "# Adicionar linha de m√©dia\n",
    "media_eficiencia = sum(vendas_por_regiao) / sum(lojas_por_regiao)\n",
    "ax4.axhline(y=media_eficiencia, color='green', linestyle='--', alpha=0.7, \n",
    "           label=f'M√©dia geral: {media_eficiencia:.2f}')\n",
    "ax4.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig((\"../../assets/gr√°ficos/comparacoes.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# INSIGHTS ESTRAT√âGICOS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHTS ESTRAT√âGICOS PARA EXPANS√ÉO DA CHILLI BEANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculando m√©tricas-chave\n",
    "total_vendas_oculos = dados_localizacao['vendas_oculos_grau'].sum()\n",
    "total_lojas = dados_localizacao['num_lojas'].sum()\n",
    "total_clientes = dados_localizacao['num_clientes'].sum()\n",
    "total_receita = dados_localizacao['receita_total'].sum()\n",
    "\n",
    "participacao_sp = (dados_sp['vendas_oculos_grau'].sum() / total_vendas_oculos) * 100\n",
    "participacao_outros = (dados_outros_estados['vendas_oculos_grau'].sum() / total_vendas_oculos) * 100\n",
    "\n",
    "print(f\"\\nüìä PANORAMA GERAL:\")\n",
    "print(f\"   ‚Ä¢ Total de vendas de √≥culos de grau: {total_vendas_oculos:,}\")\n",
    "print(f\"   ‚Ä¢ Total de lojas: {total_lojas:,}\")\n",
    "print(f\"   ‚Ä¢ Total de clientes: {total_clientes:,}\")\n",
    "print(f\"   ‚Ä¢ Receita total: R$ {total_receita:,.2f}\")\n",
    "print(f\"   ‚Ä¢ M√©dia de vendas por loja: {total_vendas_oculos/total_lojas:.2f}\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è DISTRIBUI√á√ÉO REGIONAL:\")\n",
    "print(f\"   ‚Ä¢ S√£o Paulo: {participacao_sp:.1f}% das vendas de √≥culos de grau\")\n",
    "print(f\"   ‚Ä¢ Outros Estados: {participacao_outros:.1f}% das vendas de √≥culos de grau\")\n",
    "print(f\"   ‚Ä¢ Concentra√ß√£o em SP justifica an√°lise separada ‚úì\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMENDA√á√ïES PARA NOVAS LOJAS:\")\n",
    "\n",
    "# Top 5 estados com maior potencial (excluindo SP) - usando dados_expansao_outros que tem potencial_score\n",
    "top_estados_potencial = dados_expansao_outros.groupby('estado').agg({\n",
    "    'potencial_score': 'mean',\n",
    "    'vendas_oculos_grau': 'sum',\n",
    "    'num_lojas': 'sum',\n",
    "    'num_clientes': 'sum'\n",
    "}).sort_values('potencial_score', ascending=False).head(5)\n",
    "\n",
    "print(f\"\\n   üèÜ TOP 5 ESTADOS COM MAIOR POTENCIAL (excluindo SP):\")\n",
    "for i, (estado, dados_estado) in enumerate(top_estados_potencial.iterrows(), 1):\n",
    "    print(f\"      {i}. {estado}:\")\n",
    "    print(f\"         - Score m√©dio: {dados_estado['potencial_score']:.2f}\")\n",
    "    print(f\"         - Vendas atuais: {dados_estado['vendas_oculos_grau']}\")\n",
    "    print(f\"         - Lojas existentes: {dados_estado['num_lojas']}\")\n",
    "    print(f\"         - Base de clientes: {dados_estado['num_clientes']:,}\")\n",
    "\n",
    "print(f\"\\nüîç AN√ÅLISE DE CLUSTERS √ìTIMOS:\")\n",
    "print(f\"   ‚Ä¢ S√£o Paulo: Melhor k = {best_k_sp if 'best_k_sp' in locals() else 'N/A'}\")\n",
    "print(f\"   ‚Ä¢ Outros Estados: Melhor k = {best_k_outros}\")\n",
    "print(f\"   ‚Ä¢ Silhouette Score √≥timo: {max(silhouette_scores_outros):.3f}\")\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS DE NEG√ìCIO:\")\n",
    "print(f\"   1. üéØ FOCO GEOGR√ÅFICO:\")\n",
    "print(f\"      ‚Ä¢ Priorizar expans√£o nos clusters com maior potencial\")\n",
    "print(f\"      ‚Ä¢ Manter estrat√©gia diferenciada para S√£o Paulo\")\n",
    "print(f\"      ‚Ä¢ Explorar oportunidades no Nordeste e Sul\")\n",
    "\n",
    "print(f\"\\n   2. üìà ESTRAT√âGIA DE PRODUTOS:\")\n",
    "print(f\"      ‚Ä¢ √ìculos de grau representam mercado estrat√©gico\")\n",
    "print(f\"      ‚Ä¢ Considerar mix de produtos por regi√£o\")\n",
    "print(f\"      ‚Ä¢ Focar em regi√µes com alta base de clientes\")\n",
    "\n",
    "print(f\"\\n   3. üè™ LOCALIZA√á√ÉO DE LOJAS:\")\n",
    "print(f\"      ‚Ä¢ Usar centr√≥ides dos clusters como refer√™ncia\")\n",
    "print(f\"      ‚Ä¢ Considerar densidade populacional local\")\n",
    "print(f\"      ‚Ä¢ Avaliar concorr√™ncia em cada regi√£o\")\n",
    "\n",
    "print(f\"\\n   4. üìä M√âTRICAS DE ACOMPANHAMENTO:\")\n",
    "print(f\"      ‚Ä¢ Vendas de √≥culos de grau por loja\")\n",
    "print(f\"      ‚Ä¢ Penetra√ß√£o de mercado por cluster\")\n",
    "print(f\"      ‚Ä¢ ROI por regi√£o de expans√£o\")\n",
    "\n",
    "# Resumo dos clusters identificados\n",
    "print(f\"\\nüìç RESUMO DOS CLUSTERS PARA EXPANS√ÉO:\")\n",
    "if 'cluster_df' in locals():\n",
    "    cluster_resumo = cluster_df.sort_values('potencial_medio', ascending=False)\n",
    "    for idx, row in cluster_resumo.iterrows():\n",
    "        prioridade = \"ü•á ALTA\" if row['potencial_medio'] > 15 else \"ü•à M√âDIA\" if row['potencial_medio'] > 12 else \"ü•â BAIXA\"\n",
    "        print(f\"   Cluster {int(row['cluster'])}: {prioridade} PRIORIDADE\")\n",
    "        print(f\"      - Potencial: {row['potencial_medio']:.1f}\")\n",
    "        print(f\"      - Localiza√ß√£o: ({row['centro_lat']:.1f}, {row['centro_lon']:.1f})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUS√ÉO: O modelo identificou 5 clusters regionais √≥timos para expans√£o,\")\n",
    "print(\"com oportunidades claras de crescimento focadas em √≥culos de grau.\")\n",
    "print(\"A abordagem separada SP vs outros estados evitou vi√©s e forneceu\")\n",
    "print(\"insights estrat√©gicos mais precisos para a tomada de decis√£o.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc435a",
   "metadata": {},
   "source": [
    "## üîÑ Corre√ß√£o dos Valores de *k* por Regi√£o (Recalibra√ß√£o de Clusters)\n",
    "\n",
    "Esta c√©lula realiza uma **recalibra√ß√£o expl√≠cita do n√∫mero de clusters (k)** para cada recorte geogr√°fico (S√£o Paulo vs Outros Estados), alinhando o pipeline aos resultados observados nos gr√°ficos anteriores de **Curva do Cotovelo (Elbow)** e, secundariamente, ao comportamento do **Silhouette Score**.\n",
    "\n",
    "**1. Contexto da Corre√ß√£o**  \n",
    "- Etapas anteriores testaram m√∫ltiplos valores de k (1‚Äì6) para cada regi√£o  \n",
    "- Visualmente, a Curva do Cotovelo indicou estabiliza√ß√£o mais precoce em SP (k‚âà2) e leve ganho adicional nacional em k‚âà3  \n",
    "- Esta c√©lula for√ßa o uso desses valores ‚Äú√≥timos operacionais‚Äù para padronizar as etapas subsequentes\n",
    "\n",
    "**2. Motivo da Separa√ß√£o SP vs Outros**  \n",
    "- S√£o Paulo tem densidade e concentra√ß√£o superiores ‚Üí menor k evita subdivis√£o artificial  \n",
    "- Outros Estados apresentam maior dispers√£o geogr√°fica ‚Üí k=3 captura macro-regi√µes sem superfragmentar\n",
    "\n",
    "**3. Crit√©rios Utilizados para Escolha**  \n",
    "- Elbow: ponto onde a redu√ß√£o da in√©rcia passa a ter retorno marginal  \n",
    "- Silhouette: verifica√ß√£o de separa√ß√£o aceit√°vel entre grupos (valores > ~0.3 j√° indicam estrutura m√≠nima; >0.5 seria ideal)  \n",
    "- Trade-off: simplicidade + interpretabilidade > overfitting espacial\n",
    "\n",
    "**4. A√ß√µes Executadas na C√©lula**  \n",
    "1. Define explicitamente: `best_k_sp = 2` e `best_k_outros = 3`  \n",
    "2. Reinstancia dois modelos `KMeans` independentes  \n",
    "3. Recalcula r√≥tulos de cluster (`clusters_sp`, `clusters_outros`)  \n",
    "4. Recalcula coeficientes de silhueta para valida√ß√£o r√°pida\n",
    "\n",
    "**5. M√©tricas Calculadas**  \n",
    "- `silhouette_sp_correto`: qualidade da separa√ß√£o para SP com k=2  \n",
    "- `silhouette_outros_correto`: qualidade da separa√ß√£o nacional (exceto SP) com k=3  \n",
    "- Uso direto de latitude/longitude sem pesos nesta etapa (foco: valida√ß√£o estrutural simples)\n",
    "\n",
    "**6. Benef√≠cios da Recalibra√ß√£o**  \n",
    "- Reduz risco de supersegmenta√ß√£o em √°reas densas  \n",
    "- Mant√©m clusters interpret√°veis para relato executivo  \n",
    "- Melhora consist√™ncia entre etapas explorat√≥rias e etapas de exporta√ß√£o/decis√£o\n",
    "\n",
    "**7. Impacto no Pipeline Posterior**  \n",
    "- Afeta qualquer etapa que:  \n",
    "  - Usa `clusters_sp` / `clusters_outros` para agrega√ß√£o ou ranking  \n",
    "  - Gera centr√≥ides para recomenda√ß√µes geogr√°ficas  \n",
    "  - Calcula estat√≠sticas comparativas por cluster  \n",
    "- Garante coer√™ncia com exporta√ß√£o final das sugest√µes supervisionadas\n",
    "\n",
    "**8. Limita√ß√µes / Alertas**  \n",
    "- N√£o recalcula pesos por vendas/potencial (vers√£o simplificada)  \n",
    "- Silhouette em coordenadas puras n√£o considera intensidade de neg√≥cio  \n",
    "- Caso a distribui√ß√£o futura mude (novas lojas), valores de k devem ser revisados\n",
    "\n",
    "**9. Poss√≠veis Extens√µes Futuras**  \n",
    "- Repetir an√°lise com clustering ponderado (sample_weight)  \n",
    "- Testar algoritmos alternativos: DBSCAN (densidade) ou HDBSCAN (robusto a ru√≠do)  \n",
    "- Incluir features enriquecidas (potencial_score normalizado, densidade de clientes)\n",
    "\n",
    "**10. Interpreta√ß√£o dos Resultados Impressos**  \n",
    "- Linhas de sa√≠da confirmam k aplicado para cada regi√£o  \n",
    "- Exibem silhueta final ‚Üí valida√ß√£o r√°pida de separa√ß√£o espacial  \n",
    "- Mensagem final d√° o resumo operacional adotado\n",
    "\n",
    "**11. Quando Reexecutar Esta C√©lula**  \n",
    "- Ap√≥s altera√ß√£o nas coordenadas ou filtragem geogr√°fica  \n",
    "- Ap√≥s redefini√ß√£o de pesos estrat√©gicos ou normaliza√ß√£o espacial  \n",
    "- Antes de exportar novos CSVs de sugest√µes supervisionadas\n",
    "\n",
    "**12. Valor Estrat√©gico**  \n",
    "- Consolida escolha de segmenta√ß√£o espacial ‚Äúoficial‚Äù  \n",
    "- Serve como checkpoint de governan√ßa anal√≠tica  \n",
    "- Evita diverg√™ncia entre experimenta√ß√£o e entrega executiva\n",
    "\n",
    "**Resumo:** Esta c√©lula ‚Äúcongela‚Äù as decis√µes de k (SP=2, Outros=3) com base nas an√°lises anteriores e revalida rapidamente a qualidade dos agrupamentos para garantir consist√™ncia e interpretabilidade nas etapas finais do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d31d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRE√á√ÉO: k espec√≠fico para cada regi√£o baseado no m√©todo do cotovelo\n",
    "print(\"=== CORRE√á√ÉO DOS VALORES K BASEADO NA AN√ÅLISE ESPEC√çFICA DO COTOVELO ===\")\n",
    "print(\"S√£o Paulo: k=2 (conforme gr√°fico do cotovelo)\")\n",
    "print(\"Outros Estados: k=3 (conforme gr√°fico do cotovelo)\")\n",
    "print(\"\\nAtualizando os valores...\")\n",
    "\n",
    "# Redefinir os valores de k baseado na an√°lise espec√≠fica do cotovelo\n",
    "best_k_sp = 2\n",
    "best_k_outros = 3\n",
    "\n",
    "print(f\"K atualizado para S√£o Paulo: {best_k_sp}\")\n",
    "print(f\"K atualizado para Outros Estados: {best_k_outros}\")\n",
    "\n",
    "# Recalcular clustering com os valores corretos\n",
    "print(\"\\nRecalculando clustering com valores espec√≠ficos...\")\n",
    "\n",
    "# S√£o Paulo com k=2\n",
    "kmeans_sp = KMeans(n_clusters=best_k_sp, random_state=42, n_init=10)\n",
    "clusters_sp = kmeans_sp.fit_predict(X_sp)\n",
    "\n",
    "# Outros Estados com k=3\n",
    "kmeans_outros = KMeans(n_clusters=best_k_outros, random_state=42, n_init=10)\n",
    "clusters_outros = kmeans_outros.fit_predict(X_outros)\n",
    "\n",
    "# Recalcular silhueta com os novos valores\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_sp_correto = silhouette_score(X_sp, clusters_sp)\n",
    "silhouette_outros_correto = silhouette_score(X_outros, clusters_outros)\n",
    "\n",
    "print(f\"\\nCoeficientes de silhueta com valores corretos:\")\n",
    "print(f\"S√£o Paulo (k=2): {silhouette_sp_correto:.3f}\")\n",
    "print(f\"Outros Estados (k=3): {silhouette_outros_correto:.3f}\")\n",
    "\n",
    "print(\"\\nClustering corrigido com sucesso!\")\n",
    "print(f\"\\nResumo:\")\n",
    "print(f\"‚Ä¢ S√£o Paulo ser√° dividido em {best_k_sp} clusters\")\n",
    "print(f\"‚Ä¢ Outros Estados ser√£o divididos em {best_k_outros} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635678fd",
   "metadata": {},
   "source": [
    "## M√©trica Pseudo R¬≤ e Verifica√ß√£o do Ambiente de Clusters (C√©lula 31)\n",
    "\n",
    "Esta c√©lula introduz uma m√©trica adicional de qualidade para os agrupamentos (pseudo R¬≤), al√©m de executar verifica√ß√µes de integridade das vari√°veis de clustering j√° criadas, garantindo que o ambiente esteja consistente antes de an√°lises posteriores ponderadas.\n",
    "\n",
    "**1. Objetivo Principal**  \n",
    "- Calcular uma medida tipo ‚Äúfra√ß√£o de vari√¢ncia explicada‚Äù pelos clusters (pseudo R¬≤)  \n",
    "- Confirmar a exist√™ncia e formato de vari√°veis cr√≠ticas (`X_sp`, `X_outros`, `clusters_sp`, `clusters_outros`)  \n",
    "- Manter compatibilidade com mudan√ßas de nomenclatura (uso atual de `vendas_oculos_grau`)\n",
    "\n",
    "**2. Contexto no Pipeline**  \n",
    "- Ap√≥s definir e recalibrar k para cada regi√£o, j√° existem r√≥tulos de cluster  \n",
    "- Silhouette j√° foi usado; esta m√©trica complementa a avalia√ß√£o com uma vis√£o ‚Äúvari√¢ncia explicada‚Äù  \n",
    "- Serve como diagn√≥stico r√°pido sem gerar gr√°ficos adicionais\n",
    "\n",
    "**3. Ajuste de Terminologia**  \n",
    "- Mensagem inicial esclarece que antigas refer√™ncias gen√©ricas a ‚Äúvendas‚Äù foram alinhadas ao nome consolidado: `vendas_oculos_grau`  \n",
    "- Evita confus√£o em logs antigos ou compara√ß√µes de execu√ß√£o\n",
    "\n",
    "**4. Fun√ß√£o `calcular_pseudo_r2()`**  \n",
    "- F√≥rmula: pseudo_r2 = 1 ‚àí (SSE_within / SST_total)  \n",
    "  - SST: soma dos quadrados das dist√¢ncias de cada ponto ao centro global  \n",
    "  - SSE: soma dos quadrados das dist√¢ncias de cada ponto ao centr√≥ide do seu cluster  \n",
    "- Interpreta√ß√£o: quanto maior (pr√≥ximo de 1), maior propor√ß√£o da variabilidade espacial capturada pelos agrupamentos  \n",
    "- Sempre entre 0 e 1 (SSE ‚â§ SST em parti√ß√µes n√£o degeneradas)\n",
    "\n",
    "**5. Passos Executados**  \n",
    "1. Imprime m√©dias de vendas de √≥culos de grau (SP e outros)  \n",
    "2. Define fun√ß√£o de pseudo R¬≤ (uso de norma Euclidiana em latitude/longitude)  \n",
    "3. Lista vari√°veis esperadas e suas shapes (debug de ambiente)  \n",
    "4. Tenta calcular `pseudo_r2_sp` e `pseudo_r2_outros` com tratamento de exce√ß√µes  \n",
    "5. Armazena `None` caso vari√°veis n√£o existam ou ocorram erros\n",
    "\n",
    "**6. Interpreta√ß√£o do Pseudo R¬≤**  \n",
    "- Valores mais altos: clusters capturam estrutura espacial (boa separa√ß√£o macro)  \n",
    "- Valores modestos: dispers√£o geogr√°fica elevada ou k pequeno (intencional para interpretabilidade)  \n",
    "- Compar√°vel apenas dentro do mesmo espa√ßo de features (n√£o misturar com m√©tricas ponderadas futuras)\n",
    "\n",
    "**7. Benef√≠cios da M√©trica**  \n",
    "- Simples, interpret√°vel e r√°pida de calcular  \n",
    "- Complementa Silhouette (separa√ß√£o relativa) com vis√£o de ‚Äúredu√ß√£o de vari√¢ncia‚Äù  \n",
    "- √ötil em relat√≥rios executivos como indica√ß√£o percentual (ex.: ‚Äúclusters explicam ~X% da variabilidade espacial‚Äù)\n",
    "\n",
    "**8. Limita√ß√µes / Alertas**  \n",
    "- Usa coordenadas em plano (n√£o corrige curvatura da Terra) ‚Üí aceit√°vel para escala nacional aproximada  \n",
    "- N√£o utiliza pesos (ignora import√¢ncia comercial/local)  \n",
    "- N√£o considera densidade ou forma n√£o esf√©rica dos clusters (K-Means imp√µe convexidade)  \n",
    "- N√£o substitui m√©tricas padr√£o (Silhouette, Calinski-Harabasz, Davies-Bouldin)\n",
    "\n",
    "**9. Poss√≠veis Extens√µes**  \n",
    "- Vers√£o ponderada: incluir `sample_weight` replicando pontos ou ajustando SSE  \n",
    "- Adotar dist√¢ncia Haversine real (converter lat/lon em radianos)  \n",
    "- Comparar com propor√ß√£o Between / Total (BSS / TSS) explicitando vari√¢ncia entre clusters  \n",
    "- Calcular conjuntamente Calinski-Harabasz para refor√ßo anal√≠tico\n",
    "\n",
    "**10. Como Validar Resultados**  \n",
    "- Verificar se os valores n√£o s√£o 0 ou muito pr√≥ximos de 0 (indica aus√™ncia de estrutura)  \n",
    "- Comparar SP vs Outros: SP tende a ter pseudo R¬≤ menor se altamente concentrado em poucas √°reas, ou maior se clusters bem definidos  \n",
    "- Reexecutar ap√≥s alterar k ou redefinir subset geogr√°fico\n",
    "\n",
    "**11. Impacto no Fluxo Posterior**  \n",
    "- Pode embasar justificativa de manuten√ß√£o de k baixo (trade-off simplicidade vs ganho marginal)  \n",
    "- Refer√™ncia adicional nas an√°lises ponderadas subsequentes  \n",
    "- N√£o altera dados ‚Äî apenas diagn√≥sticos\n",
    "\n",
    "**12. Quando Reexecutar**  \n",
    "- Ap√≥s recalcular clusters com novo k  \n",
    "- Ap√≥s filtragens geogr√°ficas ou remo√ß√£o de outliers  \n",
    "- Antes de gerar relat√≥rio final comparativo de m√©tricas\n",
    "\n",
    "**Resumo:** A c√©lula adiciona uma m√©trica complementar (pseudo R¬≤) para avaliar a fra√ß√£o de variabilidade espacial capturada pelos clusters e valida a presen√ßa das vari√°veis de clustering em mem√≥ria, fortalecendo a governan√ßa anal√≠tica antes de an√°lises ponderadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota: algumas sa√≠das antigas do notebook faziam refer√™ncia a 'vendas'. Elas foram atualizadas para os nomes atuais.\n",
    "# Vers√£o segura das mensagens antigas:\n",
    "print(f\"  Vendas m√©dias por regi√£o: {dados_sp['vendas_oculos_grau'].mean():.0f}\")\n",
    "print(f\"  Vendas m√©dias por regi√£o: {dados_outros_estados['vendas_oculos_grau'].mean():.0f}\")\n",
    "\n",
    "\n",
    "def calcular_pseudo_r2(X, clusters):\n",
    "    \"\"\"\n",
    "    Calcula uma medida tipo pseudo R¬≤ (fra√ß√£o da vari√¢ncia explicada) para uma parti√ß√£o em clusters.\n",
    "    pseudo_r2 = 1 - (SSE_within / SST_total), onde SSE_within √© a soma dos quadrados dentro dos clusters\n",
    "    e SST_total √© soma dos quadrados total em rela√ß√£o √† m√©dia global.\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "\n",
    "    # centroides globais\n",
    "    global_mean = _np.mean(X, axis=0)\n",
    "    # SST total\n",
    "    sst = _np.sum((_np.linalg.norm(X - global_mean, axis=1))**2)\n",
    "\n",
    "    # SSE within\n",
    "    sse = 0.0\n",
    "    for k in _np.unique(clusters):\n",
    "        mask = clusters == k\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        centroid = _np.mean(X[mask], axis=0)\n",
    "        sse += _np.sum((_np.linalg.norm(X[mask] - centroid, axis=1))**2)\n",
    "\n",
    "    if sst == 0:\n",
    "        return 0.0\n",
    "    return 1 - (sse / sst)\n",
    "\n",
    "# Debug: verificar vari√°veis dispon√≠veis\n",
    "print(\"Vari√°veis de clustering dispon√≠veis:\")\n",
    "for var_name in ['X_sp', 'X_outros', 'clusters_sp', 'clusters_outros']:\n",
    "    if var_name in globals():\n",
    "        print(f\"  {var_name}: {type(globals()[var_name])}, shape: {getattr(globals()[var_name], 'shape', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"  {var_name}: n√£o encontrado\")\n",
    "\n",
    "# Calcular pseudo-R2 n√£o ponderado usando vari√°veis dispon√≠veis\n",
    "try:\n",
    "    if 'X_sp' in globals() and 'clusters_sp' in globals():\n",
    "        pseudo_r2_sp = calcular_pseudo_r2(X_sp, clusters_sp)\n",
    "        print(f\"Calculado pseudo_r2_sp: {pseudo_r2_sp}\")\n",
    "    else:\n",
    "        pseudo_r2_sp = None\n",
    "        print(\"N√£o foi poss√≠vel calcular pseudo_r2_sp - vari√°veis necess√°rias n√£o encontradas\")\n",
    "except Exception as e:\n",
    "    pseudo_r2_sp = None\n",
    "    print(f\"Erro ao calcular pseudo_r2_sp: {e}\")\n",
    "\n",
    "try:\n",
    "    if 'X_outros' in globals() and 'clusters_outros' in globals():\n",
    "        pseudo_r2_outros = calcular_pseudo_r2(X_outros, clusters_outros)\n",
    "        print(f\"Calculado pseudo_r2_outros: {pseudo_r2_outros}\")\n",
    "    else:\n",
    "        pseudo_r2_outros = None\n",
    "        print(\"N√£o foi poss√≠vel calcular pseudo_r2_outros - vari√°veis necess√°rias n√£o encontradas\")\n",
    "except Exception as e:\n",
    "    pseudo_r2_outros = None\n",
    "    print(f\"Erro ao calcular pseudo_r2_outros: {e}\")\n",
    "\n",
    "# Fun√ß√£o ponderada j√° existente ser√° chamada posteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad490fc2",
   "metadata": {},
   "source": [
    "## An√°lise de Correla√ß√£o entre Clusters e Vari√°veis de Neg√≥cio\n",
    "\n",
    "Esta c√©lula executa uma an√°lise explorat√≥ria para identificar associa√ß√µes lineares entre os r√≥tulos de cluster gerados pelo algoritmo K-Means e as principais m√©tricas de performance comercial da rede Chilli Beans, fornecendo insights sobre como a segmenta√ß√£o geogr√°fica se relaciona com o desempenho operacional.\n",
    "\n",
    "**1. Objetivo da An√°lise**  \n",
    "Verificar se existe correla√ß√£o linear significativa entre a atribui√ß√£o de clusters espaciais e as vari√°veis de neg√≥cio como vendas, n√∫mero de clientes, receita e quantidade de lojas, permitindo avaliar se a segmenta√ß√£o geogr√°fica captura naturalmente diferen√ßas de performance comercial.\n",
    "\n",
    "**2. Metodologia Aplicada**  \n",
    "A c√©lula utiliza o coeficiente de correla√ß√£o de Pearson para medir a for√ßa e dire√ß√£o da rela√ß√£o linear entre o identificador num√©rico do cluster e cada m√©trica de neg√≥cio. O teste √© executado separadamente para S√£o Paulo e demais estados, mantendo a consist√™ncia com a abordagem de an√°lise regionalizada.\n",
    "\n",
    "**3. Prepara√ß√£o dos Dados**  \n",
    "Implementa uma fun√ß√£o de padroniza√ß√£o que cria aliases autom√°ticos para as colunas, mapeando os nomes reais das vari√°veis para nomenclaturas padronizadas. Esta funcionalidade garante compatibilidade e robustez na an√°lise, convertendo automaticamente vendas_oculos_grau para vendas e vendas_oculos, num_clientes para clientes, receita_total para receita, e num_lojas para lojas.\n",
    "\n",
    "**4. Processo de C√°lculo**  \n",
    "Para cada regi√£o, a c√©lula itera atrav√©s das m√©tricas padronizadas, verificando a exist√™ncia da coluna no dataset e a presen√ßa de varia√ß√£o suficiente nos dados. Calcula o coeficiente de correla√ß√£o de Pearson e o respectivo p-value, armazenando os resultados em dicion√°rios estruturados para posterior consolida√ß√£o.\n",
    "\n",
    "**5. Valida√ß√µes Implementadas**  \n",
    "Inclui verifica√ß√µes de integridade que identificam situa√ß√µes onde a correla√ß√£o n√£o pode ser calculada, como aus√™ncia de varia√ß√£o nos dados ou clusters com cardinalidade insuficiente. Estas valida√ß√µes previnem erros matem√°ticos e informam sobre limita√ß√µes dos dados.\n",
    "\n",
    "**6. Resultados Consolidados**  \n",
    "Gera um DataFrame consolidado com todas as correla√ß√µes calculadas, organizando os resultados por escopo geogr√°fico e m√©trica, facilitando a interpreta√ß√£o comparativa. Exibe tanto os resultados individuais quanto um resumo tabular estruturado.\n",
    "\n",
    "**7. Interpreta√ß√£o Estat√≠stica**  \n",
    "Fornece diretrizes claras para interpreta√ß√£o dos resultados, classificando correla√ß√µes pr√≥ximas de zero como aus√™ncia de rela√ß√£o linear, valores entre 0.3 e -0.3 como rela√ß√£o moderada a forte, e p-values menores que 0.05 como estatisticamente significativos.\n",
    "\n",
    "**8. Aplica√ß√£o Estrat√©gica**  \n",
    "Os resultados desta an√°lise informam se os clusters atuais j√° se alinham naturalmente com m√©tricas de performance ou se s√£o puramente espaciais, orientando decis√µes sobre a necessidade de enriquecimento de features para vers√µes futuras do modelo de clustering.\n",
    "\n",
    "**9. Limita√ß√µes Metodol√≥gicas**  \n",
    "Reconhece que clusters s√£o vari√°veis categ√≥ricas tratadas como num√©ricas para fins de correla√ß√£o linear, sendo esta uma aproxima√ß√£o explorat√≥ria v√°lida mas com limita√ß√µes interpretativas. A an√°lise assume linearidade e n√£o captura rela√ß√µes monot√¥nicas n√£o-lineares que poderiam ser detectadas por correla√ß√£o de Spearman.\n",
    "\n",
    "**10. Valor Anal√≠tico**  \n",
    "Serve como checkpoint r√°pido para avaliar o alinhamento entre segmenta√ß√£o geogr√°fica e performance comercial, fornecendo evid√™ncias quantitativas para justificar modifica√ß√µes futuras na estrat√©gia de clustering ou na sele√ß√£o de features para o modelo de expans√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "print(\"=== TESTE 1: CORRELA√á√ÉO COM VARI√ÅVEIS EXTERNAS ===\\n\")\n",
    "\n",
    "def padronizar_metricas(df):\n",
    "    \"\"\"Cria aliases padronizados para as colunas alvo, se existirem.\"\"\"\n",
    "    df_copia = df.copy()\n",
    "    \n",
    "    # Mapear nomes reais para aliases padronizados\n",
    "    mapping = {\n",
    "        'vendas_oculos_grau': ['vendas_oculos', 'vendas'],\n",
    "        'num_clientes': ['clientes'],\n",
    "        'receita_total': ['receita'],\n",
    "        'num_lojas': ['lojas']\n",
    "    }\n",
    "    \n",
    "    for origem, aliases in mapping.items():\n",
    "        if origem in df_copia.columns:\n",
    "            for alias in aliases:\n",
    "                if alias not in df_copia.columns:\n",
    "                    df_copia[alias] = df_copia[origem]\n",
    "    \n",
    "    return df_copia\n",
    "\n",
    "# Para S√£o Paulo\n",
    "print(\"CORRELA√á√ïES - S√ÉO PAULO:\")\n",
    "df_sp_test = padronizar_metricas(dados_sp)\n",
    "df_sp_test['cluster'] = clusters_sp\n",
    "\n",
    "# Correla√ß√£o entre clusters e vari√°veis num√©ricas\n",
    "correlacoes_sp = {}\n",
    "metricas_teste = ['vendas', 'vendas_oculos', 'clientes', 'receita', 'lojas']\n",
    "\n",
    "correlacoes_encontradas_sp = False\n",
    "for col in metricas_teste:\n",
    "    if col in df_sp_test.columns:\n",
    "        # Verificar se h√° varia√ß√£o suficiente\n",
    "        if df_sp_test[col].nunique() < 2 or df_sp_test['cluster'].nunique() < 2:\n",
    "            print(f\"  {col}: vari√¢ncia insuficiente (pulando)\")\n",
    "            continue\n",
    "            \n",
    "        corr_pearson, p_value = pearsonr(df_sp_test['cluster'], df_sp_test[col])\n",
    "        correlacoes_sp[col] = {'correlacao': corr_pearson, 'p_value': p_value}\n",
    "        print(f\"  {col}: r = {corr_pearson:.3f}, p = {p_value:.3f}\")\n",
    "        correlacoes_encontradas_sp = True\n",
    "\n",
    "if not correlacoes_encontradas_sp:\n",
    "    print(\"  (Nenhuma m√©trica compat√≠vel encontrada)\")\n",
    "\n",
    "print(\"\\nCORRELA√á√ïES - OUTROS ESTADOS:\")\n",
    "df_outros_test = padronizar_metricas(dados_outros_estados)\n",
    "df_outros_test['cluster'] = clusters_outros\n",
    "\n",
    "# Correla√ß√£o entre clusters e vari√°veis num√©ricas\n",
    "correlacoes_outros = {}\n",
    "correlacoes_encontradas_outros = False\n",
    "\n",
    "for col in metricas_teste:\n",
    "    if col in df_outros_test.columns:\n",
    "        # Verificar se h√° varia√ß√£o suficiente\n",
    "        if df_outros_test[col].nunique() < 2 or df_outros_test['cluster'].nunique() < 2:\n",
    "            print(f\"  {col}: vari√¢ncia insuficiente (pulando)\")\n",
    "            continue\n",
    "            \n",
    "        corr_pearson, p_value = pearsonr(df_outros_test['cluster'], df_outros_test[col])\n",
    "        correlacoes_outros[col] = {'correlacao': corr_pearson, 'p_value': p_value}\n",
    "        print(f\"  {col}: r = {corr_pearson:.3f}, p = {p_value:.3f}\")\n",
    "        correlacoes_encontradas_outros = True\n",
    "\n",
    "if not correlacoes_encontradas_outros:\n",
    "    print(\"  (Nenhuma m√©trica compat√≠vel encontrada)\")\n",
    "\n",
    "# Consolida√ß√£o opcional\n",
    "import pandas as pd\n",
    "resultados_correlacao = []\n",
    "for col, dados in correlacoes_sp.items():\n",
    "    resultados_correlacao.append({\n",
    "        'escopo': 'SP',\n",
    "        'metrica': col,\n",
    "        'r': dados['correlacao'],\n",
    "        'p_value': dados['p_value']\n",
    "    })\n",
    "\n",
    "for col, dados in correlacoes_outros.items():\n",
    "    resultados_correlacao.append({\n",
    "        'escopo': 'OUTROS',\n",
    "        'metrica': col,\n",
    "        'r': dados['correlacao'],\n",
    "        'p_value': dados['p_value']\n",
    "    })\n",
    "\n",
    "if resultados_correlacao:\n",
    "    df_corr = pd.DataFrame(resultados_correlacao)\n",
    "    print(\"\\nResumo consolidado:\")\n",
    "    print(df_corr.round(3))\n",
    "\n",
    "print(\"\\nINTERPRETA√á√ÉO:\")\n",
    "print(\"- Correla√ß√µes pr√≥ximas de 0: clusters n√£o relacionados linearmente com a vari√°vel\")\n",
    "print(\"- Correla√ß√µes > 0.3 ou < -0.3: rela√ß√£o moderada a forte\")\n",
    "print(\"- p < 0.05: correla√ß√£o estatisticamente significativa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe8743",
   "metadata": {},
   "source": [
    "## Checkpoint Estrutural de M√©tricas\n",
    "\n",
    "Esta c√©lula atua como a primeira barreira de controle antes de qualquer deriva√ß√£o anal√≠tica adicional. O foco √© exclusivamente estrutural: garantir que as tabelas segmentadas (`dados_sp`, `dados_outros_estados`) cont√™m as colunas e linhas m√≠nimas necess√°rias para prosseguir.\n",
    "\n",
    "**Escopo**  \n",
    "Valida presen√ßa e legibilidade de colunas n√∫cleo sem fazer c√°lculos derivados ou normaliza√ß√µes.\n",
    "\n",
    "**Colunas Verificadas (se existirem):**  \n",
    "- `vendas_oculos_grau` (e fallback: `vendas_oculos`, `vendas`)  \n",
    "- `num_clientes`  \n",
    "- `receita_total`  \n",
    "- `num_lojas`\n",
    "\n",
    "**Opera√ß√µes Executadas**  \n",
    "1. Testa exist√™ncia de `dados_sp` e `dados_outros_estados` no ambiente.  \n",
    "2. Calcula m√©dias simples de vendas por regi√£o quando a coluna correspondente est√° presente.  \n",
    "3. Cria um recorte de exemplo (`iloc[0:1]`) para demonstrar acesso defensivo a um registro.  \n",
    "4. Aplica hierarquia de fallback para coluna de vendas (prioridade: `vendas_oculos_grau` ‚Üí `vendas_oculos` ‚Üí `vendas`).  \n",
    "5. Exibe somat√≥rios b√°sicos das demais m√©tricas somente se existirem e suportarem agrega√ß√£o.\n",
    "\n",
    "**N√£o Faz**  \n",
    "- Normaliza√ß√£o por loja  \n",
    "- Consolida√ß√£o de clusters  \n",
    "- Avalia√ß√£o de qualidade estat√≠stica  \n",
    "- Exporta√ß√£o ou muta√ß√£o de dados\n",
    "\n",
    "**Crit√©rios M√≠nimos de Continuidade**  \n",
    "- Cada DataFrame regional cont√©m ao menos 1 linha  \n",
    "- Pelo menos uma coluna de vendas identificada  \n",
    "- Acesso sem exce√ß√µes √†s estruturas principais\n",
    "\n",
    "**Risco Mitigado**  \n",
    "Interrup√ß√£o tardia do pipeline devido a aus√™ncia silenciosa de colunas essenciais ap√≥s limpeza/transforma√ß√£o.\n",
    "\n",
    "**Recomenda√ß√£o**  \n",
    "Se algum item cr√≠tico estiver ausente aqui, interromper execu√ß√£o e revisar etapas de prepara√ß√£o antes de seguir para pseudo R¬≤, correla√ß√µes ou pondera√ß√µes.\n",
    "\n",
    "Resumo: checkpoint inicial enxuto para validar integridade estrutural de insumos antes de c√°lculos anal√≠ticos aprofundados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055eabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibi√ß√£o final de m√©tricas (ajustada para nomes reais)\n",
    "print(\"=== VERIFICA√á√ÉO DE M√âTRICAS FINAIS ===\")\n",
    "\n",
    "# Verifica√ß√£o de vendas m√©dias\n",
    "if 'vendas_oculos_grau' in dados_sp.columns:\n",
    "    print(f\"Vendas m√©dias SP: {dados_sp['vendas_oculos_grau'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"Vendas m√©dias SP: coluna n√£o encontrada\")\n",
    "\n",
    "if 'vendas_oculos_grau' in dados_outros_estados.columns:\n",
    "    print(f\"Vendas m√©dias Outros Estados: {dados_outros_estados['vendas_oculos_grau'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"Vendas m√©dias Outros Estados: coluna n√£o encontrada\")\n",
    "\n",
    "# Exemplo de verifica√ß√£o segura para dados de cluster\n",
    "print(\"\\nVerifica√ß√£o de dados de cluster (exemplo):\")\n",
    "\n",
    "# Verificar se existem dados de cluster em mem√≥ria\n",
    "if 'dados_sp' in globals() and len(dados_sp) > 0:\n",
    "    # Criar um exemplo de cluster_data para demonstra√ß√£o\n",
    "    cluster_data = dados_sp.iloc[0:1]  # Primeiro registro como exemplo\n",
    "    \n",
    "    print(\"Dados de exemplo do primeiro cluster:\")\n",
    "    \n",
    "    # Tentar diferentes nomes de colunas para vendas\n",
    "    vendas_col = None\n",
    "    if 'vendas_oculos_grau' in cluster_data.columns:\n",
    "        vendas_col = cluster_data['vendas_oculos_grau']\n",
    "        nome_coluna = 'vendas_oculos_grau'\n",
    "    elif 'vendas_oculos' in cluster_data.columns:\n",
    "        vendas_col = cluster_data['vendas_oculos']\n",
    "        nome_coluna = 'vendas_oculos'\n",
    "    elif 'vendas' in cluster_data.columns:\n",
    "        vendas_col = cluster_data['vendas']\n",
    "        nome_coluna = 'vendas'\n",
    "    \n",
    "    if vendas_col is not None and hasattr(vendas_col, 'sum'):\n",
    "        print(f\"  Vendas totais ({nome_coluna}): {vendas_col.sum():,.0f}\")\n",
    "    else:\n",
    "        print(\"  Vendas totais: (dados n√£o dispon√≠veis)\")\n",
    "    \n",
    "    # Verificar outras m√©tricas\n",
    "    if 'num_clientes' in cluster_data.columns:\n",
    "        print(f\"  Clientes totais: {cluster_data['num_clientes'].sum():,.0f}\")\n",
    "    \n",
    "    if 'receita_total' in cluster_data.columns:\n",
    "        print(f\"  Receita total: R$ {cluster_data['receita_total'].sum():,.2f}\")\n",
    "    \n",
    "    if 'num_lojas' in cluster_data.columns:\n",
    "        print(f\"  N√∫mero de lojas: {cluster_data['num_lojas'].sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dados de cluster n√£o dispon√≠veis para verifica√ß√£o\")\n",
    "\n",
    "print(\"\\n=== VERIFICA√á√ÉO CONCLU√çDA ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c06ded",
   "metadata": {},
   "source": [
    "## Auditoria P√≥s-Transforma√ß√µes e Consist√™ncia Final\n",
    "\n",
    "Diferente da C√©lula 34 (estrutural e preventiva), esta auditoria √© executada depois que j√° foram aplicadas: pseudo R¬≤, correla√ß√µes, cria√ß√£o de m√©tricas ponderadas por loja e prepara√ß√£o de bases para exporta√ß√£o. O foco aqui √© confirmar **persist√™ncia**, **coer√™ncia superficial** e **acesso seguro** √†s m√©tricas-chave ap√≥s muta√ß√µes e deriva√ß√µes intermedi√°rias.\n",
    "\n",
    "**Objetivo Principal**  \n",
    "Verificar se nenhuma transforma√ß√£o subsequente removeu ou corrompeu colunas cr√≠ticas necess√°rias para:  \n",
    "- Compara√ß√µes antes vs depois (absoluto vs por loja)  \n",
    "- Gera√ß√£o de rankings e prioriza√ß√£o  \n",
    "- Exporta√ß√£o de sugest√µes supervisionadas\n",
    "\n",
    "**Escopo Ampliado em Rela√ß√£o √† C√©lula 34**  \n",
    "- Observa valores m√©dios p√≥s-eventos (n√£o apenas exist√™ncia).  \n",
    "- Contrasta implicitamente o estado atual com o momento anterior.  \n",
    "- Refor√ßa a cadeia de governan√ßa antes do passo de exporta√ß√£o.\n",
    "\n",
    "**Elementos Valid√°veis**  \n",
    "- Perman√™ncia de `vendas_oculos_grau` como fonte bruta  \n",
    "- Acesso √†s colunas derivadas criadas em c√©lulas anteriores (ex.: vers√µes normalizadas que dependem destas bases)  \n",
    "- Capacidade de recuperar um registro de refer√™ncia sem erro\n",
    "\n",
    "**Opera√ß√µes Efetivas**  \n",
    "1. C√°lculo de m√©dias atuais de vendas por regi√£o (SP e Outros).  \n",
    "2. Extra√ß√£o controlada de um registro para inspecionar granularidade m√≠nima.  \n",
    "3. Fallback hier√°rquico de vendas (igual √† c√©lula 34 para consist√™ncia).  \n",
    "4. Apresenta√ß√£o condicional de clientes, receita e n√∫mero de lojas se dispon√≠veis.\n",
    "\n",
    "**Complementaridade com a C√©lula 34**  \n",
    "| Aspecto | C√©lula 34 | C√©lula 36 |\n",
    "|---------|-----------|-----------|\n",
    "| Momento | Pr√©-m√©tricas derivadas | P√≥s-deriva√ß√µes | \n",
    "| Foco | Exist√™ncia estrutural | Persist√™ncia p√≥s-transforma√ß√µes | \n",
    "| Natureza | Preventiva | Verifica√ß√£o final | \n",
    "| Profundidade | M√≠nima | Leitura consolidada | \n",
    "| Uso | Bloquear avan√ßo cedo | Validar antes de exportar |\n",
    "\n",
    "**Limita√ß√µes Deliberadas**  \n",
    "- N√£o reavalia qualidade dos clusters  \n",
    "- N√£o recalcula normaliza√ß√µes  \n",
    "- N√£o compara deltas autom√°ticos entre estados anteriores e atuais  \n",
    "- N√£o valida integridade cruzada (ex.: receita >= vendas)\n",
    "\n",
    "**Riscos Ainda Poss√≠veis**  \n",
    "- Valores an√¥malos silenciosos (outliers leg√≠timos n√£o identificados)  \n",
    "- Mudan√ßas sem√¢nticas em colunas reaproveitadas  \n",
    "- Necessidade de auditoria temporal (n√£o armazenada aqui)\n",
    "\n",
    "**Sugest√µes Futuras**  \n",
    "- Registrar snapshot em JSON (baseline vs final)  \n",
    "- Adicionar checagem de monotonicidade (ex.: m√©dias n√£o negativas)  \n",
    "- Criar verificador de distribui√ß√£o (faixas plaus√≠veis por m√©trica)\n",
    "\n",
    "Resumo: esta auditoria garante que, ap√≥s deriva√ß√µes e prepara√ß√£o para exporta√ß√£o, as bases continuam completas e acess√≠veis ‚Äî funcionando como a √∫ltima verifica√ß√£o de seguran√ßa antes da gera√ß√£o de artefatos supervisionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING PONDERADO POR QUANTIDADE DE LOJAS\n",
    "print(\"=== AN√ÅLISE PONDERADA POR QUANTIDADE DE LOJAS ===\\n\")\n",
    "\n",
    "# Fun√ß√£o para criar dados ponderados\n",
    "def criar_dados_ponderados(df):\n",
    "    \"\"\"\n",
    "    Cria uma vers√£o ponderada dos dados, normalizando as m√©tricas pela quantidade de lojas.\n",
    "    Esta fun√ß√£o tamb√©m cria colunas padronizadas (vendas, vendas_oculos, clientes, receita, lojas)\n",
    "    a partir das colunas existentes (por exemplo: vendas_oculos_grau, num_clientes, receita_total, num_lojas).\n",
    "    \"\"\"\n",
    "    df_ponderado = df.copy()\n",
    "\n",
    "    # Criar aliases/nomes padronizados quando poss√≠vel\n",
    "    if 'vendas_oculos_grau' in df_ponderado.columns:\n",
    "        df_ponderado['vendas_oculos'] = df_ponderado['vendas_oculos_grau']\n",
    "        # cria alias 'vendas' se n√£o existir\n",
    "        if 'vendas' not in df_ponderado.columns:\n",
    "            df_ponderado['vendas'] = df_ponderado['vendas_oculos_grau']\n",
    "    \n",
    "    if 'num_clientes' in df_ponderado.columns:\n",
    "        df_ponderado['clientes'] = df_ponderado['num_clientes']\n",
    "    \n",
    "    if 'receita_total' in df_ponderado.columns:\n",
    "        df_ponderado['receita'] = df_ponderado['receita_total']\n",
    "    \n",
    "    if 'num_lojas' in df_ponderado.columns:\n",
    "        df_ponderado['lojas'] = df_ponderado['num_lojas']\n",
    "\n",
    "    # M√©tricas para ponderar (usar nomes padronizados)\n",
    "    metricas_ponderacao = ['vendas', 'vendas_oculos', 'clientes', 'receita']\n",
    "\n",
    "    for metrica in metricas_ponderacao:\n",
    "        if metrica in df_ponderado.columns and 'lojas' in df_ponderado.columns:\n",
    "            # Criar vers√£o normalizada (por loja)\n",
    "            df_ponderado[f'{metrica}_por_loja'] = df_ponderado[metrica] / df_ponderado['lojas']\n",
    "            # Evitar divis√£o por zero\n",
    "            df_ponderado[f'{metrica}_por_loja'] = df_ponderado[f'{metrica}_por_loja'].fillna(0)\n",
    "            # Tratar infinitos (casos onde lojas = 0)\n",
    "            df_ponderado[f'{metrica}_por_loja'] = df_ponderado[f'{metrica}_por_loja'].replace([float('inf'), -float('inf')], 0)\n",
    "\n",
    "    print(f\"Colunas originais dispon√≠veis: {list(df.columns)}\")\n",
    "    print(f\"Colunas ap√≥s padroniza√ß√£o: {list(df_ponderado.columns)}\")\n",
    "    \n",
    "    return df_ponderado\n",
    "\n",
    "# Aplicar pondera√ß√£o aos dados\n",
    "print(\"Criando dados ponderados...\")\n",
    "\n",
    "print(\"\\nProcessando dados de S√£o Paulo...\")\n",
    "dados_sp_ponderado = criar_dados_ponderados(dados_sp)\n",
    "\n",
    "print(\"\\nProcessando dados de Outros Estados...\")\n",
    "dados_outros_ponderado = criar_dados_ponderados(dados_outros_estados)\n",
    "\n",
    "print(\"\\nDados ponderados criados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b568e75",
   "metadata": {},
   "source": [
    "## An√°lise Comparativa ‚Äì Totais Brutos vs M√©tricas Normalizadas por Loja\n",
    "\n",
    "Esta c√©lula realiza uma verifica√ß√£o direta do impacto da normaliza√ß√£o (pondera√ß√£o por n√∫mero de lojas) sobre as m√©tricas principais, permitindo comparar rapidamente valores absolutos agregados por localiza√ß√£o com suas vers√µes ‚Äúpor loja‚Äù, evitando interpreta√ß√µes enviesadas por concentra√ß√£o operacional.\n",
    "\n",
    "**1. Objetivo Central**  \n",
    "Fornecer um contraste imediato entre:  \n",
    "- M√©tricas originais em n√≠vel de localiza√ß√£o (totais brutos)  \n",
    "- M√©tricas derivadas normalizadas (`*_por_loja`) ap√≥s a etapa de pondera√ß√£o  \n",
    "Isso ajuda a diferenciar se altos volumes derivam de mercado potencial ou apenas de maior presen√ßa f√≠sica (mais lojas na mesma regi√£o).\n",
    "\n",
    "**2. Estruturas Utilizadas**  \n",
    "- DataFrames de base: `dados_sp` e `dados_outros_estados` (valores totais)  \n",
    "- DataFrames p√≥s-pondera√ß√£o: `dados_sp_ponderado` e `dados_outros_ponderado` (com colunas `vendas_por_loja`, `receita_por_loja`, `clientes_por_loja`)  \n",
    "\n",
    "**3. M√©tricas Comparadas (se existirem)**  \n",
    "Antes (totais):  \n",
    "- `vendas_oculos_grau`  \n",
    "- `receita_total`  \n",
    "- `num_clientes`  \n",
    "Depois (normalizadas):  \n",
    "- `vendas_por_loja`  \n",
    "- `receita_por_loja`  \n",
    "- `clientes_por_loja`  \n",
    "\n",
    "**4. L√≥gica da Impress√£o**  \n",
    "Para cada recorte (S√£o Paulo / Outros Estados):  \n",
    "1. Exibe m√©dias simples das m√©tricas totais (pr√©-normaliza√ß√£o)  \n",
    "2. Exibe m√©dias das m√©tricas por loja (p√≥s-normaliza√ß√£o)  \n",
    "3. Aplica checagem defensiva: s√≥ imprime quando a coluna existe  \n",
    "\n",
    "**5. Por Que Isso Importa**  \n",
    "- Evita supervaloriza√ß√£o de regi√µes infladas por alta densidade de lojas  \n",
    "- Evidencia efici√™ncia m√©dia por unidade operacional  \n",
    "- Ajuda a priorizar expans√£o onde o ‚Äúpor loja‚Äù ainda √© alto (potencial adicional)  \n",
    "\n",
    "**6. Diferen√ßa Conceitual (Totais vs Por Loja)**  \n",
    "| Aspecto | Totais Brutos | Normalizado por Loja |  \n",
    "|---------|---------------|----------------------|  \n",
    "| Influ√™ncia de concentra√ß√£o | Alta | Reduzida |  \n",
    "| Sens√≠vel a satura√ß√£o | N√£o distingue | Exp√µe quedas de efici√™ncia |  \n",
    "| Uso | Panorama macro | Compara√ß√£o justa entre regi√µes |  \n",
    "| Risco | Superestimar regi√µes densas | Pode mascarar escala absoluta |  \n",
    "\n",
    "**7. Casos de Interpreta√ß√£o**  \n",
    "- Alta venda total + baixa venda por loja ‚Üí Satura√ß√£o / canibaliza√ß√£o poss√≠vel  \n",
    "- Baixa venda total + alta venda por loja ‚Üí Potencial para expans√£o incremental  \n",
    "- Ambos altos ‚Üí Mercado maduro e escal√°vel  \n",
    "- Ambos baixos ‚Üí Baixa prioridade imediata  \n",
    "\n",
    "**8. Valida√ß√µes Impl√≠citas**  \n",
    "- Confirma que a etapa anterior de cria√ß√£o das colunas ponderadas ocorreu com sucesso  \n",
    "- Garante coer√™ncia nominal das colunas antes de uso em modelos de prioriza√ß√£o  \n",
    "- Detecta aus√™ncias inesperadas (mensagens ‚Äúcoluna n√£o encontrada‚Äù)  \n",
    "\n",
    "**9. Limita√ß√µes Deliberadas**  \n",
    "- N√£o calcula distribui√ß√£o (min/max/percentis)  \n",
    "- N√£o trata outliers ou dispers√£o  \n",
    "- N√£o faz ajuste temporal (tudo est√°tico no recorte atual)  \n",
    "- N√£o cruza com densidade de clientes externos ou demografia  \n",
    "\n",
    "**10. Quando Reexecutar**  \n",
    "- Ap√≥s recalcular pondera√ß√µes com novas lojas  \n",
    "- Ap√≥s filtrar regi√µes / aplicar cortes geogr√°ficos  \n",
    "- Antes de gerar dashboards de efici√™ncia operacional  \n",
    "\n",
    "**11. Poss√≠veis Extens√µes Futuras**  \n",
    "- Adicionar m√©tricas por cliente (ex.: receita_por_cliente)  \n",
    "- Calcular √≠ndice de satura√ß√£o (lojas / clientes)  \n",
    "- Incluir ranking autom√°tico de efici√™ncia relativa  \n",
    "- Exportar snapshot comparativo para auditoria  \n",
    "\n",
    "**12. Valor Estrat√©gico**  \n",
    "Fornece leitura dual (escala vs efici√™ncia) indispens√°vel para:  \n",
    "- Decidir entre abertura de novas lojas ou otimiza√ß√£o das existentes  \n",
    "- Alimentar modelos supervisionados com features enriquecidas  \n",
    "- Justificar aloca√ß√£o de capital entre expans√£o horizontal vs densifica√ß√£o  \n",
    "\n",
    "**Resumo:** A c√©lula funciona como painel r√°pido de consist√™ncia e interpreta√ß√£o operacional, contrastando desempenho bruto com efici√™ncia ajustada por loja, sustentando decis√µes mais equilibradas sobre prioriza√ß√£o geogr√°fica de expans√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db34c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== AN√ÅLISE COMPARATIVA - ANTES vs DEPOIS DA PONDERA√á√ÉO ===\")\n",
    "\n",
    "print(\"\\nS√ÉO PAULO:\")\n",
    "print(\"ANTES (valores totais):\")\n",
    "# usar colunas originais dispon√≠veis\n",
    "if 'vendas_oculos_grau' in dados_sp.columns:\n",
    "    print(f\"  Vendas m√©dias por regi√£o: {dados_sp['vendas_oculos_grau'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Vendas m√©dias por regi√£o: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'receita_total' in dados_sp.columns:\n",
    "    print(f\"  Receita m√©dia por regi√£o: R$ {dados_sp['receita_total'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Receita m√©dia por regi√£o: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'num_clientes' in dados_sp.columns:\n",
    "    print(f\"  Clientes m√©dios por regi√£o: {dados_sp['num_clientes'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Clientes m√©dios por regi√£o: (coluna n√£o encontrada)\")\n",
    "\n",
    "print(\"DEPOIS (valores por loja):\")\n",
    "if 'vendas_por_loja' in dados_sp_ponderado.columns:\n",
    "    print(f\"  Vendas m√©dias por loja: {dados_sp_ponderado['vendas_por_loja'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"  Vendas m√©dias por loja: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'receita_por_loja' in dados_sp_ponderado.columns:\n",
    "    print(f\"  Receita m√©dia por loja: R$ {dados_sp_ponderado['receita_por_loja'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Receita m√©dia por loja: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'clientes_por_loja' in dados_sp_ponderado.columns:\n",
    "    print(f\"  Clientes m√©dios por loja: {dados_sp_ponderado['clientes_por_loja'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"  Clientes m√©dios por loja: (coluna n√£o encontrada)\")\n",
    "\n",
    "print(\"\\nOUTROS ESTADOS:\")\n",
    "print(\"ANTES (valores totais):\")\n",
    "if 'vendas_oculos_grau' in dados_outros_estados.columns:\n",
    "    print(f\"  Vendas m√©dias por regi√£o: {dados_outros_estados['vendas_oculos_grau'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Vendas m√©dias por regi√£o: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'receita_total' in dados_outros_estados.columns:\n",
    "    print(f\"  Receita m√©dia por regi√£o: R$ {dados_outros_estados['receita_total'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Receita m√©dia por regi√£o: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'num_clientes' in dados_outros_estados.columns:\n",
    "    print(f\"  Clientes m√©dios por regi√£o: {dados_outros_estados['num_clientes'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Clientes m√©dios por regi√£o: (coluna n√£o encontrada)\")\n",
    "\n",
    "print(\"DEPOIS (valores por loja):\")\n",
    "if 'vendas_por_loja' in dados_outros_ponderado.columns:\n",
    "    print(f\"  Vendas m√©dias por loja: {dados_outros_ponderado['vendas_por_loja'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"  Vendas m√©dias por loja: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'receita_por_loja' in dados_outros_ponderado.columns:\n",
    "    print(f\"  Receita m√©dia por loja: R$ {dados_outros_ponderado['receita_por_loja'].mean():.0f}\")\n",
    "else:\n",
    "    print(\"  Receita m√©dia por loja: (coluna n√£o encontrada)\")\n",
    "\n",
    "if 'clientes_por_loja' in dados_outros_ponderado.columns:\n",
    "    print(f\"  Clientes m√©dios por loja: {dados_outros_ponderado['clientes_por_loja'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"  Clientes m√©dios por loja: (coluna n√£o encontrada)\")\n",
    "\n",
    "print(\"\\n=== AN√ÅLISE COMPARATIVA CONCLU√çDA ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a150549",
   "metadata": {},
   "source": [
    "## Exporta√ß√£o Final de Sugest√µes para o Modelo Supervisionado \n",
    "\n",
    "Esta c√©lula consolida todo o pipeline n√£o supervisionado em um conjunto estruturado de sugest√µes de expans√£o, pronto para ser consumido por um modelo supervisionado ou an√°lises executivas.\n",
    "\n",
    "### 1. Objetivo\n",
    "Gerar um CSV padronizado contendo, por cluster (SP e Outros Estados):\n",
    "- Coordenadas dos centr√≥ides (pontos ideais te√≥ricos)\n",
    "- Local real mais pr√≥ximo (tradu√ß√£o operacional)\n",
    "- M√©tricas agregadas e normalizadas (vendas, clientes, receita, efici√™ncia por loja)\n",
    "- Prioriza√ß√£o interna por escopo\n",
    "\n",
    "### 2. Etapas Principais do C√≥digo\n",
    "1. Verifica√ß√£o/defini√ß√£o de k (SP e Outros) com fallback inteligente  \n",
    "2. Reclusteriza√ß√£o final com os k ‚Äúoficiais‚Äù usando pesos (quando dispon√≠veis)  \n",
    "3. Gera√ß√£o de centr√≥ides e mapeamento para regi√µes reais existentes  \n",
    "4. Constru√ß√£o da lista de sugest√µes (SP e OUTROS) com m√©tricas:\n",
    "   - Totais do cluster (via agrega√ß√£o)\n",
    "   - Valores da regi√£o representativa (regi√£o mais pr√≥xima do centr√≥ide)\n",
    "   - Normaliza√ß√µes por loja (prote√ß√£o contra divis√£o por zero)\n",
    "5. Cria√ß√£o do DataFrame final + ranking de prioridade por escopo  \n",
    "6. Exporta√ß√£o para: `../../database/dataset gerado/sugestoes_expansao_para_supervisionado.csv`  \n",
    "7. Resumo validado (shape, preview e agregados por escopo)\n",
    "\n",
    "### 3. Campos-Chave do Output\n",
    "- escopo: SP | OUTROS  \n",
    "- cluster_id / prioridade_no_escopo  \n",
    "- latitude_centroid / longitude_centroid  \n",
    "- latitude_sugerida / longitude_sugerida  \n",
    "- estado / regiao_chilli  \n",
    "- potencial_score_cluster_medio / potencial_score_cluster_max  \n",
    "- vendas_oculos_grau_regiao, num_clientes_regiao, receita_total_regiao, num_lojas_regiao  \n",
    "- vendas_por_loja_regiao, receita_por_loja_regiao, clientes_por_loja_regiao  \n",
    "- distancia_km_do_centroid (Haversine)\n",
    "\n",
    "### 4. L√≥gicas de Seguran√ßa\n",
    "- Fallback para k se arrays de silhouette n√£o existirem  \n",
    "- Try/except ao usar sample_weight  \n",
    "- Prote√ß√£o para DataFrames vazios  \n",
    "- Normaliza√ß√£o segura (divis√£o s√≥ se num_lojas > 0)  \n",
    "\n",
    "### 5. Como Utilizar no Pr√≥ximo Modelo\n",
    "- Servir como base rotulada (target pode ser potencial m√©dio, receita futura, etc.)  \n",
    "- Criar features adicionais combinando scores + dist√¢ncia ao centr√≥ide  \n",
    "- Validar coer√™ncia com dados temporais (caso adicione s√©rie hist√≥rica depois)\n",
    "\n",
    "### 6. Sinais de Sucesso\n",
    "- Mensagens ‚úì para ambos os blocos (SP e Outros)  \n",
    "- CSV criado sem valores NaN cr√≠ticos  \n",
    "- Prioriza√ß√£o num√©rica cont√≠nua (1‚Ä¶N por escopo)\n",
    "\n",
    "### 7. Poss√≠veis Extens√µes Futuras\n",
    "- Inclus√£o de densidade concorrencial externa  \n",
    "- Enriquecimento com demografia ou renda m√©dia  \n",
    "- Ajuste din√¢mico de pesos por cen√°rio estrat√©gico\n",
    "\n",
    "Em resumo: esta c√©lula ‚Äúmaterializa‚Äù o aprendizado espacial em sugest√µes acion√°veis, com rastreabilidade, prioriza√ß√£o e estrutura pronta para modelagem supervisionada ou decis√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTA√á√ÉO DE SUGEST√ïES PARA O MODELO SUPERVISIONADO\n",
    "print(\"=\"*80)\n",
    "print(\"EXPORTANDO SUGEST√ïES DE LOCAIS PARA O MODELO SUPERVISIONADO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1) GARANTINDO OS KS IDEAIS\n",
    "print(\"\\n1. Determinando Ks ideais...\")\n",
    "\n",
    "# Verificar se best_k_sp e best_k_outros existem\n",
    "if 'best_k_sp' in globals() and 'best_k_outros' in globals():\n",
    "    print(f\"   Usando Ks existentes: SP={best_k_sp}, Outros={best_k_outros}\")\n",
    "else:\n",
    "    # Tentar inferir dos arrays de silhouette\n",
    "    if 'silhouette_scores_sp' in globals() and len(silhouette_scores_sp) > 0:\n",
    "        best_k_sp = np.argmax(silhouette_scores_sp) + 2  # +2 porque silhouette come√ßa em k=2\n",
    "        print(f\"   K ideal para SP inferido do silhouette: {best_k_sp}\")\n",
    "    else:\n",
    "        best_k_sp = 2\n",
    "        print(f\"   K ideal para SP (fallback): {best_k_sp}\")\n",
    "    \n",
    "    if 'silhouette_scores_outros' in globals() and len(silhouette_scores_outros) > 0:\n",
    "        best_k_outros = np.argmax(silhouette_scores_outros) + 2\n",
    "        print(f\"   K ideal para Outros inferido do silhouette: {best_k_outros}\")\n",
    "    else:\n",
    "        best_k_outros = 3\n",
    "        print(f\"   K ideal para Outros (fallback): {best_k_outros}\")\n",
    "\n",
    "# 2) RECALCULANDO CLUSTERS COM KS IDEAIS\n",
    "print(\"\\n2. Recalculando clusters com Ks ideais...\")\n",
    "\n",
    "# Para SP\n",
    "print(f\"   Clustering SP com k={best_k_sp}...\")\n",
    "if 'X_sp' in globals() and 'dados_sp' in globals():\n",
    "    kmeans_sp_ideal = KMeans(n_clusters=best_k_sp, random_state=42, n_init=10)\n",
    "    \n",
    "    # Usar sample_weight se poss√≠vel\n",
    "    try:\n",
    "        weights_sp_ideal = dados_sp['vendas_oculos_grau'] + 1\n",
    "        clusters_sp_ideais = kmeans_sp_ideal.fit_predict(X_sp, sample_weight=weights_sp_ideal)\n",
    "    except:\n",
    "        clusters_sp_ideais = kmeans_sp_ideal.fit_predict(X_sp)\n",
    "    \n",
    "    centroids_sp = kmeans_sp_ideal.cluster_centers_\n",
    "    print(f\"   ‚úì SP: {best_k_sp} clusters criados\")\n",
    "else:\n",
    "    print(\"   ‚ö† Vari√°veis X_sp ou dados_sp n√£o encontradas\")\n",
    "    clusters_sp_ideais = None\n",
    "    centroids_sp = None\n",
    "\n",
    "# Para Outros Estados - verificar se dados_localizacao existe ou usar dados_outros_estados\n",
    "print(f\"   Clustering Outros Estados com k={best_k_outros}...\")\n",
    "\n",
    "if 'dados_localizacao' in globals():\n",
    "    dados_expansao_outros = dados_localizacao[dados_localizacao['estado'] != 'SP'].copy()\n",
    "elif 'dados_outros_estados' in globals():\n",
    "    dados_expansao_outros = dados_outros_estados.copy()\n",
    "    print(\"   Usando dados_outros_estados como fonte\")\n",
    "else:\n",
    "    dados_expansao_outros = pd.DataFrame()\n",
    "    print(\"   ‚ö† Nenhuma fonte de dados para outros estados encontrada\")\n",
    "\n",
    "if not dados_expansao_outros.empty:\n",
    "    X_outros_ideal = dados_expansao_outros[['longitude', 'latitude']].values\n",
    "    kmeans_outros_ideal = KMeans(n_clusters=best_k_outros, random_state=42, n_init=10)\n",
    "    \n",
    "    # Usar sample_weight se poss√≠vel\n",
    "    try:\n",
    "        # Tentar usar potencial_score, se n√£o existir usar vendas_oculos_grau\n",
    "        if 'potencial_score' in dados_expansao_outros.columns:\n",
    "            weights_outros_ideal = dados_expansao_outros['potencial_score'] + 1\n",
    "        elif 'vendas_oculos_grau' in dados_expansao_outros.columns:\n",
    "            weights_outros_ideal = dados_expansao_outros['vendas_oculos_grau'] + 1\n",
    "        else:\n",
    "            weights_outros_ideal = None\n",
    "            \n",
    "        if weights_outros_ideal is not None:\n",
    "            clusters_outros_ideais = kmeans_outros_ideal.fit_predict(X_outros_ideal, sample_weight=weights_outros_ideal)\n",
    "        else:\n",
    "            clusters_outros_ideais = kmeans_outros_ideal.fit_predict(X_outros_ideal)\n",
    "    except:\n",
    "        clusters_outros_ideais = kmeans_outros_ideal.fit_predict(X_outros_ideal)\n",
    "    \n",
    "    centroids_outros = kmeans_outros_ideal.cluster_centers_\n",
    "    print(f\"   ‚úì Outros Estados: {best_k_outros} clusters criados\")\n",
    "else:\n",
    "    print(\"   ‚ö† Dados de outros estados vazios\")\n",
    "    clusters_outros_ideais = None\n",
    "    centroids_outros = None\n",
    "\n",
    "# 3) GERANDO SUGEST√ïES POR CLUSTER\n",
    "print(\"\\n3. Gerando sugest√µes de locais...\")\n",
    "\n",
    "sugestoes_lista = []\n",
    "\n",
    "# Fun√ß√£o auxiliar para encontrar regi√£o mais pr√≥xima\n",
    "def encontrar_regiao_proxima(lat_centro, lon_centro, df_regioes):\n",
    "    if df_regioes.empty:\n",
    "        return None, float('inf')\n",
    "    \n",
    "    distancias = []\n",
    "    for idx, row in df_regioes.iterrows():\n",
    "        dist = haversine_distance(lat_centro, lon_centro, row['latitude'], row['longitude'])\n",
    "        distancias.append((dist, idx))\n",
    "    \n",
    "    distancia_min, idx_min = min(distancias)\n",
    "    return df_regioes.loc[idx_min], distancia_min\n",
    "\n",
    "# Sugest√µes para SP\n",
    "if clusters_sp_ideais is not None and centroids_sp is not None:\n",
    "    print(f\"   Processando {best_k_sp} clusters de SP...\")\n",
    "    dados_sp_com_clusters = dados_sp.copy()\n",
    "    dados_sp_com_clusters['cluster'] = clusters_sp_ideais\n",
    "    \n",
    "    for cluster_id in range(best_k_sp):\n",
    "        cluster_mask = clusters_sp_ideais == cluster_id\n",
    "        cluster_data = dados_sp_com_clusters[cluster_mask]\n",
    "        \n",
    "        if cluster_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Centr√≥ide do cluster\n",
    "        centro_lon, centro_lat = centroids_sp[cluster_id]\n",
    "        \n",
    "        # Encontrar regi√£o real mais pr√≥xima\n",
    "        regiao_sugerida, distancia_km = encontrar_regiao_proxima(centro_lat, centro_lon, dados_sp)\n",
    "        \n",
    "        if regiao_sugerida is None:\n",
    "            continue\n",
    "        \n",
    "        # M√©tricas agregadas do cluster - usar nomes corretos das colunas\n",
    "        potencial_medio = cluster_data['potencial_score'].mean() if 'potencial_score' in cluster_data.columns else 0\n",
    "        potencial_max = cluster_data['potencial_score'].max() if 'potencial_score' in cluster_data.columns else 0\n",
    "        \n",
    "        vendas_total = cluster_data['vendas_oculos_grau'].sum() if 'vendas_oculos_grau' in cluster_data.columns else 0\n",
    "        clientes_total = cluster_data['num_clientes'].sum() if 'num_clientes' in cluster_data.columns else 0\n",
    "        receita_total = cluster_data['receita_total'].sum() if 'receita_total' in cluster_data.columns else 0\n",
    "        lojas_total = cluster_data['num_lojas'].sum() if 'num_lojas' in cluster_data.columns else 0\n",
    "        \n",
    "        # M√©tricas da regi√£o sugerida - usar nomes corretos das colunas\n",
    "        vendas_regiao = regiao_sugerida['vendas_oculos_grau'] if 'vendas_oculos_grau' in regiao_sugerida.index else 0\n",
    "        clientes_regiao = regiao_sugerida['num_clientes'] if 'num_clientes' in regiao_sugerida.index else 0\n",
    "        receita_regiao = regiao_sugerida['receita_total'] if 'receita_total' in regiao_sugerida.index else 0\n",
    "        lojas_regiao = regiao_sugerida['num_lojas'] if 'num_lojas' in regiao_sugerida.index else 0\n",
    "        \n",
    "        # Normaliza√ß√µes por loja (com prote√ß√£o)\n",
    "        vendas_por_loja = vendas_regiao / lojas_regiao if lojas_regiao > 0 else 0\n",
    "        clientes_por_loja = clientes_regiao / lojas_regiao if lojas_regiao > 0 else 0\n",
    "        receita_por_loja = receita_regiao / lojas_regiao if lojas_regiao > 0 else 0\n",
    "        \n",
    "        sugestoes_lista.append({\n",
    "            'escopo': 'SP',\n",
    "            'cluster_id': int(cluster_id),\n",
    "            'latitude_centroid': float(centro_lat),\n",
    "            'longitude_centroid': float(centro_lon),\n",
    "            'latitude_sugerida': float(regiao_sugerida['latitude']),\n",
    "            'longitude_sugerida': float(regiao_sugerida['longitude']),\n",
    "            'estado': regiao_sugerida['estado'] if 'estado' in regiao_sugerida.index else 'SP',\n",
    "            'regiao_chilli': regiao_sugerida.get('regiao_chilli', 'S√ÉO PAULO'),\n",
    "            'potencial_score_cluster_medio': float(potencial_medio),\n",
    "            'potencial_score_cluster_max': float(potencial_max),\n",
    "            'vendas_oculos_grau_regiao': int(vendas_regiao),\n",
    "            'num_clientes_regiao': int(clientes_regiao),\n",
    "            'receita_total_regiao': float(receita_regiao),\n",
    "            'num_lojas_regiao': int(lojas_regiao),\n",
    "            'vendas_por_loja_regiao': float(vendas_por_loja),\n",
    "            'receita_por_loja_regiao': float(receita_por_loja),\n",
    "            'clientes_por_loja_regiao': float(clientes_por_loja),\n",
    "            'distancia_km_do_centroid': float(distancia_km)\n",
    "        })\n",
    "\n",
    "# Sugest√µes para Outros Estados\n",
    "if clusters_outros_ideais is not None and centroids_outros is not None and not dados_expansao_outros.empty:\n",
    "    print(f\"   Processando {best_k_outros} clusters de Outros Estados...\")\n",
    "    dados_expansao_outros['cluster'] = clusters_outros_ideais\n",
    "    \n",
    "    for cluster_id in range(best_k_outros):\n",
    "        cluster_mask = clusters_outros_ideais == cluster_id\n",
    "        cluster_data = dados_expansao_outros[cluster_mask]\n",
    "        \n",
    "        if cluster_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Centr√≥ide do cluster\n",
    "        centro_lon, centro_lat = centroids_outros[cluster_id]\n",
    "        \n",
    "        # Encontrar regi√£o real mais pr√≥xima\n",
    "        regiao_sugerida, distancia_km = encontrar_regiao_proxima(centro_lat, centro_lon, dados_expansao_outros)\n",
    "        \n",
    "        if regiao_sugerida is None:\n",
    "            continue\n",
    "        \n",
    "        # M√©tricas agregadas do cluster - usar nomes corretos das colunas\n",
    "        potencial_medio = cluster_data['potencial_score'].mean() if 'potencial_score' in cluster_data.columns else 0\n",
    "        potencial_max = cluster_data['potencial_score'].max() if 'potencial_score' in cluster_data.columns else 0\n",
    "        \n",
    "        vendas_total = cluster_data['vendas_oculos_grau'].sum() if 'vendas_oculos_grau' in cluster_data.columns else 0\n",
    "        clientes_total = cluster_data['num_clientes'].sum() if 'num_clientes' in cluster_data.columns else 0\n",
    "        receita_total = cluster_data['receita_total'].sum() if 'receita_total' in cluster_data.columns else 0\n",
    "        lojas_total = cluster_data['num_lojas'].sum() if 'num_lojas' in cluster_data.columns else 0\n",
    "        \n",
    "        # M√©tricas da regi√£o sugerida - usar nomes corretos das colunas\n",
    "        vendas_regiao = regiao_sugerida['vendas_oculos_grau'] if 'vendas_oculos_grau' in regiao_sugerida.index else 0\n",
    "        clientes_regiao = regiao_sugerida['num_clientes'] if 'num_clientes' in regiao_sugerida.index else 0\n",
    "        receita_regiao = regiao_sugerida['receita_total'] if 'receita_total' in regiao_sugerida.index else 0\n",
    "        lojas_regiao = regiao_sugerida['num_lojas'] if 'num_lojas' in regiao_sugerida.index else 0\n",
    "        \n",
    "        # Normaliza√ß√µes por loja (com prote√ß√£o)\n",
    "        vendas_por_loja = vendas_regiao / lojas_regiao if lojas_regiao > 0 else 0\n",
    "        clientes_por_loja = clientes_regiao / lojas_regiao if lojas_regiao > 0 else 0\n",
    "        receita_por_loja = receita_regiao / lojas_regiao if lojas_regiao > 0 else 0\n",
    "        \n",
    "        sugestoes_lista.append({\n",
    "            'escopo': 'OUTROS',\n",
    "            'cluster_id': int(cluster_id),\n",
    "            'latitude_centroid': float(centro_lat),\n",
    "            'longitude_centroid': float(centro_lon),\n",
    "            'latitude_sugerida': float(regiao_sugerida['latitude']),\n",
    "            'longitude_sugerida': float(regiao_sugerida['longitude']),\n",
    "            'estado': regiao_sugerida['estado'] if 'estado' in regiao_sugerida.index else 'DESCONHECIDO',\n",
    "            'regiao_chilli': regiao_sugerida.get('regiao_chilli', 'DESCONHECIDA'),\n",
    "            'potencial_score_cluster_medio': float(potencial_medio),\n",
    "            'potencial_score_cluster_max': float(potencial_max),\n",
    "            'vendas_oculos_grau_regiao': int(vendas_regiao),\n",
    "            'num_clientes_regiao': int(clientes_regiao),\n",
    "            'receita_total_regiao': float(receita_regiao),\n",
    "            'num_lojas_regiao': int(lojas_regiao),\n",
    "            'vendas_por_loja_regiao': float(vendas_por_loja),\n",
    "            'receita_por_loja_regiao': float(receita_por_loja),\n",
    "            'clientes_por_loja_regiao': float(clientes_por_loja),\n",
    "            'distancia_km_do_centroid': float(distancia_km)\n",
    "        })\n",
    "\n",
    "# 4) MONTANDO DATAFRAME FINAL\n",
    "print(\"\\n4. Montando DataFrame final...\")\n",
    "\n",
    "if sugestoes_lista:\n",
    "    sugestoes_expansao = pd.DataFrame(sugestoes_lista)\n",
    "    \n",
    "    # Adicionando prioridade dentro de cada escopo\n",
    "    sugestoes_expansao['prioridade_no_escopo'] = 0\n",
    "    \n",
    "    for escopo in ['SP', 'OUTROS']:\n",
    "        mask = sugestoes_expansao['escopo'] == escopo\n",
    "        if mask.any():\n",
    "            escopo_df = sugestoes_expansao[mask].copy()\n",
    "            escopo_df = escopo_df.sort_values('potencial_score_cluster_medio', ascending=False)\n",
    "            escopo_df['prioridade_no_escopo'] = range(1, len(escopo_df) + 1)\n",
    "            sugestoes_expansao.loc[mask, 'prioridade_no_escopo'] = escopo_df['prioridade_no_escopo'].values\n",
    "    \n",
    "    # Garantindo tipos corretos\n",
    "    sugestoes_expansao['cluster_id'] = sugestoes_expansao['cluster_id'].astype(int)\n",
    "    sugestoes_expansao['prioridade_no_escopo'] = sugestoes_expansao['prioridade_no_escopo'].astype(int)\n",
    "    \n",
    "    print(f\"   ‚úì DataFrame criado com {len(sugestoes_expansao)} sugest√µes\")\n",
    "    \n",
    "    # 5) SALVANDO CSV\n",
    "    print(\"\\n5. Salvando arquivo CSV...\")\n",
    "    \n",
    "    output_path = \"../../database/dataset gerado/sugestoes_expansao_para_supervisionado.csv\"\n",
    "    sugestoes_expansao.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"   ‚úì Arquivo salvo em: {output_path}\")\n",
    "    print(f\"   ‚úì Shape: {sugestoes_expansao.shape}\")\n",
    "    print(f\"\\n   Primeiras 5 linhas:\")\n",
    "    print(sugestoes_expansao.head())\n",
    "    \n",
    "    # Resumo por escopo\n",
    "    print(f\"\\n   Resumo por escopo:\")\n",
    "    resumo = sugestoes_expansao.groupby('escopo').agg({\n",
    "        'cluster_id': 'count',\n",
    "        'potencial_score_cluster_medio': 'mean',\n",
    "        'vendas_oculos_grau_regiao': 'sum'\n",
    "    }).round(2)\n",
    "    resumo.columns = ['num_sugestoes', 'potencial_medio', 'vendas_total']\n",
    "    print(resumo)\n",
    "    \n",
    "else:\n",
    "    print(\"   ‚ö† Nenhuma sugest√£o gerada - verificar dados de entrada\")\n",
    "    sugestoes_expansao = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTA√á√ÉO CONCLU√çDA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1fa6b",
   "metadata": {},
   "source": [
    "# üéØ S√≠ntese Anal√≠tica Final: Pipeline de Clusteriza√ß√£o para Expans√£o Geogr√°fica\n",
    "\n",
    "## üìä Resumo Executivo do Pipeline\n",
    "\n",
    "Este notebook implementou um **pipeline completo de an√°lise n√£o supervisionada** para identificar oportunidades de expans√£o geogr√°fica da empresa Chillie Beans, processando dados de vendas, clientes, receita e presen√ßa operacional em diferentes regi√µes do Brasil. O trabalho foi estruturado em etapas sequenciais, desde o pr√©-processamento at√© a gera√ß√£o de sugest√µes acion√°veis para modelos supervisionados.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Metodologia e Etapas Implementadas**\n",
    "\n",
    "### **1. Prepara√ß√£o e Enriquecimento dos Dados**\n",
    "- **Limpeza e valida√ß√£o**: Tratamento de valores nulos, padroniza√ß√£o de nomenclaturas geogr√°ficas\n",
    "- **Enriquecimento geogr√°fico**: Integra√ß√£o de coordenadas latitude/longitude para an√°lise espacial\n",
    "- **Codifica√ß√£o categ√≥rica**: Convers√£o de vari√°veis geogr√°ficas (estado, cidade, bairro) em c√≥digos num√©ricos\n",
    "- **Score de potencial**: Cria√ß√£o de m√©trica composta baseada em vendas de √≥culos de grau, n√∫mero de clientes e receita total\n",
    "\n",
    "### **2. Segmenta√ß√£o Estrat√©gica**\n",
    "- **Separa√ß√£o SP vs Outros Estados**: Reconhecimento de que S√£o Paulo possui din√¢mica de mercado distinta\n",
    "- **Justificativa**: SP concentra maior densidade de lojas e diferentes padr√µes de consumo\n",
    "- **Datasets independentes**: `dados_sp` (108 registros) e `dados_outros_estados` (169 registros)\n",
    "\n",
    "### **3. An√°lise de Clustering por Segmento**\n",
    "\n",
    "#### **S√£o Paulo (SP)**\n",
    "- **M√©todo**: KMeans com range k=2 a 8\n",
    "- **K √≥timo identificado**: **2 clusters**\n",
    "- **M√©tricas de qualidade**:\n",
    "  - **Silhouette Score**: 0.6429 (boa separa√ß√£o)\n",
    "  - **Pseudo R¬≤**: 0.6488 (explica 64.88% da vari√¢ncia)\n",
    "- **Interpreta√ß√£o**: SP divide-se em dois mercados distintos - provavelmente capital/metropolitana vs interior\n",
    "\n",
    "#### **Outros Estados**\n",
    "- **M√©todo**: KMeans com range k=2 a 8  \n",
    "- **K √≥timo identificado**: **3 clusters**\n",
    "- **M√©tricas de qualidade**:\n",
    "  - **Silhouette Score**: 0.5476 (separa√ß√£o moderada)\n",
    "  - **Pseudo R¬≤**: 0.4833 (explica 48.33% da vari√¢ncia)\n",
    "- **Interpreta√ß√£o**: Tr√™s perfis regionais distintos - possivelmente Sul/Sudeste, Nordeste, Norte/Centro-Oeste\n",
    "\n",
    "### **4. Valida√ß√£o e Normaliza√ß√£o**\n",
    "- **Cria√ß√£o de m√©tricas por loja**: `vendas_por_loja`, `receita_por_loja`, `clientes_por_loja`\n",
    "- **Objetivo**: Evitar vi√©s de regi√µes com alta concentra√ß√£o de lojas\n",
    "- **Correla√ß√µes identificadas**: Valida√ß√£o de associa√ß√µes entre clusters e m√©tricas de performance\n",
    "- **Checkpoints estruturais**: Verifica√ß√£o de integridade antes e ap√≥s transforma√ß√µes\n",
    "\n",
    "### **5. Identifica√ß√£o de Oportunidades**\n",
    "- **Centr√≥ides geogr√°ficos**: C√°lculo de pontos ideais teoricamente √≥timos por cluster\n",
    "- **Mapeamento para locais reais**: Utiliza√ß√£o de dist√¢ncia Haversine para encontrar regi√µes existentes mais pr√≥ximas\n",
    "- **Prioriza√ß√£o interna**: Ranking de clusters por potencial m√©dio dentro de cada escopo (SP/Outros)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **Principais Resultados e Insights**\n",
    "\n",
    "### **üéØ Sugest√µes de Expans√£o Geradas**\n",
    "- **Total de sugest√µes**: **5 locais estrat√©gicos**\n",
    "- **Distribui√ß√£o**: 2 em S√£o Paulo + 3 em outros estados\n",
    "- **Arquivo exportado**: `sugestoes_expansao_para_supervisionado.csv` (19 colunas, estrutura completa)\n",
    "\n",
    "### **üîç An√°lise Detalhada por Escopo**\n",
    "\n",
    "#### **S√£o Paulo - 2 Clusters Identificados**\n",
    "\n",
    "**Cluster 0 (Prioridade 1)**:\n",
    "- **Localiza√ß√£o sugerida**: S√£o Paulo Capital (-23.55, -46.63)\n",
    "- **Potencial m√©dio**: 658.76 (alt√≠ssimo)\n",
    "- **Potencial m√°ximo**: 3,276.19\n",
    "- **Vendas atuais na regi√£o**: 1,061 √≥culos de grau\n",
    "- **Efici√™ncia**: 3.89 vendas/loja, 8.76 receita/loja\n",
    "- **Insight**: **Mercado maduro de alta performance** - regi√£o central/metropolitana com excelente retorno por loja\n",
    "\n",
    "**Cluster 1 (Prioridade 2)**:\n",
    "- **Localiza√ß√£o sugerida**: Interior de SP (-21.18, -47.81)\n",
    "- **Potencial m√©dio**: 17.45 (baixo-moderado)\n",
    "- **Vendas atuais**: 13 √≥culos de grau\n",
    "- **Efici√™ncia**: 3.25 vendas/loja, 8.99 receita/loja\n",
    "- **Insight**: **Mercado emergente no interior** - menor volume absoluto mas boa efici√™ncia por loja, indicando potencial de crescimento\n",
    "\n",
    "#### **Outros Estados - 3 Clusters Identificados**\n",
    "\n",
    "**Cluster 0 (Prioridade 1)**:\n",
    "- **Localiza√ß√£o sugerida**: Sudeste/MG (-21.56, -45.43)\n",
    "- **Potencial m√©dio**: 102.40\n",
    "- **Insight**: **Extens√£o natural do eixo SP-MG** - aproveita proximidade e similaridade cultural\n",
    "\n",
    "**Cluster 1 (Prioridade 2)**:\n",
    "- **Localiza√ß√£o sugerida**: Nordeste/AL (-9.65, -35.73)\n",
    "- **Potencial m√©dio**: 75.72\n",
    "- **Insight**: **Mercado nordestino consolidado** - regi√£o com bom potencial e presen√ßa existente\n",
    "\n",
    "**Cluster 2 (Prioridade 3)**:\n",
    "- **Localiza√ß√£o sugerida**: Norte/AP (0.04, -51.06)\n",
    "- **Potencial m√©dio**: 70.75\n",
    "- **Insight**: **Fronteira de expans√£o** - maior risco mas potencial inexplorado\n",
    "\n",
    "### **üìä Padr√µes Estrat√©gicos Identificados**\n",
    "\n",
    "#### **1. Concentra√ß√£o vs Efici√™ncia**\n",
    "- **SP domina volumes absolutos**: 1,074 vs 24 vendas (ratio 45:1)\n",
    "- **Efici√™ncia por loja similar**: SP e outros estados mant√™m produtividade compar√°vel\n",
    "- **Implica√ß√£o**: Expans√£o em SP deve focar densidade; outros estados devem priorizar penetra√ß√£o inicial\n",
    "\n",
    "#### **2. Qualidade dos Clusters**\n",
    "- **SP mais homog√™neo**: Silhouette 0.64 indica clusters bem definidos\n",
    "- **Outros estados mais diversos**: Silhouette 0.55 reflete maior heterogeneidade regional\n",
    "- **Pseudo R¬≤ sugere**: Vari√°veis escolhidas explicam melhor comportamento de SP (64.88%) vs outros (48.33%)\n",
    "\n",
    "#### **3. Dispers√£o Geogr√°fica**\n",
    "- **SP concentrado**: 2 clusters em raio relativamente pequeno\n",
    "- **Outros espalhados**: 3 clusters cobrindo Sudeste, Nordeste e Norte\n",
    "- **Estrat√©gia log√≠stica**: SP permite opera√ß√£o centralizada; outros exigem hubs regionais\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Insights Estrat√©gicos e Recomenda√ß√µes**\n",
    "\n",
    "### **üéØ Prioriza√ß√£o de Investimentos**\n",
    "\n",
    "**1. Curto Prazo (0-6 meses)**\n",
    "- **Foco em SP Cluster 0**: ROI mais previs√≠vel, mercado conhecido\n",
    "- **Teste em MG (Outros Cluster 0)**: Expans√£o natural do sucesso paulista\n",
    "\n",
    "**2. M√©dio Prazo (6-18 meses)**\n",
    "- **Interior de SP**: Aproveitar marca consolidada para penetrar interior\n",
    "- **Nordeste**: Estabelecer posi√ß√£o em mercado regional estrat√©gico\n",
    "\n",
    "**3. Longo Prazo (18+ meses)**\n",
    "- **Norte/Centro-Oeste**: Avalia√ß√£o ap√≥s aprendizado em outros mercados\n",
    "\n",
    "### **üî¨ Valida√ß√µes T√©cnicas Realizadas**\n",
    "\n",
    "**1. Robustez Metodol√≥gica**\n",
    "- M√∫ltiplas m√©tricas de valida√ß√£o (silhouette, in√©rcia, pseudo R¬≤)\n",
    "- Tratamento defensivo de dados (verifica√ß√µes de exist√™ncia, fallbacks)\n",
    "- Normaliza√ß√£o por loja para evitar vi√©s de escala\n",
    "\n",
    "**2. Consist√™ncia dos Resultados**\n",
    "- Correla√ß√µes positivas entre clusters e m√©tricas de performance\n",
    "- Padr√µes geogr√°ficos coerentes com conhecimento de mercado\n",
    "- M√©tricas balanceadas entre volume e efici√™ncia\n",
    "\n",
    "**3. Reprodutibilidade**\n",
    "- Pipeline estruturado e documentado\n",
    "- Fun√ß√µes reutiliz√°veis (Haversine, pseudo R¬≤, normaliza√ß√£o)\n",
    "- Exporta√ß√£o padronizada para etapas posteriores\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **Limita√ß√µes e Considera√ß√µes**\n",
    "\n",
    "### **1. Limita√ß√µes dos Dados**\n",
    "- **Snapshot temporal**: An√°lise baseada em momento espec√≠fico, sem s√©ries hist√≥ricas\n",
    "- **Vari√°veis limitadas**: N√£o incorpora sazonalidade, competi√ß√£o, demographics\n",
    "- **Granularidade**: Agrega√ß√£o por regi√£o pode mascarar micro-oportunidades\n",
    "\n",
    "### **2. Limita√ß√µes Metodol√≥gicas**\n",
    "- **KMeans assume clusters esf√©ricos**: Pode n√£o capturar formas geogr√°ficas complexas\n",
    "- **K √≥timo baseado em m√©tricas internas**: Valida√ß√£o externa com dados de neg√≥cio seria ideal\n",
    "- **Pesos uniformes**: N√£o diferencia import√¢ncia relativa de vendas vs receita vs clientes\n",
    "\n",
    "### **3. Riscos de Implementa√ß√£o**\n",
    "- **Centr√≥ides te√≥ricos**: Locais sugeridos podem n√£o ter infraestrutura adequada\n",
    "- **Sazonalidade n√£o modelada**: Padr√µes podem variar significativamente ao longo do ano\n",
    "- **Competi√ß√£o local**: An√°lise n√£o considera presen√ßa de concorrentes\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Pr√≥ximos Passos Recomendados**\n",
    "\n",
    "### **1. Valida√ß√£o de Campo**\n",
    "- Visitas t√©cnicas aos locais sugeridos\n",
    "- An√°lise de infraestrutura e acessibilidade\n",
    "- Pesquisa de presen√ßa concorrencial\n",
    "\n",
    "### **2. Enriquecimento de Dados**\n",
    "- Incorpora√ß√£o de dados demogr√°ficos (renda, idade, educa√ß√£o)\n",
    "- An√°lise de sazonalidade (dados hist√≥ricos)\n",
    "- M√©tricas de concorr√™ncia e market share\n",
    "\n",
    "### **3. Modelo Supervisionado**\n",
    "- Utiliza√ß√£o do CSV exportado como base para predi√ß√£o\n",
    "- Incorpora√ß√£o de vari√°veis externas (economia, clima, eventos)\n",
    "- Desenvolvimento de sistema de scoring din√¢mico\n",
    "\n",
    "### **4. Monitoramento Cont√≠nuo**\n",
    "- Pipeline de atualiza√ß√£o autom√°tica\n",
    "- Dashboard executivo com m√©tricas-chave\n",
    "- Sistema de alertas para mudan√ßas significativas\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Valor Estrat√©gico Entregue**\n",
    "\n",
    "### **1. Para o Neg√≥cio**\n",
    "- **5 oportunidades concretas** de expans√£o com localiza√ß√£o espec√≠fica\n",
    "- **Prioriza√ß√£o baseada em dados** para aloca√ß√£o de capital\n",
    "- **Redu√ß√£o de risco** atrav√©s de an√°lise quantitativa\n",
    "\n",
    "### **2. Para Analytics**\n",
    "- **Pipeline replic√°vel** para an√°lises futuras\n",
    "- **Base estruturada** para modelos supervisionados\n",
    "- **Metodologia escal√°vel** para outros produtos/regi√µes\n",
    "\n",
    "### **3. Para Opera√ß√µes**\n",
    "- **Coordenadas precisas** para scouting de pontos\n",
    "- **M√©tricas de benchmark** para avalia√ß√£o de performance\n",
    "- **Framework de decis√£o** para aprova√ß√£o de investimentos\n",
    "\n",
    "---\n",
    "\n",
    "**üèÜ Conclus√£o**: O pipeline de clusteriza√ß√£o n√£o supervisionado successfully identificou **padr√µes geogr√°ficos claros** nos dados de vendas da Chillie Beans, gerando **5 sugest√µes estrat√©gicas** de expans√£o com alta qualidade t√©cnica (Silhouette > 0.54, Pseudo R¬≤ > 0.48) e **relev√¢ncia operacional comprovada**. As descobertas suportam uma **estrat√©gia de expans√£o em duas frentes**: densifica√ß√£o no mercado maduro de S√£o Paulo e penetra√ß√£o seletiva em mercados emergentes regionais, proporcionando **funda√ß√£o s√≥lida** para decis√µes de investimento baseadas em evid√™ncias quantitativas.\n",
    "\n",
    "**Dataset Final**: `sugestoes_expansao_para_supervisionado.csv` - 5 locais √ó 19 vari√°veis estrat√©gicas prontas para modelagem supervisionada e execu√ß√£o operacional."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
