{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f3c920",
   "metadata": {},
   "source": [
    "### Modelo Supervisionado - KNN\n",
    "\n",
    "O modelo KNN (K-Vizinhos Mais Próximos) é um algoritmo de aprendizado supervisionado que classifica ou prevê um novo ponto de dados com base nos seus 'k' vizinhos mais próximos, assumindo que pontos similares tendem a se agrupar.\n",
    "\n",
    "**Objetivo do Modelo**: <br>\n",
    "Identificar desempenho de cada categoria nas localidades de novas lojas previstas\n",
    "\n",
    "**Datasets Utilizados**: <br>\n",
    "- dataset_limpo.csv: dataset com as informações de venda das lojas;\n",
    "\n",
    "- sugestoes_expansao_para_supervisionado.csv: dataset com as informações das novas localidades identificadas (previstas) para abertura de novas lojas (sem dados prévios de venda);\n",
    "\n",
    "- com coordenadas.csv: dataset com as coordenadas de localização de todas as lojas (para fazer uma análise específica, não apenas por grandes regiões como norte, sul, etc...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab19a59",
   "metadata": {},
   "source": [
    "### Etapa 1 - Importação de bibliotecas e dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efad9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Importação de bases\n",
    "df_vendas = pd.read_csv('../../database/dataset gerado/dataset_limpo.csv')\n",
    "df_sugestoes = pd.read_csv('../../database/dataset gerado/sugestoes_expansao_para_supervisionado.csv')\n",
    "df_coords = pd.read_csv('../../database/dataset gerado/com_coordenadas.csv')\n",
    "\n",
    "# Criando categorias mescladas (hierarquia Grupo_Produto + GRUPO_CHILLI)\n",
    "def norm_cat(s):\n",
    "    return (s.astype(str)\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .str.normalize('NFKD')\n",
    "        .str.encode('ascii', errors='ignore')\n",
    "        .str.decode('utf-8')\n",
    "        .str.replace(r'\\s+', '_', regex=True))\n",
    "\n",
    "df_vendas['Categoria_Combo'] = (\n",
    "    norm_cat(df_vendas['Dim_Produtos.Grupo_Produto']).fillna('DESCONHECIDO') + '_' + \n",
    "    norm_cat(df_vendas['Dim_Produtos.GRUPO_CHILLI']).fillna('DESCONHECIDO')\n",
    ")\n",
    "\n",
    "# Conferindo tamanho\n",
    "print(f\"Tamanho df_vendas: {df_vendas.shape}\")\n",
    "print(f\"Tamanho df_sugestoes: {df_sugestoes.shape}\")\n",
    "print(f\"Tamanho df_coords: {df_coords.shape}\")\n",
    "\n",
    "# Olhando as primeiras linhas\n",
    "display(df_vendas.head())\n",
    "display(df_sugestoes.head())\n",
    "display(df_coords.head())\n",
    "\n",
    "print('Categorias únicas (Grupo_Produto):', df_vendas['Dim_Produtos.Grupo_Produto'].nunique())\n",
    "print('Categorias únicas (GRUPO_CHILLI):', df_vendas['Dim_Produtos.GRUPO_CHILLI'].nunique())\n",
    "print('Categorias únicas (Categoria_Combo):', df_vendas['Categoria_Combo'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988749c2",
   "metadata": {},
   "source": [
    "### Etapa 2 - Padronizar texto das localizações e mergear coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a679eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronização de texto \n",
    "def padroniza_texto(s: pd.Series) -> pd.Series:\n",
    "    s = s.copy()\n",
    "    mask = s.notna()  # só transforma onde NÃO é nulo\n",
    "    s.loc[mask] = (s.loc[mask].astype(str)\n",
    "                   .str.strip()\n",
    "                   .str.upper()               \n",
    "                   .str.normalize('NFKD')     \n",
    "                   .str.encode('ascii', errors='ignore')\n",
    "                   .str.decode('utf-8'))\n",
    "    return s\n",
    "\n",
    "chave3 = [\"Dim_Lojas.Bairro_Emp\", \"Dim_Lojas.Cidade_Emp\", \"Dim_Lojas.Estado_Emp\"]\n",
    "\n",
    "# Padronizar chaves em AMBAS as tabelas\n",
    "for col in chave3:\n",
    "    df_vendas[col] = padroniza_texto(df_vendas[col])\n",
    "    df_coords[col] = padroniza_texto(df_coords[col])\n",
    "\n",
    "# Tornar df_coords ÚNICO por (Bairro, Cidade, Estado)\n",
    "# usar média = centróide quando houver múltiplos registros da mesma chave\n",
    "df_coords_uniq = (\n",
    "    df_coords\n",
    "      .groupby(chave3, as_index=False)[[\"Latitude\", \"Longitude\"]]\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# Checagem de unicidade após agregação\n",
    "assert not df_coords_uniq.duplicated(subset=chave3).any(), \"df_coords_uniq ainda tem duplicatas na chave.\"\n",
    "\n",
    "# MERGE LIMPO em um NOVO DataFrame + validação de cardinalidade\n",
    "df_vendas_geo = df_vendas.merge(\n",
    "    df_coords_uniq[chave3 + [\"Latitude\", \"Longitude\"]],\n",
    "    on=chave3,\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\",   \n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "print(\"Linhas base (df_vendas):\", len(df_vendas))\n",
    "print(\"Linhas pós-merge (df_vendas_geo):\", len(df_vendas_geo))\n",
    "print(\"Nulos em Latitude:\", df_vendas_geo[\"Latitude\"].isna().sum())\n",
    "print(df_vendas_geo[\"_merge\"].value_counts())\n",
    "\n",
    "# usar df_vendas_geo como base corrente\n",
    "df_vendas = df_vendas_geo.drop(columns=[\"_merge\"])\n",
    "\n",
    "# visualização incial da base\n",
    "display(df_vendas.head())\n",
    "\n",
    "# Análise dos faltantes\n",
    "df_vendas['Latitude']  = pd.to_numeric(df_vendas['Latitude'], errors='coerce')\n",
    "df_vendas['Longitude'] = pd.to_numeric(df_vendas['Longitude'], errors='coerce')\n",
    "\n",
    "print(\"Nulos em Latitude:\", df_vendas['Latitude'].isna().sum())\n",
    "print(\"Nulos em Longitude:\", df_vendas['Longitude'].isna().sum())\n",
    "\n",
    "faltantes = df_vendas[df_vendas['Latitude'].isna()]\n",
    "print(\"Total faltantes:\", len(faltantes))\n",
    "display(faltantes.head())\n",
    "\n",
    "# Agrupa e conta faltantes por cidade/estado\n",
    "mapa_faltantes = (\n",
    "    faltantes\n",
    "      .groupby([\"Dim_Lojas.Cidade_Emp\",\"Dim_Lojas.Estado_Emp\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"qtd_faltantes\")\n",
    "      .sort_values(\"qtd_faltantes\", ascending=False)\n",
    ")\n",
    "\n",
    "display(mapa_faltantes.head(20))\n",
    "\n",
    "# Agrupa e conta faltantes por categoria\n",
    "faltantes_cat = (\n",
    "    faltantes['Categoria_Combo']\n",
    "      .value_counts()\n",
    "      .reset_index()\n",
    "      .rename(columns={'index':'Categoria_Combo','Categoria_Combo':'qtd'})\n",
    ")\n",
    "\n",
    "display(faltantes_cat.head(10))\n",
    "\n",
    "# Remover registros sem latitude ou longitude\n",
    "df_vendas = df_vendas.dropna(subset=[\"Latitude\", \"Longitude\"]).reset_index(drop=True)\n",
    "\n",
    "# Conferindo resultado\n",
    "print(\"Shape após drop:\", df_vendas.shape)\n",
    "print(\"Nulos em Latitude:\", df_vendas['Latitude'].isna().sum())\n",
    "print(\"Nulos em Longitude:\", df_vendas['Longitude'].isna().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687aeb76",
   "metadata": {},
   "source": [
    "#### Descarte de registros sem coordenadas\n",
    "\n",
    "Durante a integração entre a base de vendas (`df_vendas`) e a base de coordenadas (`df_coords`), foram identificados **189 registros (≈0,6% do total)** sem informações de latitude/longitude. Esses casos estavam concentrados em apenas **9 cidades específicas** e distribuídos principalmente entre as categorias **ÓCULOS** e **ACESSÓRIOS**.\n",
    "\n",
    "Decidimos **dropar esses registros** porque:\n",
    "- representam uma fração mínima da base, com impacto estatístico irrelevante;\n",
    "- manter valores nulos exigiria tratamentos adicionais sem ganho real para o modelo;\n",
    "- garantir uma base **100% consistente geograficamente** simplifica as próximas etapas do pipeline (rotulagem e treinamento do KNN).\n",
    "\n",
    "Após o descarte, seguimos com **31.217 registros válidos**, assegurando consistência espacial sem comprometer a robustez da análise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5b1eb",
   "metadata": {},
   "source": [
    "### Etapa 3 - Gerar os rótulos (bom / mediano / ruim) por localidade × Categoria_Combo usando o volume de vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8203d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cidade = \"Dim_Lojas.Cidade_Emp\"\n",
    "col_uf     = \"Dim_Lojas.Estado_Emp\"\n",
    "\n",
    "# Cria localidade = \"CIDADE-UF\"\n",
    "df_vendas[\"localidade\"] = df_vendas[col_cidade].astype(str).str.strip() + \"-\" + df_vendas[col_uf].astype(str).str.strip()\n",
    "\n",
    "# Sanity check\n",
    "print(\"Localidades únicas:\", df_vendas[\"localidade\"].nunique())\n",
    "print(\"\\nTop 10 localidades por nº de linhas:\")\n",
    "display(df_vendas[\"localidade\"].value_counts().head(10))\n",
    "\n",
    "# Agregar volume de vendas por localidade e categoria\n",
    "df_volumes = (\n",
    "    df_vendas\n",
    "      .groupby([\"localidade\", \"Categoria_Combo\"])\n",
    "      .size() \n",
    "      .reset_index(name=\"volume_vendas\")\n",
    ")\n",
    "\n",
    "print(\"Shape da tabela agregada:\", df_volumes.shape)\n",
    "display(df_volumes.head(10))\n",
    "\n",
    "def rotular_categorias(grupo):\n",
    "\n",
    "    grupo = grupo.sort_values(by='volume_vendas', ascending=False).reset_index(drop=True)\n",
    "    n = len(grupo)\n",
    "\n",
    "    if n == 1:\n",
    "        grupo[\"desempenho\"] = \"bom\"\n",
    "    elif n == 2:\n",
    "        grupo[\"desempenho\"] = [\"bom\", \"ruim\"]\n",
    "    else:\n",
    "        tercil_1 = int(n / 3)\n",
    "        tercil_2 = int(2 * n / 3)\n",
    "\n",
    "        grupo.loc[:tercil_1 - 1, \"desempenho\"] = \"bom\"\n",
    "        grupo.loc[tercil_1+1: tercil_2, \"desempenho\"] = \"mediano\"\n",
    "        grupo.loc[tercil_2 + 1:, \"desempenho\"] = \"ruim\"\n",
    "    \n",
    "    return grupo\n",
    "\n",
    "df_rotulado = df_volumes.groupby(\"localidade\", group_keys=False).apply(rotular_categorias)\n",
    "\n",
    "print(\"Shape da tabela rotulada:\", df_rotulado.shape)\n",
    "display(df_rotulado.head(10))\n",
    "\n",
    "print(\"Visualização da distribuição geral dos rótulos:\")\n",
    "\n",
    "# Contagem geral dos rótulos\n",
    "label_counts = df_rotulado[\"desempenho\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "label_counts.plot(kind=\"bar\", color=[\"#2ca02c\",\"#ff7f0e\",\"#d62728\"])  # verde, laranja, vermelho\n",
    "plt.title(\"Distribuição geral dos rótulos de desempenho\")\n",
    "plt.xlabel(\"Rótulo\")\n",
    "plt.ylabel(\"Nº de pares localidade × categoria\")\n",
    "plt.show()\n",
    "\n",
    "print(label_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbf831",
   "metadata": {},
   "source": [
    "### Observação sobre a agregação (`localidade × Categoria_Combo`)\n",
    "\n",
    "Após a agregação, obtivemos **1.540 pares distintos de `localidade × Categoria_Combo`**.  \n",
    "Isso significa que, no período, **cada cidade não teve vendas em todas as 17 categorias possíveis**.  \n",
    "\n",
    "- Em localidades **menores**, aparecem apenas 1 ou 2 categorias.  \n",
    "- Em localidades **maiores**, espera-se encontrar um número maior de categorias ativas.  \n",
    "\n",
    "Esse comportamento é esperado e reflete a realidade de vendas:  \n",
    "cada mercado local concentra-se apenas em parte do portfólio da marca.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b572721",
   "metadata": {},
   "source": [
    "### Etapa 4 - Separar a base em conjunto de teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ETAPA 4 — Base de modelagem com LOCALIDADE como feature\n",
    "# =========================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 4.1) Ligar df_rotulado (localidade × Categoria_Combo × desempenho) às REGIÕES por localidade\n",
    "\n",
    "def moda(s: pd.Series):\n",
    "    m = s.mode(dropna=True)\n",
    "    return m.iloc[0] if not m.empty else np.nan\n",
    "\n",
    "# Tabela de atributos regionais por localidade (valor mais frequente)\n",
    "regioes_por_localidade = (\n",
    "    df_vendas.groupby(\"localidade\")[[\"Dim_Lojas.Regiao\", \"Dim_Lojas.REGIAO_CHILLI\"]]\n",
    "             .agg(moda)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "# Base final para modelagem: rótulo + regiões (mantém localidade explícita)\n",
    "df_modelo = (\n",
    "    df_rotulado.merge(regioes_por_localidade, on=\"localidade\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"df_modelo shape:\", df_modelo.shape)\n",
    "display(df_modelo.head())\n",
    "\n",
    "# 4.2) Preparar X e y (SEM vazamento: NÃO usar volume_vendas)\n",
    "y = df_modelo[\"desempenho\"].copy()\n",
    "\n",
    "# >>> AQUI entra a granularidade: 'localidade' como feature categórica\n",
    "X = df_modelo[[\"localidade\", \"Categoria_Combo\", \"Dim_Lojas.Regiao\", \"Dim_Lojas.REGIAO_CHILLI\"]].copy()\n",
    "\n",
    "# Remover linhas com rótulo nulo (se houver)\n",
    "mask = y.notna()\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# One-hot encoding das categóricas (inclui LOCALIDADE)\n",
    "X = pd.get_dummies(\n",
    "    X,\n",
    "    columns=[\"localidade\", \"Categoria_Combo\", \"Dim_Lojas.Regiao\", \"Dim_Lojas.REGIAO_CHILLI\"],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "print(\"Shape de X após one-hot:\", X.shape)\n",
    "print(\"Classes de y:\", y.unique())\n",
    "\n",
    "# 4.3) Split treino/teste estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"| X_test:\", X_test.shape)\n",
    "print(\"\\nDistribuição rótulos treino (%):\")\n",
    "print((y_train.value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "print(\"\\nDistribuição rótulos teste (%):\")\n",
    "print((y_test.value_counts(normalize=True) * 100).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09482f4f",
   "metadata": {},
   "source": [
    "### Etapa 5 - Treinar e avaliar modelo (acurácia, matriz de confusão e classification report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee506d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Testar k de 1 até 30\n",
    "k_values = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "best_k = k_values[np.argmax(cv_scores)]\n",
    "best_score = max(cv_scores)\n",
    "\n",
    "print(f\"Melhor k encontrado: {best_k} com acurácia média de {best_score:.2%}\")\n",
    "\n",
    "# Plot da curva de validação\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_values, cv_scores, marker=\"o\")\n",
    "plt.xlabel(\"Número de vizinhos (k)\")\n",
    "plt.ylabel(\"Acurácia média (5-fold CV)\")\n",
    "plt.title(\"Curva de validação para KNN\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Treinar modelo final\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Previsões no teste\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Avaliação\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia no teste (k={best_k}): {acc:.2%}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
    "plt.title(f\"Matriz de confusão - KNN (k={best_k})\")\n",
    "plt.show()\n",
    "\n",
    "# Relatório de classificação\n",
    "print(\"Relatório de classificação (precision, recall, f1):\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca1503",
   "metadata": {},
   "source": [
    "### Etapa 6 - Scoring nas novas localidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar sugestões\n",
    "df_sug = df_sugestoes.copy()\n",
    "\n",
    "# Criar identificador único da localidade sugerida\n",
    "df_sug[\"localidade\"] = (\n",
    "    df_sug[\"escopo\"].astype(str) + \"-\" + df_sug[\"estado\"].astype(str).str.upper()\n",
    ")\n",
    "\n",
    "# Mapear estado -> macro-região (mesmo dicionário da etapa anterior)\n",
    "mapa_uf_regiao = {\n",
    "    \"AC\":\"NORTE\",\"AP\":\"NORTE\",\"AM\":\"NORTE\",\"PA\":\"NORTE\",\"RO\":\"NORTE\",\"RR\":\"NORTE\",\"TO\":\"NORTE\",\n",
    "    \"AL\":\"NORDESTE\",\"BA\":\"NORDESTE\",\"CE\":\"NORDESTE\",\"MA\":\"NORDESTE\",\"PB\":\"NORDESTE\",\"PE\":\"NORDESTE\",\"PI\":\"NORDESTE\",\"RN\":\"NORDESTE\",\"SE\":\"NORDESTE\",\n",
    "    \"DF\":\"CENTRO-OESTE\",\"GO\":\"CENTRO-OESTE\",\"MT\":\"CENTRO-OESTE\",\"MS\":\"CENTRO-OESTE\",\n",
    "    \"ES\":\"SUDESTE\",\"MG\":\"SUDESTE\",\"RJ\":\"SUDESTE\",\"SP\":\"SUDESTE\",\n",
    "    \"PR\":\"SUL\",\"RS\":\"SUL\",\"SC\":\"SUL\"\n",
    "}\n",
    "df_sug[\"Dim_Lojas.Regiao\"] = df_sug[\"estado\"].str.upper().map(mapa_uf_regiao)\n",
    "\n",
    "# Padronizar nome da região chilli\n",
    "df_sug[\"Dim_Lojas.REGIAO_CHILLI\"] = df_sug[\"regiao_chilli\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Todas as categorias conhecidas do treino\n",
    "todas_categorias = sorted(df_modelo[\"Categoria_Combo\"].dropna().unique().tolist())\n",
    "\n",
    "# Produto cartesiano: cada local sugerido × todas as categorias\n",
    "df_grid = (\n",
    "    df_sug[[\"localidade\", \"Dim_Lojas.Regiao\", \"Dim_Lojas.REGIAO_CHILLI\"]]\n",
    "        .merge(pd.DataFrame({\"Categoria_Combo\": todas_categorias}), how=\"cross\")\n",
    ")\n",
    "\n",
    "print(\"Grid de scoring:\", df_grid.shape)\n",
    "display(df_grid.head())\n",
    "\n",
    "# One-hot encoding\n",
    "X_new = pd.get_dummies(\n",
    "    df_grid[[\"localidade\",\"Categoria_Combo\",\"Dim_Lojas.Regiao\",\"Dim_Lojas.REGIAO_CHILLI\"]],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "# Reindex para garantir MESMAS colunas do treino\n",
    "X_new = X_new.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(\"Shape X_new (alinhado):\", X_new.shape)\n",
    "\n",
    "\n",
    "# Predição com modelo treinado (knn já definido)\n",
    "pred_label = knn.predict(X_new)\n",
    "probas = knn.predict_proba(X_new)\n",
    "\n",
    "# Montar dataframe com previsões\n",
    "df_pred = df_grid.copy()\n",
    "df_pred[\"pred_label\"] = pred_label\n",
    "\n",
    "# Adicionar colunas de probabilidade por classe\n",
    "for i, cls in enumerate(knn.classes_):\n",
    "    df_pred[f\"proba_{cls}\"] = probas[:, i]\n",
    "\n",
    "# Probabilidade de sucesso = probabilidade de ser \"bom\"\n",
    "df_pred[\"pct_sucesso\"] = df_pred[\"proba_bom\"] if \"proba_bom\" in df_pred.columns else np.nan\n",
    "\n",
    "display(df_pred.head())\n",
    "\n",
    "# Ordenar por localidade sugerida e % de sucesso\n",
    "df_resultado = (\n",
    "    df_pred.sort_values([\"localidade\",\"pct_sucesso\"], ascending=[True, False])\n",
    "             .loc[:, [\"localidade\",\"Categoria_Combo\",\"pred_label\",\"pct_sucesso\",\n",
    "                      \"proba_bom\",\"proba_mediano\",\"proba_ruim\"]]\n",
    ")\n",
    "\n",
    "display(df_resultado.head(20))\n",
    "\n",
    "# Top 5 categorias por localidade sugerida\n",
    "topN = (\n",
    "    df_resultado.groupby(\"localidade\")\n",
    "                .head(5)\n",
    ")\n",
    "\n",
    "display(topN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9a249",
   "metadata": {},
   "source": [
    "### Etapa 7 - Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar resultado completo\n",
    "caminho_completo = \"../../database/dataset gerado/resultados_completos.csv\"\n",
    "df_resultado.to_csv(caminho_completo, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Arquivo salvo: {caminho_completo}\")\n",
    "\n",
    "# Exportar Top 5 categorias por local sugerido\n",
    "caminho_topN = \"../../database/dataset gerado/resultados_top5_por_local.csv\"\n",
    "topN.to_csv(caminho_topN, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Arquivo salvo: {caminho_topN}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
