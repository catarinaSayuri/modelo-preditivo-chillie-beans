{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9ff08a",
   "metadata": {},
   "source": [
    "## Modelo Supervisionado \n",
    "\n",
    "**Projeto**: Expansão por grupos — classificador de “Bom Desempenho” por região×grupo\n",
    "**Objetivo**: Treinar e avaliar um RandomForest com pipeline (pré-processamento + OHE), escolher limiar ótimo por F1, salvar o modelo e ranquear candidatos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1e33e",
   "metadata": {},
   "source": [
    "### Configuração inicial, importações e parâmetros globais\n",
    "\n",
    "- Importa bibliotecas essenciais para:\n",
    "  - Manipulação e análise de dados (`pandas`, `numpy`)\n",
    "  - Visualização (`matplotlib`)\n",
    "  - Modelagem supervisionada (`scikit-learn`)\n",
    "  - Persistência de modelos (`joblib`)\n",
    "\n",
    "- Define configurações globais:\n",
    "  - Supressão de warnings não críticos\n",
    "  - Ajustes de exibição do pandas (linhas, colunas e largura)\n",
    "\n",
    "- Estabelece parâmetros do projeto:\n",
    "  - Caminhos para datasets e artefatos\n",
    "  - Hiperparâmetros globais (ex.: `TOP_N`, `TOP_K`, `RANDOM_STATE`)\n",
    "\n",
    "- Implementa função `read_csv_flex`:\n",
    "  - Busca arquivos `.csv` em múltiplos diretórios possíveis\n",
    "  - Testa diferentes separadores (`;`, `,`, autodetecção)\n",
    "  - Retorna `DataFrame` válido ou lança erro com histórico de tentativas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports principais =====\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== Módulos do scikit-learn =====\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit  # validação estratificada\n",
    "from sklearn.compose import ColumnTransformer                               # pré-processamento por tipo de coluna\n",
    "from sklearn.pipeline import Pipeline                                       # pipeline unificado\n",
    "from sklearn.impute import SimpleImputer                                    # imputação de valores faltantes\n",
    "from sklearn.preprocessing import OneHotEncoder                             # codificação categórica\n",
    "from sklearn.ensemble import RandomForestClassifier                         # classificador de árvores\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score, precision_recall_curve, roc_curve)  # métricas de avaliação\n",
    "from sklearn.inspection import permutation_importance                       # interpretabilidade\n",
    "from joblib import dump, load                                               # persistência de modelos\n",
    "\n",
    "# ===== Configurações globais =====\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # suprime avisos não críticos\n",
    "pd.set_option(\"display.max_columns\", None)              # exibe todas as colunas\n",
    "pd.set_option(\"display.width\", 220)                     # define largura máxima de exibição\n",
    "pd.set_option(\"display.max_rows\", 200)                  # aumenta limite de linhas exibidas\n",
    "\n",
    "# ===== Parâmetros do projeto =====\n",
    "CAMINHO_DF_HIST = \"../../../database/dataset gerado/dataset_limpo.csv\"                 # dataset histórico (limpo)\n",
    "ARQ_MODELO      = Path(\"../../../database/dataset gerado/modelo_grupo_chilli_simplificado.joblib\")    # arquivo para salvar/carregar modelo\n",
    "ARQ_NOVAS       = \"../../../database/dataset gerado/sugestoes_expansao_para_supervisionado\"  # sugestões do não supervisionado\n",
    "TOP_N           = 10                                                 # número de categorias no ranking final\n",
    "TOP_K           = 1                                                  # top-k localidades\n",
    "FORCAR_TREINO   = False                                              # se True, força re-treinamento\n",
    "LIMIAR          = None                                               # limiar de decisão (se não definido, será calibrado)\n",
    "RANDOM_STATE    = 42                                                 # semente aleatória para reprodutibilidade\n",
    "\n",
    "# ===== Função utilitária para leitura flexível de CSV =====\n",
    "def read_csv_flex(base_path_or_stem: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lê um arquivo CSV de forma flexível, testando múltiplos caminhos e separadores.\n",
    "    Retorna um DataFrame se encontrado, caso contrário levanta FileNotFoundError.\n",
    "    \"\"\"\n",
    "    # Constrói lista de candidatos (caminho direto, com extensão, em pastas comuns)\n",
    "    candidates = [\n",
    "        Path(base_path_or_stem),\n",
    "        Path(f\"{base_path_or_stem}.csv\"),\n",
    "        Path(\"data\") / f\"{base_path_or_stem}.csv\",\n",
    "        Path(\"datasets\") / f\"{base_path_or_stem}.csv\",\n",
    "    ]\n",
    "    # Acrescenta arquivos encontrados no diretório de forma recursiva\n",
    "    for p in glob.glob(f\"**/{Path(base_path_or_stem).stem}*.csv\", recursive=True):\n",
    "        candidates.append(Path(p))\n",
    "\n",
    "    tried = []  # registra tentativas de leitura\n",
    "    for path in candidates:\n",
    "        if not path.exists():\n",
    "            tried.append(str(path))\n",
    "            continue\n",
    "        # Testa diferentes separadores\n",
    "        for sep in [None, ';', ',']:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, engine=\"python\")\n",
    "                print(f\"✔ Arquivo carregado de: {path}\")\n",
    "                return df\n",
    "            except Exception:\n",
    "                tried.append(f\"{path}(sep={sep})\")\n",
    "    # Se nenhum arquivo foi lido, lança erro com histórico de tentativas\n",
    "    raise FileNotFoundError(\"CSV não encontrado. Tentativas: \" + \" | \".join(tried))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4ffaa",
   "metadata": {},
   "source": [
    "### Construção da base supervisionada (features + rótulo por REGIÃO × GRUPO)\n",
    "\n",
    "- **Finalidade:** consolidar, em uma linha por par `REGIÃO × GRUPO`, as variáveis explicativas e o rótulo binário de desempenho.\n",
    "- **Entradas principais:** `df_hist` com colunas de preço (bruto e líquido), desconto, identificador de loja, região da loja e grupo de produto.\n",
    "- **Regras de negócio:**\n",
    "  - O rótulo `Bom_Desempenho` marca como 1 os **Top-K grupos por região** (maior receita líquida por par `REGIÃO × GRUPO`); demais pares recebem 0.\n",
    "  - As **features** incluem: receita total por região, número de lojas por região, receita por loja na região e **medianas por par `REGIÃO × GRUPO`** (preço de varejo e desconto).\n",
    "- **Artefatos auxiliares:**\n",
    "  - `catalogo` agrega metadados por grupo (ex.: `Grupo_Produto` e mediana de preço de varejo) para cruzar depois com as novas localidades.\n",
    "  - `agg_reg` e `grp_meds` ficam disponíveis para análises e enriquecimentos adicionais.\n",
    "- **Validações rápidas:**\n",
    "  - Conferir presença das colunas exigidas antes de agrupar.\n",
    "  - Conferir distribuição de `Bom_Desempenho` e a forma de `X_all`/`y_all`.\n",
    "  - Visualizar distribuição do alvo com um gráfico de barras simples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES + RÓTULO (1 linha por REGIÃO×GRUPO) \n",
    "def build_train_table(df_hist: pd.DataFrame, top_k: int = 1):\n",
    "    # Checagem de colunas necessárias para garantir integridade mínima da base\n",
    "    need = [\"ID_Loja\",\"Desconto\",\"Total_Preco_Varejo\",\"Total_Preco_Liquido\",\n",
    "            \"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\",\"Dim_Produtos.Grupo_Produto\"]\n",
    "    miss = [c for c in need if c not in df_hist.columns]\n",
    "    if miss: raise ValueError(f\"Faltando colunas no histórico: {miss}\")\n",
    "\n",
    "    # ------- Agregações em nível de REGIÃO -------\n",
    "    # Contagem de lojas distintas por região e soma de receita líquida por região\n",
    "    reg_lojas = df_hist.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"ID_Loja\"].nunique()\n",
    "    reg_receita = df_hist.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"Total_Preco_Liquido\"].sum()\n",
    "    # Junta agregações em uma única tabela por região e nomeia colunas\n",
    "    agg_reg = (pd.concat([reg_receita, reg_lojas], axis=1)\n",
    "                 .reset_index()\n",
    "                 .rename(columns={\"Total_Preco_Liquido\":\"regiao_receita_total\",\n",
    "                                  \"ID_Loja\":\"regiao_num_lojas\"}))\n",
    "    # Receita média por loja na região (com proteção a divisão por zero)\n",
    "    agg_reg[\"regiao_receita_por_loja\"] = (\n",
    "        agg_reg[\"regiao_receita_total\"]/agg_reg[\"regiao_num_lojas\"].replace(0,np.nan)\n",
    "    ).fillna(0)\n",
    "\n",
    "    # ------- Medianas por REGIÃO × GRUPO (para features; não definem alvo diretamente) -------\n",
    "    grp_meds = (df_hist.groupby([\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\"])\n",
    "                       .agg(grupo_med_preco_varejo=(\"Total_Preco_Varejo\",\"median\"),\n",
    "                            grupo_med_desconto=(\"Desconto\",\"median\"))\n",
    "                       .reset_index())\n",
    "\n",
    "    # ------- Volume por REGIÃO × GRUPO para definir Top-K e rótulo -------\n",
    "    vol = (df_hist.groupby([\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\"])[\"Total_Preco_Liquido\"]\n",
    "                 .sum().reset_index()\n",
    "                 .rename(columns={\"Total_Preco_Liquido\":\"grupo_regiao_receita\"}))\n",
    "    # Ranking dentro de cada região por receita do par (quanto menor o rank, maior a receita)\n",
    "    vol[\"rank_na_regiao\"] = (vol.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"grupo_regiao_receita\"]\n",
    "                                .rank(method=\"first\", ascending=False))\n",
    "    # Rótulo binário: Top-K na região recebe 1; demais recebem 0\n",
    "    vol[\"Bom_Desempenho\"] = (vol[\"rank_na_regiao\"] <= top_k).astype(int)\n",
    "\n",
    "    # ------- Montagem de features X e rótulo y (chave = REGIÃO × GRUPO) -------\n",
    "    X = (vol[[\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\"]]\n",
    "         .merge(agg_reg, on=\"Dim_Lojas.REGIAO_CHILLI\", how=\"left\")\n",
    "         .merge(grp_meds, on=[\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\"], how=\"left\"))\n",
    "    y = vol[\"Bom_Desempenho\"].astype(int)\n",
    "\n",
    "    # ------- Catálogo por GRUPO (metadados úteis para cruzar em novas localidades) -------\n",
    "    catalogo = (df_hist.groupby([\"Dim_Produtos.GRUPO_CHILLI\",\"Dim_Produtos.Grupo_Produto\"])\n",
    "                        .agg(Preco_Varejo_med=(\"Total_Preco_Varejo\",\"median\"))\n",
    "                        .reset_index())\n",
    "\n",
    "    # Retorna matriz de atributos, rótulo e agregados auxiliares\n",
    "    return X, y, catalogo, agg_reg, grp_meds\n",
    "\n",
    "# HISTÓRICO \n",
    "dfh = pd.read_csv(CAMINHO_DF_HIST)\n",
    "# Remoção de colunas não utilizadas na construção de features/label\n",
    "dfh = dfh.drop(columns=[\n",
    "    \"ID_Cliente\",\"Dim_Cliente.Data_Nascimento\",\"Dim_Cliente.Regiao_Cliente\",\n",
    "    \"ID_Produto\",\"Dim_Lojas.Nome_Emp\",\"Dim_Lojas.Bairro_Emp\",\"Dim_Lojas.Cidade_Emp\",\n",
    "    \"Dim_Lojas.CANAL_VENDA\",\"Dim_Lojas.Tipo_PDV\",\"Dim_Lojas.Regiao\"  # não usados\n",
    "], errors=\"ignore\")\n",
    "\n",
    "# Construção da base supervisionada (X_all, y_all) e artefatos auxiliares\n",
    "X_all, y_all, catalogo, agg_reg, grp_meds = build_train_table(dfh, TOP_K)\n",
    "print(X_all.shape, y_all.mean().round(3))  # forma de X e taxa de positivos\n",
    "\n",
    "# Visual 2.1 — Distribuição do alvo\n",
    "vals = y_all.value_counts().sort_index()\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(vals.index.astype(str), vals.values)\n",
    "plt.title(\"Distribuição do alvo (Bom_Desempenho)\")\n",
    "plt.xlabel(\"Classe\"); plt.ylabel(\"Contagem\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f8e0d",
   "metadata": {},
   "source": [
    "### Definição do pipeline supervisionado (pré-processamento + RandomForest)\n",
    "\n",
    "- **Objetivo:** encapsular, em um único `Pipeline`, o pré-processamento (imputação + codificação) e o classificador.\n",
    "- **Entradas esperadas:** `X` com colunas categóricas (`Dim_Produtos.GRUPO_CHILLI`, `Dim_Lojas.REGIAO_CHILLI`) e numéricas (agregações regionais e medianas por par).\n",
    "- **Pré-processamento:**\n",
    "  - Categóricas: imputação por moda (`most_frequent`) e `OneHotEncoder` com `handle_unknown=\"ignore\"` (compatível com categorias inéditas na inferência).\n",
    "  - Numéricas: imputação por mediana (`median`).\n",
    "  - Compatibilidade: seleção automática entre `sparse_output=False` (sklearn ≥ 1.2) e `sparse=False` (versões anteriores).\n",
    "- **Modelo:** `RandomForestClassifier` com:\n",
    "  - `class_weight=\"balanced\"` para mitigar desbalanceamento do alvo,\n",
    "  - `n_estimators=500`, `max_depth=12`, `min_samples_leaf=2` (viés-variância mais controlado),\n",
    "  - `random_state` fixo e `n_jobs=-1` para paralelismo.\n",
    "- **Saída:** objeto `Pipeline` pronto para `fit`, `predict_proba` e `predict`, mantendo consistência de transformações entre treino e inferência.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd48175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(X):\n",
    "    # Define dinamicamente as colunas categóricas presentes em X\n",
    "    cat_cols = [c for c in [\"Dim_Produtos.GRUPO_CHILLI\",\"Dim_Lojas.REGIAO_CHILLI\"] if c in X.columns]\n",
    "    # Define as colunas numéricas esperadas (agregações regionais e medianas por par REGIÃO × GRUPO)\n",
    "    num_cols = [\"regiao_receita_total\",\"regiao_num_lojas\",\"regiao_receita_por_loja\",\n",
    "                \"grupo_med_preco_varejo\",\"grupo_med_desconto\"]\n",
    "\n",
    "    # OneHotEncoder: compatibilidade entre versões do sklearn (sparse_output vs sparse)\n",
    "    if \"sparse_output\" in OneHotEncoder().get_params().keys():\n",
    "        oh = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn mais recente\n",
    "    else:\n",
    "        oh = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn anterior\n",
    "\n",
    "    # Pré-processamento por tipo de coluna: categóricas (imputação + one-hot) e numéricas (imputação)\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", oh)]), cat_cols),\n",
    "            (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"  # descarta colunas não especificadas\n",
    "    )\n",
    "\n",
    "    # Classificador: RandomForest com pesos balanceados para lidar com desbalanceamento do alvo\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=500, max_depth=12, min_samples_leaf=2,\n",
    "        class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Retorna um Pipeline unificado (pré-processamento + modelo)\n",
    "    return Pipeline([(\"pre\", pre), (\"clf\", rf)])\n",
    "\n",
    "# Instancia o pipeline final com base nas colunas de X_all (garante compatibilidade de features)\n",
    "model = make_model(X_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f26e11",
   "metadata": {},
   "source": [
    "### Validação e escolha de limiar (CV estratificada + hold-out com curvas)\n",
    "\n",
    "- **Finalidade:** estimar desempenho médio via **Stratified K-Fold** e selecionar um **limiar de decisão** a partir de um **hold-out estratificado**.\n",
    "- **Cross-validation (CV):**\n",
    "  - Recria o `Pipeline` a cada dobra, ajusta em treino e avalia em teste.\n",
    "  - Coleta **AP (PR-AUC)** e **ACC** usando limiar fixo 0,50 (diagnóstico inicial).\n",
    "- **Hold-out (seleção de limiar):**\n",
    "  - Separa 20% estratificado, treina, obtém **probabilidades** e calcula métricas.\n",
    "  - Identifica o **limiar que maximiza F1** pela curva **Precision–Recall**.\n",
    "  - Exibe gráficos: PR, **F1 × limiar**, ROC e **matriz de confusão** no limiar ótimo.\n",
    "- **Saídas:**\n",
    "  - Impressões de **AP/ACC médios** na CV.\n",
    "  - Relatório de **hold-out** (classification report, matriz, ROC-AUC, PR-AUC).\n",
    "  - **Limiar escolhido** (retornado para uso posterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval_scores(model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)  # CV estratificada e reprodutível\n",
    "    aps, accs = [], []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        m = Pipeline(model.steps)                 # recria o Pipeline para cada dobra (evita contaminação)\n",
    "        m.fit(X.iloc[tr], y.iloc[tr])             # ajusta apenas com a parte de treino da dobra\n",
    "        proba = m.predict_proba(X.iloc[te])[:,1]  # probabilidades da classe positiva no conjunto de teste\n",
    "        pred  = (proba >= 0.5).astype(int)        # predição binária com limiar 0.50 (diagnóstico)\n",
    "        aps.append(average_precision_score(y.iloc[te], proba))  # AP (PR-AUC) da dobra\n",
    "        accs.append((pred == y.iloc[te]).mean())                 # acurácia da dobra\n",
    "    print(f\"\\nCV (RF)  AP={np.mean(aps):.3f}  ACC={np.mean(accs):.3f}\")  # médias das dobras\n",
    "\n",
    "def choose_threshold_and_plots(model, X, y, limiar=None):\n",
    "    if limiar is not None:\n",
    "        return float(limiar)  # respeita limiar pré-definido (pula seleção automática)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)  # hold-out estratificado 80/20\n",
    "    tr, te = next(sss.split(X, y))\n",
    "    model.fit(X.iloc[tr], y.iloc[tr])                     # ajuste no conjunto de treino do hold-out\n",
    "    proba = model.predict_proba(X.iloc[te])[:,1]          # probabilidades no conjunto de teste\n",
    "    yte   = y.iloc[te].values\n",
    "\n",
    "    print(\"\\n=== Hold-out (estratificado) — limiar 0.50 ===\")\n",
    "    pred050 = (proba >= 0.5).astype(int)                  # diagnóstico com limiar padrão 0.50\n",
    "    print(classification_report(yte, pred050, digits=3))  # métricas detalhadas\n",
    "    print(\"Matriz:\\n\", confusion_matrix(yte, pred050))\n",
    "    print(\"ROC-AUC:\", round(roc_auc_score(yte, proba), 3),\n",
    "          \" | PR-AUC:\", round(average_precision_score(yte, proba), 3))\n",
    "\n",
    "    # Curva PR e busca do limiar que maximiza F1\n",
    "    prec, rec, thr = precision_recall_curve(yte, proba)   # pontos (precision, recall) por limiar\n",
    "    f1s = 2*(prec*rec)/(prec+rec+1e-9)                    # F1 em cada ponto da curva\n",
    "    idx = np.nanargmax(f1s)                                # índice do F1 máximo\n",
    "    chosen = float(thr[idx]) if idx < len(thr) else 0.5    # limiar escolhido (fallback 0.5 se necessário)\n",
    "    print(f\"\\n→ Limiar ótimo (F1 máx): {chosen:.3f}\")\n",
    "\n",
    "    # Visual 4.1 — Precision-Recall\n",
    "    plt.figure(figsize=(4.2,3.2))\n",
    "    plt.plot(rec, prec)                                    # curva PR\n",
    "    plt.scatter(rec[idx], prec[idx])                       # marca o ponto de F1 máximo\n",
    "    plt.title(\"Curva Precision–Recall (hold-out)\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.show()\n",
    "\n",
    "    # Visual 4.2 — F1 vs Threshold\n",
    "    plt.figure(figsize=(4.2,3.2))\n",
    "    xs = thr if len(thr) else np.array([0.5])              # eixo x: limiares conhecidos (ou 0.5)\n",
    "    ys = f1s[:-1] if len(thr) else np.array([0.0])         # eixo y: F1 correspondente (descarta último ponto PR)\n",
    "    plt.plot(xs, ys)\n",
    "    if len(thr):\n",
    "        plt.axvline(chosen, linestyle=\"--\")                # linha vertical no limiar escolhido\n",
    "    plt.title(\"F1 × Limiar (hold-out)\")\n",
    "    plt.xlabel(\"Limiar\"); plt.ylabel(\"F1\")\n",
    "    plt.show()\n",
    "\n",
    "    # Visual 4.3 — ROC\n",
    "    fpr, tpr, _ = roc_curve(yte, proba)\n",
    "    plt.figure(figsize=(4.2,3.2))\n",
    "    plt.plot(fpr, tpr)                                     # curva ROC\n",
    "    plt.plot([0,1],[0,1])                                  # diagonal (classificador aleatório)\n",
    "    plt.title(\"Curva ROC (hold-out)\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "    plt.show()\n",
    "\n",
    "    # Visual 4.4 — Matriz de confusão (limiar ótimo)\n",
    "    pred_opt = (proba >= chosen).astype(int)               # predição com o limiar selecionado\n",
    "    cm = confusion_matrix(yte, pred_opt)\n",
    "    plt.figure(figsize=(3.4,3))\n",
    "    plt.imshow(cm)                                         # heatmap simples\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\")  # valores nas células\n",
    "    plt.title(\"Matriz de confusão (limiar ótimo)\")\n",
    "    plt.xticks([0,1], [\"Neg\",\"Pos\"]); plt.yticks([0,1], [\"Neg\",\"Pos\"])\n",
    "    plt.xlabel(\"Predito\"); plt.ylabel(\"Verdadeiro\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    return chosen  # retorna o limiar escolhido para uso nas próximas etapas\n",
    "\n",
    "# Execução\n",
    "crossval_scores(model, X_all, y_all)                           # desempenho médio em CV (AP, ACC)\n",
    "chosen_thr = choose_threshold_and_plots(model, X_all, y_all, LIMIAR)  # seleção do limiar final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb00565",
   "metadata": {},
   "source": [
    "### Treinamento final, persistência do modelo e análise de importância\n",
    "\n",
    "- **Objetivo:** ajustar o modelo supervisionado em toda a base disponível, salvar o artefato treinado e interpretar globalmente a contribuição das variáveis.\n",
    "- **Treinamento e salvamento:**\n",
    "  - O modelo é ajustado em `X_all` e `y_all`.\n",
    "  - O objeto resultante é salvo no arquivo definido em `ARQ_MODELO` (`.joblib`).\n",
    "- **Importância por permutação:**\n",
    "  - Mede a queda média de desempenho quando uma feature é embaralhada.\n",
    "  - Calculada no próprio `X_all` (interpretação global do dataset).\n",
    "  - Exibe tabela das variáveis mais relevantes e gráfico horizontal com o Top-10.\n",
    "- **Validações rápidas:**\n",
    "  - Conferir mensagem de salvamento do modelo.\n",
    "  - Avaliar se as variáveis com maior importância fazem sentido para o negócio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139221b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina final e salva\n",
    "model.fit(X_all, y_all)                                    # ajuste final em toda a base disponível\n",
    "dump(model, ARQ_MODELO.as_posix())                         # persistência do modelo em arquivo .joblib\n",
    "print(f\"\\nModelo salvo em: {ARQ_MODELO.name}\")\n",
    "\n",
    "# Importância por permutação (no próprio conjunto X_all para interpretação global)\n",
    "result = permutation_importance(\n",
    "    model, X_all, y_all,\n",
    "    n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1     # 10 repetições, paralelizado\n",
    ")\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": X_all.columns,\n",
    "    \"importance_mean\": result.importances_mean,            # importância média\n",
    "    \"importance_std\": result.importances_std               # desvio padrão das repetições\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "display(importances.head(15))                              # tabela das 15 variáveis mais relevantes\n",
    "\n",
    "# Visual 5.1 — Top-10 importâncias\n",
    "top = importances.head(10).iloc[::-1]                      # seleciona top-10 e inverte para plotar de baixo para cima\n",
    "plt.figure(figsize=(5,3.8))\n",
    "plt.barh(top[\"feature\"], top[\"importance_mean\"])           # gráfico horizontal\n",
    "plt.title(\"Importância por Permutação — Top 10\")\n",
    "plt.xlabel(\"Δ métrica (mean)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11125f5",
   "metadata": {},
   "source": [
    "### Novas localidades e geração do ranking de categorias\n",
    "\n",
    "- **Objetivo:** aplicar o modelo treinado às novas localidades sugeridas, calcular a probabilidade de sucesso e gerar um ranking das categorias mais promissoras.\n",
    "- **Processo:**\n",
    "  - Leitura e normalização dos dados de novas localidades (`novas_raw`).\n",
    "  - Padronização de nomes de colunas (`estado`, `regiao_chilli`) para alinhar com o histórico.\n",
    "  - Geração de candidatos por produto: combinação cruzada de cada nova localidade com o catálogo de grupos de produtos.\n",
    "  - Enriquecimento dos candidatos com agregados regionais (`agg_reg`) e medianas regionais por grupo (`grp_meds`).\n",
    "  - Seleção das mesmas features utilizadas no treino (`feat_cols`) para manter consistência.\n",
    "  - Predição da **probabilidade de sucesso** e classificação binária com base no limiar escolhido (`chosen_thr`).\n",
    "  - Ordenação dos resultados e seleção do **Top-N** por localidade.\n",
    "- **Saídas:**\n",
    "  - DataFrame `saida` com ranking das categorias por nova localidade, contendo escopo, cluster, grupo, preço mediano, probabilidade e previsão binária.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- NOVAS LOCALIZAÇÕES + RANKING --------------------\n",
    "novas_raw = read_csv_flex(ARQ_NOVAS)  # leitura das novas localidades sugeridas\n",
    "print(\"\\nColunas nas novas localizações:\", list(novas_raw.columns))\n",
    "\n",
    "# Normaliza nomes de colunas para compatibilidade com o histórico\n",
    "rename_map = {}\n",
    "for c in novas_raw.columns:\n",
    "    cl = c.strip().lower()\n",
    "    if cl == \"estado\":\n",
    "        rename_map[c] = \"Dim_Lojas.Estado_Emp\"\n",
    "    elif cl == \"regiao_chilli\":\n",
    "        rename_map[c] = \"Dim_Lojas.REGIAO_CHILLI\"\n",
    "novas = novas_raw.rename(columns=rename_map).copy()\n",
    "# Garante a presença das colunas identificadoras mínimas\n",
    "for c in [\"escopo\",\"cluster_id\"]:\n",
    "    if c not in novas.columns: novas[c] = \"NA\"\n",
    "\n",
    "# Criação de candidatos: cada nova localidade × cada grupo de produto\n",
    "base = novas.rename(columns={\"Dim_Lojas.REGIAO_CHILLI\":\"REG_TMP\"}, errors=\"ignore\").copy()\n",
    "cand = base.merge(\n",
    "    catalogo[[\"Dim_Produtos.GRUPO_CHILLI\",\"Dim_Produtos.Grupo_Produto\",\"Preco_Varejo_med\"]],\n",
    "    how=\"cross\"  # produto cartesiano\n",
    ")\n",
    "\n",
    "# Anexa agregados regionais e medianas de grupo (vindos do histórico)\n",
    "cand = cand.merge(agg_reg, left_on=\"REG_TMP\", right_on=\"Dim_Lojas.REGIAO_CHILLI\", how=\"left\")\n",
    "cand = cand.merge(grp_meds, left_on=[\"REG_TMP\",\"Dim_Produtos.GRUPO_CHILLI\"],\n",
    "                  right_on=[\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\"], how=\"left\",\n",
    "                  suffixes=(\"\",\"_dup\"))\n",
    "\n",
    "# Ajusta coluna de região para manter consistência com treino\n",
    "cand[\"Dim_Lojas.REGIAO_CHILLI\"] = cand[\"REG_TMP\"]\n",
    "\n",
    "# Seleciona as mesmas features usadas no treinamento\n",
    "feat_cols = [\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\",\n",
    "             \"regiao_receita_total\",\"regiao_num_lojas\",\"regiao_receita_por_loja\",\n",
    "             \"grupo_med_preco_varejo\",\"grupo_med_desconto\"]\n",
    "cand_feat = cand[feat_cols].copy()\n",
    "\n",
    "# Predição da probabilidade e classe (binária) para cada candidato\n",
    "proba = model.predict_proba(cand_feat)[:,1]\n",
    "cand[\"Probabilidade_Sucesso\"] = proba\n",
    "cand[\"Previsao\"] = (proba >= chosen_thr).astype(int)\n",
    "cand[\"Probabilidade_pct\"] = (proba*100).round(1)  # em porcentagem arredondada\n",
    "\n",
    "# Ranking Top-N por localidade\n",
    "id_cols = [\"escopo\",\"cluster_id\"]\n",
    "rank = cand.sort_values(id_cols + [\"Probabilidade_Sucesso\"], ascending=[True, True, False]).reset_index(drop=True)\n",
    "top_n = rank.groupby(id_cols, group_keys=False).head(TOP_N)\n",
    "\n",
    "# Seleciona colunas de saída\n",
    "cols_out = (id_cols + [\"Dim_Lojas.REGIAO_CHILLI\",\"Dim_Produtos.GRUPO_CHILLI\",\"Dim_Produtos.Grupo_Produto\",\n",
    "                       \"Preco_Varejo_med\",\"Probabilidade_pct\",\"Previsao\"])\n",
    "saida = top_n[[c for c in cols_out if c in top_n.columns]].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6a180",
   "metadata": {},
   "source": [
    "### Exibição e exportação do ranking por localidade\n",
    "\n",
    "- **Objetivo:** apresentar os resultados finais de ranking das categorias por nova localidade e salvar em arquivo `.csv`.\n",
    "- **Processo:**\n",
    "  - Impressão no console: para cada par `escopo × cluster_id`, mostra as categorias ranqueadas (sem repetir colunas de identificação).\n",
    "  - Visualização: gráfico de barras com as probabilidades previstas das **Top-N categorias** do primeiro local.\n",
    "  - Exportação: grava o ranking consolidado em `ranking_grupo_chilli_por_local_simplificado.csv` para uso externo.\n",
    "- **Validações rápidas:**\n",
    "  - Conferir se cada localidade aparece com seu respectivo Top-N.\n",
    "  - Avaliar se o gráfico de barras reflete corretamente a ordem das probabilidades.\n",
    "  - Confirmar a criação do arquivo `.csv` no diretório de saída.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2526335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==================== RANKING TOP-N POR LOCAL ====================\")\n",
    "# Itera sobre cada local (escopo × cluster_id) e imprime tabela formatada\n",
    "for (esc, clu), df_loc in saida.groupby([\"escopo\",\"cluster_id\"]):\n",
    "    print(f\"\\n>>> Local: escopo={esc} | cluster_id={clu}  (Top-{TOP_N})\")\n",
    "    # imprime apenas colunas relevantes (sem escopo e cluster_id)\n",
    "    print(df_loc.drop(columns=[\"escopo\",\"cluster_id\"]).to_string(index=False))\n",
    "\n",
    "# Visual 7.1 — Para o primeiro local, barras das Top-N probabilidades\n",
    "first_key = next(iter(saida.groupby([\"escopo\",\"cluster_id\"]).groups.keys()))  # pega o primeiro local\n",
    "ex = saida.set_index([\"escopo\",\"cluster_id\"]).loc[first_key].reset_index(drop=True)\n",
    "plt.figure(figsize=(6,3.5))\n",
    "plt.bar(range(len(ex)), ex[\"Probabilidade_pct\"])                             # barras = probabilidades (%)\n",
    "plt.xticks(range(len(ex)), ex[\"Dim_Produtos.GRUPO_CHILLI\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Probabilidade (%)\")\n",
    "plt.title(f\"Top-{TOP_N} — {first_key}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exportação do resultado consolidado\n",
    "ARQ_OUT = \"../../../database/dataset gerado/ranking_grupo_chilli_por_local_simplificado.csv\"\n",
    "saida.to_csv(ARQ_OUT, index=False, sep=\";\")\n",
    "print(f\"\\nArquivo exportado: {ARQ_OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843e54b",
   "metadata": {},
   "source": [
    "### Checagem de consistência: nulos e cobertura de regiões\n",
    "\n",
    "- **Objetivo:** verificar a qualidade e consistência dos dados antes de finalizar a etapa supervisionada.\n",
    "- **Processo:**\n",
    "  - **Nulos nas features de treino:** contabiliza valores ausentes em `X_all` e lista as 10 variáveis mais afetadas.\n",
    "  - **Cobertura de regiões:** compara as regiões presentes nas novas localidades (`novas`) com aquelas do histórico agregado (`agg_reg`).\n",
    "- **Critérios de atenção:**\n",
    "  - Se existirem muitas features com nulos, pode ser necessário revisar imputação ou fontes de dados.\n",
    "  - Se alguma região nova não tiver correspondência no histórico, o modelo pode não gerar predições confiáveis para ela.\n",
    "- **Saídas:**\n",
    "  - Relatório textual com top-10 variáveis mais afetadas por nulos.\n",
    "  - Aviso indicando se existem regiões novas sem histórico ou confirmação de cobertura completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa nulos nas features de treino\n",
    "nulls = X_all.isna().sum().sort_values(ascending=False)\n",
    "print(\"Nulos nas features (top 10):\")\n",
    "print(nulls.head(10).to_string())  # exibe as 10 variáveis com mais valores ausentes\n",
    "\n",
    "# Verifica cobertura de regiões nas novas vs. histórico\n",
    "if \"Dim_Lojas.REGIAO_CHILLI\" in novas.columns:\n",
    "    regs_novas = set(novas[\"Dim_Lojas.REGIAO_CHILLI\"].dropna().unique())  # regiões presentes nas novas localidades\n",
    "    regs_hist  = set(agg_reg[\"Dim_Lojas.REGIAO_CHILLI\"].unique())         # regiões existentes no histórico agregado\n",
    "    faltando = regs_novas - regs_hist\n",
    "    if faltando:\n",
    "        print(\"\\n⚠ Regiões nas novas SEM histórico agregado:\", faltando)\n",
    "    else:\n",
    "        print(\"\\n✔ Todas as regiões das novas têm histórico agregado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd549cd",
   "metadata": {},
   "source": [
    "### Visualização do ranking exportado (`ARQ_OUT`)\n",
    "\n",
    "- **Objetivo:** inspecionar rapidamente o arquivo de ranking exportado e destacar padrões úteis para decisão.\n",
    "- **Conteúdo:**\n",
    "  - Leitura do CSV exportado (`ARQ_OUT`) e **amostra** das linhas.\n",
    "  - **Histograma** das probabilidades previstas (visão geral de confiança).\n",
    "  - **Gráfico de barras** das Top-N categorias do primeiro local (leitura rápida do mix recomendado).\n",
    "  - **Heatmap simples** (localidade × grupo) com a probabilidade (%) — facilita comparar locais/categorias.\n",
    "  - **Tabela ranqueada** por local (escopo, cluster) com as principais colunas.\n",
    "- **Observações:**\n",
    "  - Os gráficos usam apenas `matplotlib` e não definem cores específicas.\n",
    "  - Caso o arquivo não exista, a célula informa o problema sem interromper o notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df264c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Visualização do ranking exportado ===================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Leitura e amostra\n",
    "if not os.path.exists(ARQ_OUT):\n",
    "    print(f\"⚠ Arquivo não encontrado: {ARQ_OUT}. Execute a etapa de exportação antes desta célula.\")\n",
    "else:\n",
    "    df_rank = pd.read_csv(ARQ_OUT, sep=\";\")\n",
    "    print(f\"✔ Ranking carregado: {ARQ_OUT} | shape={df_rank.shape}\")\n",
    "    display(df_rank.head(10))\n",
    "\n",
    "    # Confere colunas essenciais\n",
    "    required_cols = {\"escopo\",\"cluster_id\",\"Dim_Produtos.GRUPO_CHILLI\",\"Probabilidade_pct\"}\n",
    "    missing = required_cols - set(df_rank.columns)\n",
    "    if missing:\n",
    "        print(f\"⚠ Colunas ausentes para visualizações completas: {missing}\")\n",
    "\n",
    "    # 2) Histograma das probabilidades (visão global)\n",
    "    if \"Probabilidade_pct\" in df_rank.columns:\n",
    "        plt.figure(figsize=(5.5,3.6))\n",
    "        plt.hist(df_rank[\"Probabilidade_pct\"].dropna(), bins=20)\n",
    "        plt.xlabel(\"Probabilidade prevista (%)\")\n",
    "        plt.ylabel(\"Frequência\")\n",
    "        plt.title(\"Distribuição de Probabilidades (Ranking Exportado)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 3) Barras das Top-N do primeiro local (escopo × cluster)\n",
    "    if {\"escopo\",\"cluster_id\",\"Dim_Produtos.GRUPO_CHILLI\",\"Probabilidade_pct\"} <= set(df_rank.columns):\n",
    "        first_key = next(iter(df_rank.groupby([\"escopo\",\"cluster_id\"]).groups.keys()))\n",
    "        amostra = (df_rank.set_index([\"escopo\",\"cluster_id\"])\n",
    "                           .loc[first_key]\n",
    "                           .reset_index(drop=True))\n",
    "        plt.figure(figsize=(6,3.6))\n",
    "        plt.bar(range(len(amostra)), amostra[\"Probabilidade_pct\"])\n",
    "        plt.xticks(range(len(amostra)), amostra[\"Dim_Produtos.GRUPO_CHILLI\"], rotation=45, ha=\"right\")\n",
    "        plt.ylabel(\"Probabilidade (%)\")\n",
    "        plt.title(f\"Top-N — {first_key}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 4) Heatmap simples (escopo × grupo) de Probabilidade_pct\n",
    "    if {\"escopo\",\"cluster_id\",\"Dim_Produtos.GRUPO_CHILLI\",\"Probabilidade_pct\"} <= set(df_rank.columns):\n",
    "        # Seleciona um subconjunto de locais para manter o gráfico legível (ex.: até 8 locais)\n",
    "        keys = list(df_rank.groupby([\"escopo\",\"cluster_id\"]).groups.keys())[:8]\n",
    "        sub = (df_rank.set_index([\"escopo\",\"cluster_id\"])\n",
    "                       .loc[keys]\n",
    "                       .reset_index())\n",
    "        # Pivot: linhas = local (escopo|cluster), colunas = grupo, valores = prob %\n",
    "        sub[\"local\"] = sub[\"escopo\"].astype(str) + \" | \" + sub[\"cluster_id\"].astype(str)\n",
    "        piv = sub.pivot_table(index=\"local\", columns=\"Dim_Produtos.GRUPO_CHILLI\",\n",
    "                              values=\"Probabilidade_pct\", aggfunc=\"max\")\n",
    "        # Preenche ausências com 0 para exibição\n",
    "        M = piv.fillna(0).values\n",
    "        plt.figure(figsize=(max(6, 0.35*M.shape[1]+3), max(3.5, 0.35*M.shape[0]+2)))\n",
    "        plt.imshow(M, aspect=\"auto\")\n",
    "        plt.xticks(range(piv.shape[1]), piv.columns, rotation=45, ha=\"right\")\n",
    "        plt.yticks(range(piv.shape[0]), piv.index)\n",
    "        plt.colorbar(label=\"Probabilidade (%)\")\n",
    "        plt.title(\"Heatmap — Probabilidade por Local × Grupo (amostra)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5) Tabela ranqueada por local (impressão legível)\n",
    "    cols_print = [c for c in [\n",
    "        \"escopo\",\"cluster_id\",\"Dim_Lojas.REGIAO_CHILLI\",\n",
    "        \"Dim_Produtos.GRUPO_CHILLI\",\"Dim_Produtos.Grupo_Produto\",\n",
    "        \"Preco_Varejo_med\",\"Probabilidade_pct\",\"Previsao\"\n",
    "    ] if c in df_rank.columns]\n",
    "\n",
    "    print(\"\\n==================== RANKING (amostra tabular ordenada) ====================\")\n",
    "    for (esc, clu), df_loc in df_rank.groupby([\"escopo\",\"cluster_id\"]):\n",
    "        print(f\"\\n>>> Local: escopo={esc} | cluster_id={clu}\")\n",
    "        df_show = df_loc.sort_values(\"Probabilidade_pct\", ascending=False)[cols_print].copy()\n",
    "        # Limita a impressão a 12 linhas por local para manter a leitura rápida\n",
    "        print(df_show.head(12).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
