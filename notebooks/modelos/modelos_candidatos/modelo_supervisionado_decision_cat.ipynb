{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9ff08a",
   "metadata": {},
   "source": [
    "## Modelo Supervisionado \n",
    "\n",
    "**Projeto**: Expansão por grupos — classificador de “Bom Desempenho” por região×grupo\n",
    "**Objetivo**: Treinar e avaliar um RandomForest com pipeline (pré-processamento + OHE), escolher limiar ótimo por F1, salvar o modelo e ranquear candidatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 1: Configurações Iniciais\n",
    "# Configurações globais, importações e parâmetros do projeto.\n",
    "# Execute este notebook primeiro para preparar o ambiente.\n",
    "\n",
    "# ===== Imports principais =====\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Adicionado para visualizações mais ricas\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler  # Adicionado StandardScaler para normalização\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score, precision_recall_curve, roc_curve, accuracy_score, f1_score)\n",
    "from sklearn.inspection import permutation_importance\n",
    "from joblib import dump, load\n",
    "import logging  # Adicionado para logs\n",
    "import seaborn as sns\n",
    "# ===== Configurações globais =====\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 220)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "sns.set_style(\"whitegrid\")  # Estilo moderno do seabornlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ===== Parâmetros do projeto =====\n",
    "CAMINHO_DF_HIST = \"../../../database/dataset gerado/dataset_limpo.csv\"\n",
    "ARQ_MODELO = Path(\"../../../database/dataset gerado/modelo_arvore_decisao_perfeito.joblib\")\n",
    "ARQ_NOVAS = \"../../../database/dataset gerado/sugestoes_expansao_para_supervisionado\"\n",
    "TOP_N = 10\n",
    "TOP_K = 1\n",
    "FORCAR_TREINO = True\n",
    "LIMIAR = None\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1  # Paralelização total\n",
    "\n",
    "# ===== Função utilitária para leitura de CSV =====\n",
    "def read_csv_flex(base_path_or_stem: str) -> pd.DataFrame:\n",
    "    \"\"\"Lê CSV de forma flexível, testando múltiplos caminhos e separadores.\"\"\"\n",
    "    candidates = [\n",
    "        Path(base_path_or_stem),\n",
    "        Path(f\"{base_path_or_stem}.csv\"),\n",
    "        Path(\"data\") / f\"{base_path_or_stem}.csv\",\n",
    "        Path(\"datasets\") / f\"{base_path_or_stem}.csv\",\n",
    "    ]\n",
    "    for p in glob.glob(f\"**/{Path(base_path_or_stem).stem}*.csv\", recursive=True):\n",
    "        candidates.append(Path(p))\n",
    "    \n",
    "    tried = []\n",
    "    for path in candidates:\n",
    "        if not path.exists():\n",
    "            tried.append(str(path))\n",
    "            continue\n",
    "        for sep in [None, ';', ',']:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, engine=\"python\")\n",
    "                logging.info(f\"Arquivo carregado: {path}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                tried.append(f\"{path}(sep={sep}): {str(e)}\")\n",
    "    raise FileNotFoundError(\"CSV não encontrado. Tentativas: \" + \" | \".join(tried))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 2: Preparação de Dados\n",
    "# Constrói a base de treino com features otimizadas e labels.\n",
    "# Execute após o Notebook 1.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Função para construir a tabela de treino\n",
    "def build_train_table(df_hist: pd.DataFrame, top_k: int = 1):\n",
    "    \"\"\"Constrói a tabela de treino com features otimizadas e rótulos.\"\"\"\n",
    "    need = [\"ID_Loja\", \"Desconto\", \"Total_Preco_Varejo\", \"Total_Preco_Liquido\",\n",
    "            \"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\", \"Dim_Produtos.Grupo_Produto\"]\n",
    "    miss = [c for c in need if c not in df_hist.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Faltando colunas no histórico: {miss}\")\n",
    "\n",
    "    # Tratamento de outliers (limite de 3 desvios padrão para variáveis numéricas)\n",
    "    for col in [\"Total_Preco_Varejo\", \"Total_Preco_Liquido\", \"Desconto\"]:\n",
    "        if col in df_hist:\n",
    "            mean, std = df_hist[col].mean(), df_hist[col].std()\n",
    "            df_hist[col] = df_hist[col].clip(lower=mean - 3 * std, upper=mean + 3 * std)\n",
    "\n",
    "    # Agregações por região\n",
    "    reg_lojas = df_hist.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"ID_Loja\"].nunique()\n",
    "    reg_receita = df_hist.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"Total_Preco_Liquido\"].sum()\n",
    "    reg_receita_med = df_hist.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"Total_Preco_Liquido\"].median()\n",
    "    reg_receita_p75 = df_hist.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"Total_Preco_Liquido\"].quantile(0.75)  # Percentil 75\n",
    "    agg_reg = pd.concat([reg_receita, reg_lojas, reg_receita_med, reg_receita_p75], axis=1).reset_index()\n",
    "    agg_reg.columns = [\"Dim_Lojas.REGIAO_CHILLI\", \"regiao_receita_total\", \"regiao_num_lojas\",\n",
    "                       \"regiao_receita_mediana\", \"regiao_receita_p75\"]\n",
    "    agg_reg[\"regiao_receita_por_loja\"] = agg_reg[\"regiao_receita_total\"] / agg_reg[\"regiao_num_lojas\"].replace(0, np.nan).fillna(0)\n",
    "\n",
    "    # Medianas e variâncias por região x grupo\n",
    "    grp_meds = df_hist.groupby([\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\"]).agg(\n",
    "        grupo_med_preco_varejo=(\"Total_Preco_Varejo\", \"median\"),\n",
    "        grupo_med_desconto=(\"Desconto\", \"median\"),\n",
    "        grupo_var_preco_varejo=(\"Total_Preco_Varejo\", \"var\"),\n",
    "        grupo_p90_preco_varejo=(\"Total_Preco_Varejo\", lambda x: np.percentile(x, 90))  # Percentil 90\n",
    "    ).reset_index()\n",
    "\n",
    "    # Volume por região x grupo\n",
    "    vol = df_hist.groupby([\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\"])[\"Total_Preco_Liquido\"].sum().reset_index()\n",
    "    vol.columns = [\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\", \"grupo_regiao_receita\"]\n",
    "    vol[\"rank_na_regiao\"] = vol.groupby(\"Dim_Lojas.REGIAO_CHILLI\")[\"grupo_regiao_receita\"].rank(method=\"first\", ascending=False)\n",
    "    vol[\"Bom_Desempenho\"] = (vol[\"rank_na_regiao\"] <= top_k).astype(int)\n",
    "\n",
    "    # Montagem de X e y\n",
    "    X = vol[[\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\"]].merge(\n",
    "        agg_reg, on=\"Dim_Lojas.REGIAO_CHILLI\", how=\"left\"\n",
    "    ).merge(\n",
    "        grp_meds, on=[\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\"], how=\"left\"\n",
    "    )\n",
    "    y = vol[\"Bom_Desempenho\"].astype(int)\n",
    "\n",
    "    # Normalização das variáveis numéricas\n",
    "    num_cols = [\"regiao_receita_total\", \"regiao_num_lojas\", \"regiao_receita_por_loja\",\n",
    "                \"regiao_receita_mediana\", \"regiao_receita_p75\", \"grupo_med_preco_varejo\",\n",
    "                \"grupo_med_desconto\", \"grupo_var_preco_varejo\", \"grupo_p90_preco_varejo\"]\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols].fillna(0))\n",
    "\n",
    "    # Catálogo por grupo\n",
    "    catalogo = df_hist.groupby([\"Dim_Produtos.GRUPO_CHILLI\", \"Dim_Produtos.Grupo_Produto\"]).agg(\n",
    "        Preco_Varejo_med=(\"Total_Preco_Varejo\", \"median\")\n",
    "    ).reset_index()\n",
    "\n",
    "    return X, y, catalogo, agg_reg, grp_meds\n",
    "\n",
    "# Carregamento do histórico\n",
    "dfh = read_csv_flex(CAMINHO_DF_HIST)\n",
    "dfh = dfh.drop(columns=[\n",
    "    \"ID_Cliente\", \"Dim_Cliente.Data_Nascimento\", \"Dim_Cliente.Regiao_Cliente\",\n",
    "    \"ID_Produto\", \"Dim_Lojas.Nome_Emp\", \"Dim_Lojas.Bairro_Emp\", \"Dim_Lojas.Cidade_Emp\",\n",
    "    \"Dim_Lojas.CANAL_VENDA\", \"Dim_Lojas.Tipo_PDV\", \"Dim_Lojas.Regiao\"\n",
    "], errors=\"ignore\")\n",
    "\n",
    "# Construção da base\n",
    "X_all, y_all, catalogo, agg_reg, grp_meds = build_train_table(dfh, TOP_K)\n",
    "logging.info(f\"Forma de X: {X_all.shape} | Taxa de positivos: {y_all.mean().round(3)}\")\n",
    "\n",
    "# Visualização da distribuição do alvo\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y_all, palette=\"viridis\")\n",
    "plt.title(\"Distribuição do Alvo (Bom_Desempenho)\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Contagem\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 3: Definição e Otimização do Modelo\n",
    "# Cria e otimiza o modelo de Árvore de Decisão com GridSearchCV.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Função para criar o pré-processador\n",
    "def make_preprocessor(X):\n",
    "    \"\"\"Cria o pré-processador para transformar os dados.\"\"\"\n",
    "    cat_cols = [c for c in [\"Dim_Produtos.GRUPO_CHILLI\", \"Dim_Lojas.REGIAO_CHILLI\"] if c in X.columns]\n",
    "    num_cols = [\"regiao_receita_total\", \"regiao_num_lojas\", \"regiao_receita_por_loja\",\n",
    "                \"regiao_receita_mediana\", \"regiao_receita_p75\", \"grupo_med_preco_varejo\",\n",
    "                \"grupo_med_desconto\", \"grupo_var_preco_varejo\", \"grupo_p90_preco_varejo\"]\n",
    "\n",
    "    oh = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", oh)]), cat_cols),\n",
    "            (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]), num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return pre, cat_cols + num_cols\n",
    "\n",
    "# Função para criar o pipeline do modelo (sem pré-processamento)\n",
    "def make_model():\n",
    "    \"\"\"Cria pipeline apenas com o classificador para dados pré-processados.\"\"\"\n",
    "    clf = DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE)\n",
    "    pipeline = Pipeline([(\"clf\", clf)])\n",
    "\n",
    "    param_grid = {\n",
    "        'clf__max_depth': [2, 3, 5, 7],  # Profundidades menores para evitar overfitting\n",
    "        'clf__min_samples_leaf': [20, 50, 100],  # Mais amostras por folha\n",
    "        'clf__min_samples_split': [20, 50, 100],  # Mais amostras para split\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "    search = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring='average_precision', n_jobs=N_JOBS)\n",
    "    return search\n",
    "\n",
    "# Função para validação cruzada com dados pré-processados\n",
    "def crossval_scores(model, X, y):\n",
    "    \"\"\"Executa validação cruzada e retorna métricas detalhadas.\"\"\"\n",
    "    model.fit(X, y)\n",
    "    logging.info(f\"Melhores parâmetros: {model.best_params_}\")\n",
    "    aps, accs, aucs, f1s = [], [], [], []\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "    for tr, te in cv.split(X, y):\n",
    "        m = Pipeline(model.best_estimator_.steps)\n",
    "        m.fit(X[tr], y[tr])\n",
    "        proba = m.predict_proba(X[te])[:, 1]\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "        aps.append(average_precision_score(y[te], proba))\n",
    "        accs.append(accuracy_score(y[te], pred))\n",
    "        aucs.append(roc_auc_score(y[te], proba))\n",
    "        f1s.append(f1_score(y[te], pred))\n",
    "    logging.info(f\"CV (DT Otimizada) AP={np.mean(aps):.3f} ± {np.std(aps):.3f} | \"\n",
    "                 f\"ACC={np.mean(accs):.3f} ± {np.std(accs):.3f} | \"\n",
    "                 f\"ROC-AUC={np.mean(aucs):.3f} ± {np.std(aucs):.3f} | \"\n",
    "                 f\"F1={np.mean(f1s):.3f} ± {np.std(f1s):.3f}\")\n",
    "\n",
    "# Criação do pré-processador\n",
    "preprocessor, feature_names = make_preprocessor(X_all)\n",
    "\n",
    "# Pré-processamento antes do SMOTE\n",
    "X_all_transformed = preprocessor.fit_transform(X_all)\n",
    "logging.info(f\"Forma após pré-processamento: {X_all_transformed.shape}\")\n",
    "\n",
    "# Aplicar SMOTE\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_all_balanced, y_all_balanced = smote.fit_resample(X_all_transformed, y_all)\n",
    "logging.info(f\"Proporção após SMOTE: {y_all_balanced.mean():.3f}\")\n",
    "\n",
    "# Criação e avaliação do modelo com dados balanceados\n",
    "model = make_model()\n",
    "crossval_scores(model, X_all_balanced, y_all_balanced)\n",
    "\n",
    "# Salvar variáveis globais para uso nos notebooks subsequentes\n",
    "globals()['X_all_balanced'] = X_all_balanced\n",
    "globals()['y_all_balanced'] = y_all_balanced\n",
    "globals()['preprocessor'] = preprocessor\n",
    "globals()['feature_names'] = feature_names\n",
    "logging.info(\"Variáveis globais salvas: X_all_balanced, y_all_balanced, preprocessor, feature_names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a16b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Notebook 4: Seleção de Limiar e Visualizações\n",
    "# Escolhe o limiar ótimo e gera visualizações.\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Função para escolher limiar e gerar plots\n",
    "def choose_threshold_and_plots(model, X, y, limiar=None):\n",
    "    \"\"\"Seleciona limiar ótimo e gera visualizações.\"\"\"\n",
    "    if limiar is not None:\n",
    "        return float(limiar)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=RANDOM_STATE)  # Aumentado para 0.4\n",
    "    tr, te = next(sss.split(X, y))\n",
    "    model.fit(X[tr], y[tr])  # Dados já pré-processados\n",
    "    proba = model.predict_proba(X[te])[:, 1]\n",
    "    yte = y[te]\n",
    "\n",
    "    logging.info(\"\\n=== Hold-out (estratificado) — limiar 0.50 ===\")\n",
    "    pred050 = (proba >= 0.5).astype(int)\n",
    "    acc_050 = accuracy_score(yte, pred050)\n",
    "    print(f\"Acurácia (limiar 0.50): {acc_050:.3f}\")\n",
    "    print(classification_report(yte, pred050, digits=3))\n",
    "    print(\"Matriz:\\n\", confusion_matrix(yte, pred050))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(yte, proba):.3f} | PR-AUC: {average_precision_score(yte, proba):.3f}\")\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(yte, proba)\n",
    "    f1s = 2 * (prec * rec) / (prec + rec + 1e-9)\n",
    "    idx = np.nanargmax(f1s)\n",
    "    chosen = float(thr[idx]) if idx < len(thr) else 0.5\n",
    "    logging.info(f\"\\n→ Limiar ótimo (F1 máx): {chosen:.3f}\")\n",
    "\n",
    "    pred_opt = (proba >= chosen).astype(int)\n",
    "    acc_opt = accuracy_score(yte, pred_opt)\n",
    "    print(f\"Acurácia (limiar ótimo {chosen:.3f}): {acc_opt:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.lineplot(x=rec, y=prec, label=\"Curva PR\")\n",
    "    plt.scatter(rec[idx], prec[idx], color='red', label=\"F1 Máximo\")\n",
    "    plt.title(\"Curva Precision–Recall (Hold-out)\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    xs = thr if len(thr) else np.array([0.5])\n",
    "    ys = f1s[:-1] if len(thr) else np.array([0.0])\n",
    "    sns.lineplot(x=xs, y=ys, label=\"F1 Score\")\n",
    "    if len(thr):\n",
    "        plt.axvline(chosen, linestyle=\"--\", color='red', label=\"Limiar Ótimo\")\n",
    "    plt.title(\"F1 × Limiar (Hold-out)\")\n",
    "    plt.xlabel(\"Limiar\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(yte, proba)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.lineplot(x=fpr, y=tpr, label=\"Curva ROC\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color='gray', label=\"Aleatório\")\n",
    "    plt.title(\"Curva ROC (Hold-out)\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    cm = confusion_matrix(yte, pred_opt)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Matriz de Confusão (Limiar Ótimo)\")\n",
    "    plt.xticks([0.5, 1.5], [\"Neg\", \"Pos\"])\n",
    "    plt.yticks([0.5, 1.5], [\"Neg\", \"Pos\"])\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Verdadeiro\")\n",
    "    plt.show()\n",
    "\n",
    "    return chosen\n",
    "\n",
    "# Execução\n",
    "chosen_thr = choose_threshold_and_plots(model, X_all_balanced, y_all_balanced, LIMIAR)\n",
    "globals()['chosen_thr'] = chosen_thr  # Salvar para notebooks subsequentes\n",
    "logging.info(f\"Limiar escolhido salvo: {chosen_thr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb70538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 5: Treino Final e Importância de Features\n",
    "# Realiza o treino final, salva o modelo e analisa importância de features.\n",
    "\n",
    "from joblib import dump\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "# Verificar se as variáveis globais estão disponíveis\n",
    "if 'X_all_balanced' not in globals() or 'y_all_balanced' not in globals() or 'preprocessor' not in globals():\n",
    "    raise ValueError(\"Variáveis globais X_all_balanced, y_all_balanced ou preprocessor não encontradas. Execute o Notebook 3 primeiro.\")\n",
    "\n",
    "# Treino final com dados pré-processados e balanceados\n",
    "model.fit(X_all_balanced, y_all_balanced)\n",
    "dump(model, ARQ_MODELO.as_posix())\n",
    "logging.info(f\"Modelo salvo em: {ARQ_MODELO.name}\")\n",
    "\n",
    "# Importância por permutação\n",
    "feature_names = preprocessor.get_feature_names_out()  # Nomes das colunas transformadas\n",
    "result = permutation_importance(model, X_all_balanced, y_all_balanced, n_repeats=10, random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance_mean\": result.importances_mean,\n",
    "    \"importance_std\": result.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "display(importances.head(15))\n",
    "\n",
    "# Visualização Top-10 importâncias\n",
    "top = importances.head(10).iloc[::-1]\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=\"importance_mean\", y=\"feature\", data=top, palette=\"viridis\")\n",
    "plt.title(\"Importância por Permutação — Top 10\")\n",
    "plt.xlabel(\"Δ Métrica (Média)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 6: Predições em Novas Localizações e Ranking\n",
    "# Aplica o modelo em novas localizações e exporta o ranking.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "# Verificar se as variáveis globais estão disponíveis\n",
    "if 'preprocessor' not in globals() or 'model' not in globals() or 'chosen_thr' not in globals():\n",
    "    raise ValueError(\"Variáveis globais preprocessor, model ou chosen_thr não encontradas. Execute os Notebooks 3 e 4 primeiro.\")\n",
    "\n",
    "# Leitura de novas localizações\n",
    "novas_raw = read_csv_flex(ARQ_NOVAS)\n",
    "logging.info(f\"Colunas nas novas localizações: {list(novas_raw.columns)}\")\n",
    "\n",
    "rename_map = {c: \"Dim_Lojas.Estado_Emp\" if c.strip().lower() == \"estado\" else\n",
    "              \"Dim_Lojas.REGIAO_CHILLI\" if c.strip().lower() == \"regiao_chilli\" else c\n",
    "              for c in novas_raw.columns}\n",
    "novas = novas_raw.rename(columns=rename_map).copy()\n",
    "for c in [\"escopo\", \"cluster_id\"]:\n",
    "    if c not in novas.columns:\n",
    "        novas[c] = \"NA\"\n",
    "\n",
    "# Criação de candidatos\n",
    "base = novas.rename(columns={\"Dim_Lojas.REGIAO_CHILLI\": \"REG_TMP\"}, errors=\"ignore\").copy()\n",
    "cand = base.merge(\n",
    "    catalogo[[\"Dim_Produtos.GRUPO_CHILLI\", \"Dim_Produtos.Grupo_Produto\", \"Preco_Varejo_med\"]],\n",
    "    how=\"cross\"\n",
    ")\n",
    "\n",
    "# Anexação de agregados\n",
    "cand = cand.merge(agg_reg, left_on=\"REG_TMP\", right_on=\"Dim_Lojas.REGIAO_CHILLI\", how=\"left\")\n",
    "cand = cand.merge(grp_meds, left_on=[\"REG_TMP\", \"Dim_Produtos.GRUPO_CHILLI\"],\n",
    "                  right_on=[\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\"], how=\"left\",\n",
    "                  suffixes=(\"\", \"_dup\"))\n",
    "cand[\"Dim_Lojas.REGIAO_CHILLI\"] = cand[\"REG_TMP\"]\n",
    "\n",
    "# Features\n",
    "feat_cols = [\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\",\n",
    "             \"regiao_receita_total\", \"regiao_num_lojas\", \"regiao_receita_por_loja\",\n",
    "             \"regiao_receita_mediana\", \"regiao_receita_p75\", \"grupo_med_preco_varejo\",\n",
    "             \"grupo_med_desconto\", \"grupo_var_preco_varejo\", \"grupo_p90_preco_varejo\"]\n",
    "cand_feat = cand[feat_cols].copy()\n",
    "\n",
    "# Pré-processamento das features com o mesmo preprocessor do Notebook 3\n",
    "cand_feat_transformed = preprocessor.transform(cand_feat)\n",
    "\n",
    "# Predições\n",
    "proba = model.predict_proba(cand_feat_transformed)[:, 1]\n",
    "cand[\"Probabilidade_Sucesso\"] = proba\n",
    "cand[\"Previsao\"] = (proba >= chosen_thr).astype(int)\n",
    "cand[\"Probabilidade_pct\"] = (proba * 100).round(1)\n",
    "\n",
    "# Ranking\n",
    "id_cols = [\"escopo\", \"cluster_id\"]\n",
    "rank = cand.sort_values(id_cols + [\"Probabilidade_Sucesso\"], ascending=[True, True, False]).reset_index(drop=True)\n",
    "top_n = rank.groupby(id_cols, group_keys=False).head(TOP_N)\n",
    "\n",
    "cols_out = (id_cols + [\"Dim_Lojas.REGIAO_CHILLI\", \"Dim_Produtos.GRUPO_CHILLI\", \"Dim_Produtos.Grupo_Produto\",\n",
    "                       \"Preco_Varejo_med\", \"Probabilidade_pct\", \"Previsao\"])\n",
    "saida = top_n[[c for c in cols_out if c in top_n.columns]].copy()\n",
    "\n",
    "print(\"\\n==================== RANKING TOP-N POR LOCAL ====================\")\n",
    "for (esc, clu), df_loc in saida.groupby([\"escopo\", \"cluster_id\"]):\n",
    "    print(f\"\\n>>> Local: escopo={esc} | cluster_id={clu}  (Top-{TOP_N})\")\n",
    "    print(df_loc.drop(columns=[\"escopo\", \"cluster_id\"]).to_string(index=False))\n",
    "\n",
    "# Visualização exemplo\n",
    "first_key = next(iter(saida.groupby([\"escopo\", \"cluster_id\"]).groups.keys()))\n",
    "ex = saida.set_index([\"escopo\", \"cluster_id\"]).loc[first_key].reset_index(drop=True)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=\"Probabilidade_pct\", y=\"Dim_Produtos.GRUPO_CHILLI\", data=ex, palette=\"viridis\")\n",
    "plt.title(f\"Top-{TOP_N} — {first_key}\")\n",
    "plt.xlabel(\"Probabilidade (%)\")\n",
    "plt.ylabel(\"Grupo\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exportação em CSV e JSON\n",
    "ARQ_OUT = \"../../../database/dataset gerado/ranking_arvore_decisao_perfeito.csv\"\n",
    "ARQ_OUT_JSON = \"../../../database/dataset gerado/ranking_arvore_decisao_perfeito.json\"\n",
    "saida.to_csv(ARQ_OUT, index=False, sep=\";\")\n",
    "saida.to_json(ARQ_OUT_JSON, orient=\"records\", lines=True)\n",
    "logging.info(f\"Arquivos exportados: {ARQ_OUT}, {ARQ_OUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 7: Análise Final e Visualizações Adicionais\n",
    "# Analisa o ranking exportado e realiza checagens finais.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "\n",
    "# Leitura do ranking exportado\n",
    "if not os.path.exists(ARQ_OUT):\n",
    "    logging.error(f\"Arquivo não encontrado: {ARQ_OUT}.\")\n",
    "else:\n",
    "    df_rank = pd.read_csv(ARQ_OUT, sep=\";\")\n",
    "    logging.info(f\"Ranking carregado: {ARQ_OUT} | shape={df_rank.shape}\")\n",
    "    display(df_rank.head(10))\n",
    "\n",
    "    # Histograma de probabilidades\n",
    "    if \"Probabilidade_pct\" in df_rank.columns:\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        sns.histplot(df_rank[\"Probabilidade_pct\"].dropna(), bins=20, kde=True, color=\"blue\")\n",
    "        plt.xlabel(\"Probabilidade Prevista (%)\")\n",
    "        plt.ylabel(\"Frequência\")\n",
    "        plt.title(\"Distribuição de Probabilidades (Ranking Exportado)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Barras Top-N primeiro local\n",
    "    if {\"escopo\", \"cluster_id\", \"Dim_Produtos.GRUPO_CHILLI\", \"Probabilidade_pct\"} <= set(df_rank.columns):\n",
    "        first_key = next(iter(df_rank.groupby([\"escopo\", \"cluster_id\"]).groups.keys()))\n",
    "        amostra = df_rank.set_index([\"escopo\", \"cluster_id\"]).loc[first_key].reset_index(drop=True)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=\"Probabilidade_pct\", y=\"Dim_Produtos.GRUPO_CHILLI\", data=amostra, palette=\"viridis\")\n",
    "        plt.title(f\"Top-N — {first_key}\")\n",
    "        plt.xlabel(\"Probabilidade (%)\")\n",
    "        plt.ylabel(\"Grupo\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Heatmap\n",
    "    if {\"escopo\", \"cluster_id\", \"Dim_Produtos.GRUPO_CHILLI\", \"Probabilidade_pct\"} <= set(df_rank.columns):\n",
    "        keys = list(df_rank.groupby([\"escopo\", \"cluster_id\"]).groups.keys())[:8]\n",
    "        sub = df_rank.set_index([\"escopo\", \"cluster_id\"]).loc[keys].reset_index()\n",
    "        sub[\"local\"] = sub[\"escopo\"].astype(str) + \" | \" + sub[\"cluster_id\"].astype(str)\n",
    "        piv = sub.pivot_table(index=\"local\", columns=\"Dim_Produtos.GRUPO_CHILLI\",\n",
    "                              values=\"Probabilidade_pct\", aggfunc=\"max\")\n",
    "        plt.figure(figsize=(max(8, 0.35 * piv.shape[1] + 3), max(5, 0.35 * piv.shape[0] + 2)))\n",
    "        sns.heatmap(piv.fillna(0), annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Probabilidade (%)'})\n",
    "        plt.title(\"Heatmap — Probabilidade por Local × Grupo (Amostra)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Tabela ranqueada\n",
    "    cols_print = [c for c in [\n",
    "        \"escopo\", \"cluster_id\", \"Dim_Lojas.REGIAO_CHILLI\",\n",
    "        \"Dim_Produtos.GRUPO_CHILLI\", \"Dim_Produtos.Grupo_Produto\",\n",
    "        \"Preco_Varejo_med\", \"Probabilidade_pct\", \"Previsao\"\n",
    "    ] if c in df_rank.columns]\n",
    "    print(\"\\n==================== RANKING (Amostra Tabular Ordenada) ====================\")\n",
    "    for (esc, clu), df_loc in df_rank.groupby([\"escopo\", \"cluster_id\"]):\n",
    "        print(f\"\\n>>> Local: escopo={esc} | cluster_id={clu}\")\n",
    "        df_show = df_loc.sort_values(\"Probabilidade_pct\", ascending=False)[cols_print].copy()\n",
    "        print(df_show.head(12).to_string(index=False))\n",
    "\n",
    "# Checagens finais\n",
    "nulls = X_all.isna().sum().sort_values(ascending=False)\n",
    "logging.info(\"Nulos nas features (top 10):\")\n",
    "print(nulls.head(10).to_string())\n",
    "\n",
    "if \"Dim_Lojas.REGIAO_CHILLI\" in novas.columns:\n",
    "    regs_novas = set(novas[\"Dim_Lojas.REGIAO_CHILLI\"].dropna().unique())\n",
    "    regs_hist = set(agg_reg[\"Dim_Lojas.REGIAO_CHILLI\"].unique())\n",
    "    faltando = regs_novas - regs_hist\n",
    "    if faltando:\n",
    "        logging.warning(f\"Regiões nas novas SEM histórico: {faltando}\")\n",
    "    else:\n",
    "        logging.info(\"Todas as regiões das novas têm histórico.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
