{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be42f3d",
   "metadata": {},
   "source": [
    "# Modelo de Regressão Logística \n",
    "###### Modelo Individual Giorgia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507b690",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "A seguir, apresento a modelagem de um **modelo de Regressão Logística** aplicado ao projeto da **Chilli Beans**, com o objetivo de prever o desempenho de suas lojas.  \n",
    "Essa técnica foi escolhida por sua capacidade de **classificação binária**, permitindo estimar a probabilidade de uma unidade apresentar **Bom Desempenho** ou **Baixo Desempenho** a partir de variáveis explicativas.  \n",
    "\n",
    "Além de avaliar o desempenho atual, o modelo também funciona como uma **ferramenta de apoio estratégico**, capaz de indicar os **melhores locais para a abertura de novas lojas pelo Brasil**, com base nos padrões identificados nas variáveis de maior impacto.  \n",
    "\n",
    "O processo de modelagem foi estruturado em etapas: **preparação e tratamento dos dados**, **divisão em treino e teste**, **normalização**, **codificação de variáveis categóricas**, **ajuste de hiperparâmetros** e **interpretação dos coeficientes**. Essa abordagem garante não apenas a robustez estatística, mas também a geração de **insights práticos** que podem orientar tanto a **expansão da rede** quanto a **otimização de canais e produtos já existentes**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Importação das bibliotecas\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Exibir mais colunas no pandas\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ad13a",
   "metadata": {},
   "source": [
    "### Criação da Variável Alvo e Preparação do Dataset\n",
    "- No primeiro passo, foram carregados dois datasets: o df_limpo, e o df_codificado. \n",
    "- Em seguida, a receita total de cada loja foi agregada, e a mediana dessas receitas foi utilizada como limiar para definir a variável alvo Bom_Desempenho. \n",
    "\n",
    "- Lojas com receita igual ou superior à mediana foram classificadas como de bom desempenho (1), enquanto as demais foram classificadas como de baixo desempenho (0). \n",
    "\n",
    "Essa variável foi então incorporada ao dataset codificado, garantindo que cada registro possuísse a informação do desempenho da respectiva loja. O objetivo desse processamento é transformar o problema em uma tarefa de classificação binária, permitindo que modelos preditivos aprendam padrões que diferenciam lojas de alto e baixo desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b919469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Carregar dataset limpo\n",
    "df_limpo = pd.read_csv(\"../../../database/dataset gerado/dataset_limpo.csv\")\n",
    "\n",
    "# Agregar receita total por loja\n",
    "desempenho_loja = df_limpo.groupby(\"ID_Loja\")[\"Total_Preco_Liquido\"].sum().reset_index()\n",
    "\n",
    "# Definir threshold (mediana da receita total)\n",
    "threshold = desempenho_loja[\"Total_Preco_Liquido\"].median()\n",
    "\n",
    "# Criar variável alvo\n",
    "desempenho_loja[\"Bom_Desempenho\"] = (desempenho_loja[\"Total_Preco_Liquido\"] >= threshold).astype(int)\n",
    "\n",
    "# Agora juntamos essa variável ao dataset codificado\n",
    "df_cod = pd.read_csv(\"../../../database/dataset gerado/dataset_codificado.csv\")\n",
    "\n",
    "df_cod = df_cod.merge(desempenho_loja[[\"ID_Loja\", \"Bom_Desempenho\"]], on=\"ID_Loja\", how=\"left\")\n",
    "\n",
    "print(df_cod[[\"ID_Loja\", \"Bom_Desempenho\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Carregar o dataset\n",
    "\n",
    "df_limpo = pd.read_csv(\"../../../database/dataset gerado/dataset_limpo.csv\")\n",
    "df_codificado = pd.read_csv(\"../../../database/dataset gerado/dataset_codificado.csv\")\n",
    "\n",
    "print(\"Dataset limpo:\", df_limpo.shape)\n",
    "print(\"Colunas disponíveis:\", df_limpo.columns.tolist())\n",
    "\n",
    "print(\"Dataset codificado:\", df_codificado.shape)\n",
    "print(\"Colunas disponíveis:\", df_codificado.columns.tolist())\n",
    "\n",
    "display(df_limpo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784aa61f",
   "metadata": {},
   "source": [
    "\n",
    "_O dataset final, pronto para a modelagem, consolidou-se com 31.406 observações e 27 features. Uma característica crucial identificada foi o forte desbalanceamento de classes, com aproximadamente 83% das lojas pertencendo à classe de \"Bom Desempenho\". Este é um fator que demanda atenção especial na etapa de treinamento, a fim de garantir que o modelo aprenda a identificar corretamente ambas as classes, e não apenas a majoritária._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca3612",
   "metadata": {},
   "source": [
    "### Preparação dos Dados para Modelagem\n",
    "Neste bloco, finalizamos a preparação dos dados: selecionamos apenas as features importantes para compor o conjunto X e isolamos nossa variável-alvo em y. \n",
    "Com isso, traduzimos nosso problema de negócio em um formato limpo e estruturado, pronto para ser resolvido por um modelo de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Lista de colunas a serem removidas\n",
    "to_drop = [\n",
    "    \"ID_Loja\", \"ID_Cliente\", \"ID_Produto\",\n",
    "    \"Dim_Cliente.Data_Nascimento\", \"Dim_Lojas.Nome_Emp\", \"Dim_Lojas.Bairro_Emp\",\n",
    "    \"Dim_Lojas.Cidade_Emp\", \"Dim_Lojas.Estado_Emp\", \"Dim_Lojas.Regiao\",\n",
    "    \"Dim_Produtos.Grupo_Produto\", \"Dim_Produtos.GRUPO_CHILLI\"\n",
    "]\n",
    "to_drop_existing = [c for c in to_drop if c in df_cod.columns]\n",
    "\n",
    "# Separação de features (X) e alvo (y) a partir de df_cod\n",
    "X = df_cod.drop(columns=to_drop_existing + [\"Bom_Desempenho\"], errors=\"ignore\")\n",
    "\n",
    "# Preencher valores ausentes\n",
    "y = df_cod[\"Bom_Desempenho\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Shape X:\", X.shape, \"Shape y:\", y.shape)\n",
    "print(\"Algumas features (head):\")\n",
    "display(X.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ae27c",
   "metadata": {},
   "source": [
    "### Tratamento de Valores Ausentes e Conversão de Tipos\n",
    "Neste bloco, foi feita uma verificação de valores ausentes em todas as features (X).\n",
    "Para garantir consistência dos dados:\n",
    "- Colunas numéricas tiveram valores faltantes preenchidos com a mediana.\n",
    "- Colunas categóricas codificadas ou one-hot foram preenchidas com 0.\n",
    "\n",
    "Por fim, todas as colunas foram convertidas para tipos numéricos, preenchendo eventuais NaN remanescentes com 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Tratar valores ausentes e converter tipos\n",
    "# Ver número de missing por coluna\n",
    "miss = X.isna().sum()\n",
    "miss = miss[miss > 0].sort_values(ascending=False)\n",
    "print(\"Colunas com missing:\\n\", miss)\n",
    "\n",
    "# - numéricos: preencher com mediana\n",
    "# - categóricos codificados/one-hot: preencher com 0\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "for c in num_cols:\n",
    "    med = X[c].median()\n",
    "    X[c].fillna(med, inplace=True)\n",
    "for c in cat_cols:\n",
    "    X[c].fillna(0, inplace=True)\n",
    "\n",
    "# Garantir que todas as colunas são numéricas\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9a53e",
   "metadata": {},
   "source": [
    "\n",
    "_**Conclusão**: O dataset X ficou totalmente limpo, sem valores ausentes, garantindo que o modelo de regressão logística possa ser treinado sem problemas de inconsistência de dados._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a27387",
   "metadata": {},
   "source": [
    "### Divisão Estratificada dos Dados em Treino e Teste\n",
    "Neste passo, dividimos nosso dataset em dois conjuntos: um para treinar o modelo (80% dos dados) e outro, completamente separado, para testar sua performance (20%).\n",
    "\n",
    "A parte mais importante foi usar o parâmetro stratify=y.\n",
    "Como 83% das nossas lojas são de \"Bom Desempenho\", essa opção garante que essa mesma proporção seja mantida tanto no treino quanto no teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab531d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Split treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Treino:\", X_train.shape, \"Teste:\", X_test.shape)\n",
    "print(\"Proporção target (treino):\", y_train.mean(), \" (teste):\", y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74feba0",
   "metadata": {},
   "source": [
    "O output confirma que isso foi feito com sucesso, nos dando a certeza de que a avaliação final do modelo será justa e representativa da realidade dos nossos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a46b8",
   "metadata": {},
   "source": [
    "### Pipeline, GridSearchCV e Treinamento do Modelo\n",
    "Neste bloco, foi criado um pipeline que primeiro aplica StandardScaler para normalizar as features e depois treina um modelo de Regressão Logística com penalização balanceada para lidar com o desbalanceamento de classes.\n",
    "\n",
    "Em seguida, utilizou-se o GridSearchCV com validação cruzada estratificada para encontrar os melhores hiperparâmetros (C e penalty) otimizando a métrica ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac64a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Pipeline e GridSearchCV\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"liblinear\", class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__C\": [0.01, 0.1, 1, 5, 10],\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline, param_grid,\n",
    "    scoring=\"roc_auc\", cv=cv, n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhor score CV (roc_auc):\", grid.best_score_)\n",
    "print(\"Melhos params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed495f",
   "metadata": {},
   "source": [
    "_**Análise:** O modelo encontrou a melhor combinação de parâmetros (C=1 e penalty='l1') com um ROC AUC médio de 0,727, indicando uma boa capacidade do modelo em diferenciar lojas de bom e baixo desempenho._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37deb5",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo\n",
    "Neste bloco, o modelo treinado foi avaliado usando o conjunto de teste. Foram calculadas métricas de classificação como accuracy, precision, recall, F1 e ROC AUC, além de gerar o classification report e a matriz de confusão.\n",
    "\n",
    "- ROC: É um gráfico que mostra o quão bom o modelo é em separar as lojas de \"Bom Desempenho\" das de \"Mau Desempenho\".\n",
    "\n",
    "- AUC: É a nota que o modelo tira nesse gráfico, de 0.5 a 1.0. Quanto mais perto de 1.0, melhor ele é em acertar a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60912bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Avaliação\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff839532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ruim\",\"Bom\"], yticklabels=[\"Ruim\",\"Bom\"])\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce405e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, th = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\", linewidth=0.6)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262a1e4",
   "metadata": {},
   "source": [
    "#### Conclusão do output:\n",
    "O modelo apresenta accuracy de 66%, com alta precisão para a classe de bom desempenho (0,91), mas menor para a classe de baixo desempenho (0,28).\n",
    "- O recall indica que 66% das lojas de bom desempenho foram corretamente identificadas.\n",
    "- O ROC AUC de 0,72 confirma uma boa capacidade discriminativa do modelo.\n",
    "- A matriz de confusão mostra que a maioria dos erros ocorre na classe minoritária (lojas de baixo desempenho), evidenciando o desbalanceamento das classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1861b9a",
   "metadata": {},
   "source": [
    "### Interpretação dos coeficientes:\n",
    "Os coeficientes do modelo indicam o impacto de cada feature na probabilidade de uma loja ser de bom desempenho. Valores positivos aumentam a chance, enquanto valores negativos reduzem. Por exemplo, Total_Preco_Liquido tem forte efeito positivo, mostrando que lojas com maior faturamento tendem a performar melhor. Alguns canais de venda específicos aparecem como negativos, sugerindo que eles podem estar associados a menor desempenho. Essa análise ajuda a identificar quais características das lojas mais influenciam o sucesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37468297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 - Interpretacao dos coeficientes\n",
    "# Recuperar coeficientes do classificador (após scaler os coef correspondem às features)\n",
    "clf = best_model.named_steps[\"clf\"]\n",
    "coefs = clf.coef_[0]\n",
    "features = X.columns\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"coef\": coefs,\n",
    "    \"odds_ratio\": np.exp(coefs)\n",
    "}).sort_values(by=\"odds_ratio\", ascending=False)\n",
    "\n",
    "# Top positivos (aumentam chance de Bom_Desempenho)\n",
    "print(\"Top 15 que aumentam odds (odds_ratio > 1):\")\n",
    "display(coef_df.head(15))\n",
    "\n",
    "# Top negativos (reduzem chance de Bom_Desempenho)\n",
    "print(\"Top 15 que reduzem odds (odds_ratio < 1):\")\n",
    "display(coef_df.tail(15).sort_values(\"odds_ratio\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba879e",
   "metadata": {},
   "source": [
    "### Predição de novas localizações:\n",
    "A função desenvolvida permite estimar o desempenho esperado de novas lojas, retornando a probabilidade de bom desempenho e uma classificação binária. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 - Função para predizer novas localizações\n",
    "def prever_novas_localizacoes(df_novas, model=best_model, features=X.columns):\n",
    "    \"\"\"\n",
    "    Recebe um DataFrame com as mesmas colunas/features que X (codificadas),\n",
    "    retorna probabilidade de Bom_Desempenho e previsao binaria.\n",
    "    \"\"\"\n",
    "    # garantir colunas na ordem correta, criar colunas faltantes com 0\n",
    "    df_copy = df_novas.copy()\n",
    "    for c in features:\n",
    "        if c not in df_copy.columns:\n",
    "            df_copy[c] = 0\n",
    "    df_copy = df_copy[features]\n",
    "    probs = model.predict_proba(df_copy)[:,1]\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return pd.DataFrame({\"prob_bom_desempenho\": probs, \"pred_bom_desempenho\": preds})\n",
    "\n",
    "# Exemplo de uso com amostra do próprio X_test\n",
    "amostra = X_test.sample(5, random_state=42)\n",
    "resultado = prever_novas_localizacoes(amostra)\n",
    "display(amostra.head())\n",
    "display(resultado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0032ce8",
   "metadata": {},
   "source": [
    "#### Predição em Cenários de Novas Localizações\n",
    "Após o treinamento e validação do modelo supervisionado, a próxima etapa foi aplicá-lo em um novo conjunto de dados que não pertence ao dataset original. Esses dados representam cenários de possíveis novas localizações e foram previamente gerados a partir do modelo não supervisionado, responsável por agrupar e simplificar as variáveis relevantes.\n",
    "A ideia dessa etapa é simular o uso real do modelo: em vez de apenas avaliar seu desempenho em dados históricos, passamos a aplicá-lo em situações inéditas, obtendo a probabilidade de sucesso para cada nova localização. Isso nos permite comparar locais entre si e priorizar aqueles com maior potencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar dataset das novas localizações\n",
    "df_novas = pd.read_csv(\"../../../database/dataset gerado/ranking_grupo_chilli_por_local_simplificado.csv\")\n",
    "# 2. Dropar colunas que não são features (se existirem)\n",
    "df_novas = df_novas.drop(columns=to_drop_existing, errors=\"ignore\")\n",
    "\n",
    "# 3. Prever desempenho\n",
    "resultado_novas = prever_novas_localizacoes(df_novas, model=best_model, features=X.columns)\n",
    "\n",
    "# 4. Exibir resultados\n",
    "display(resultado_novas.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66461888",
   "metadata": {},
   "source": [
    "#### Conclusão dos Resultados em Novas Localizações\n",
    "Os resultados obtidos indicam que, para as novas localizações simuladas a partir do modelo não supervisionado, o classificador atribuiu uma probabilidade extremamente elevada de bom desempenho (próxima de 1 em todos os casos). Isso significa que, segundo os padrões aprendidos pelo modelo supervisionado, esses cenários apresentam características muito semelhantes às unidades já existentes que tiveram performance positiva no histórico.\n",
    "\n",
    "Do ponto de vista da análise exploratória regional, esse achado reforça dois pontos:\n",
    "Coerência entre os modelos, os agrupamentos gerados pela análise não supervisionada selecionaram pontos com perfil consistente de sucesso, o que valida a complementaridade das duas abordagens.\n",
    "\n",
    "- Potencial de expansão homogêneo: a uniformidade nas probabilidades sugere que, para a amostra testada, não há distinções marcantes entre as regiões: todas são vistas pelo modelo como oportunidades favoráveis.\n",
    "\n",
    "Esse resultado não implica necessariamente que todas as novas localizações terão sucesso garantido na prática, mas mostra que, com base nas variáveis disponíveis, o modelo não identificou sinais de risco relevante em nenhum dos cenários simulados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a9b78",
   "metadata": {},
   "source": [
    "## Análise Exploratória Regional "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c7c24",
   "metadata": {},
   "source": [
    "Para complementar os resultados do modelo, foi realizada uma análise exploratória regional. O objetivo foi entender se padrões de desempenho das lojas variam por região e se as diferenças observadas refletem características reais dos dados, como concentração de lojas ou receita média, em vez de serem apenas um efeito do modelo. Essa abordagem ajuda a interpretar os insights de forma mais contextualizada e a validar que certas regiões apresentam, de fato, maior probabilidade de lojas de bom desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ec744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar um mapeamento de ID_Loja para Regiao a partir do df_limpo\n",
    "#    Usamos drop_duplicates para ter certeza que temos uma linha por loja\n",
    "loja_regiao_map = df_limpo[['ID_Loja', 'Dim_Lojas.REGIAO_CHILLI']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Juntar essa informação de região com nosso dataframe de desempenho\n",
    "#    desempenho_loja já contém ID_Loja, Total_Preco_Liquido (agregado), e Bom_Desempenho\n",
    "analise_regional_df = pd.merge(desempenho_loja, loja_regiao_map, on='ID_Loja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Agrupar por região e calcular as métricas\n",
    "sumario_regional = analise_regional_df.groupby('Dim_Lojas.REGIAO_CHILLI').agg(\n",
    "    N_Lojas=('ID_Loja', 'nunique'),\n",
    "    Receita_Media_por_Loja=('Total_Preco_Liquido', 'mean'),\n",
    "    Proporcao_Bom_Desempenho=('Bom_Desempenho', 'mean')\n",
    ").sort_values(by='N_Lojas', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6aeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Formatar para melhor visualização\n",
    "sumario_regional['Proporcao_Bom_Desempenho'] = (sumario_regional['Proporcao_Bom_Desempenho'] * 100).map('{:.2f}%'.format)\n",
    "sumario_regional['Receita_Media_por_Loja'] = sumario_regional['Receita_Media_por_Loja'].map('R$ {:,.2f}'.format)\n",
    "\n",
    "print(\"Análise de Desempenho por Região:\")\n",
    "display(sumario_regional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ba95a",
   "metadata": {},
   "source": [
    "### Conclusão Geral e Estratégica\n",
    "\n",
    "O modelo de regressão logística desenvolvido demonstrou boa capacidade de classificação das lojas da Chilli Beans em Bom Desempenho e Baixo Desempenho, alcançando ROC AUC de 0,72 e acurácia de 66%, mesmo diante do forte desbalanceamento de classes. \n",
    "\n",
    "A análise dos coeficientes mostrou que variáveis como receita total, estado, região da loja e canais de venda têm impacto significativo na performance, oferecendo insights diretos sobre os fatores que impulsionam ou limitam o sucesso das unidades.\n",
    "\n",
    "A análise regional revelou padrões claros: São Paulo concentra o maior número de lojas e apresenta a maior receita média, enquanto Nordeste e Sudeste mostram oportunidades de melhoria. \n",
    "Sul e Norte, apesar de possuírem menos lojas, apresentam proporções relativamente altas de bom desempenho, indicando que características locais podem superar a simples quantidade de unidades. \n",
    "\n",
    "Esses resultados permitem à empresa priorizar regiões estratégicas, ajustar mix de produtos e canais de venda, e planejar novas aberturas com base em probabilidade de sucesso, minimizando riscos.\n",
    "\n",
    "Em termos de negócios, o modelo transforma dados históricos em decisões acionáveis, conectando análise quantitativa à performance real das lojas. Ele fornece uma ferramenta prática para otimizar operações, direcionar investimentos e apoiar decisões estratégicas de expansão, mostrando como a inteligência de dados pode gerar vantagem competitiva sustentável para a rede.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fef27a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe84472",
   "metadata": {},
   "source": [
    "A seguir, apresentamos gráficos que permitem observar de forma visual o desempenho do modelo, a distribuição das probabilidades previstas, o impacto das principais variáveis e as diferenças regionais. Essas representações facilitam a interpretação dos resultados e ajudam a conectar a análise estatística a insights práticos para decisões estratégicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4a5d0",
   "metadata": {},
   "source": [
    "### Distribuição das Probabilidades Previstas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(y_proba, bins=30, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribuição das Probabilidades Previstas de Bom Desempenho\")\n",
    "plt.xlabel(\"Probabilidade de Bom Desempenho\")\n",
    "plt.ylabel(\"Quantidade de Lojas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73863fd7",
   "metadata": {},
   "source": [
    "### Importância das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262637d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 positivos e negativos\n",
    "top_features = pd.concat([coef_df.head(10), coef_df.tail(10)])\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"odds_ratio\", y=\"feature\", data=top_features, palette=\"coolwarm\")\n",
    "plt.axvline(1, color=\"black\", linestyle=\"--\")\n",
    "plt.title(\"Impacto das Features na Probabilidade de Bom Desempenho (Odds Ratio)\")\n",
    "plt.xlabel(\"Odds Ratio\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8ae18",
   "metadata": {},
   "source": [
    "### Proporção de Bom Desempenho por Região"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter porcentagem de string para float para plot\n",
    "sumario_regional_plot = sumario_regional.copy()\n",
    "sumario_regional_plot['Proporcao_Bom_Desempenho'] = sumario_regional_plot['Proporcao_Bom_Desempenho'].str.replace('%','').astype(float)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=sumario_regional_plot.index, y='Proporcao_Bom_Desempenho', data=sumario_regional_plot, palette=\"viridis\")\n",
    "plt.title(\"Proporção de Lojas de Bom Desempenho por Região\")\n",
    "plt.ylabel(\"Proporção (%)\")\n",
    "plt.xlabel(\"Região\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785737a",
   "metadata": {},
   "source": [
    "### Matriz de Confusão com Porcentagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ab93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = cm / cm.sum(axis=1)[:, None] * 100\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_percent, annot=True, fmt=\".1f\", cmap=\"Blues\", xticklabels=[\"Ruim\",\"Bom\"], yticklabels=[\"Ruim\",\"Bom\"])\n",
    "plt.xlabel(\"Previsto\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusão (%)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e47cb",
   "metadata": {},
   "source": [
    "### Boxplot de Receita por Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89afbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=\"Bom_Desempenho\", y=\"Total_Preco_Liquido\", data=desempenho_loja, palette=\"Set2\")\n",
    "plt.xticks([0,1], [\"Baixo Desempenho\",\"Bom Desempenho\"])\n",
    "plt.title(\"Distribuição da Receita Total por Classe de Desempenho\")\n",
    "plt.ylabel(\"Receita Total\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
