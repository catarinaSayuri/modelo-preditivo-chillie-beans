{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584b724e",
   "metadata": {},
   "source": [
    "#  Modelo Preditivo N√£o Supervisionado Otimizado - Chilli Beans\n",
    "\n",
    "##  An√°lise Avan√ßada de Localiza√ß√£o para Expans√£o de Lojas com Grid Search, PyCaret e M√©tricas de Classifica√ß√£o\n",
    "\n",
    "Este notebook implementa uma vers√£o **altamente otimizada** do modelo de clusteriza√ß√£o para identificar as melhores localiza√ß√µes para novas lojas da Chilli Beans. \n",
    "\n",
    "### üéØ **Principais Melhorias Implementadas:**\n",
    "\n",
    "1. **üîç Grid Search com Silhouette Score**: Otimiza√ß√£o autom√°tica de hiperpar√¢metros\n",
    "2. **üìä Elbow Method**: Valida√ß√£o visual da escolha de clusters\n",
    "3. **ü§ñ PyCaret AutoML**: Valida√ß√£o independente com ferramentas de AutoML\n",
    "4. **üéØ Matriz de Confus√£o**: Avalia√ß√£o pseudo-classifica√ß√£o para validar clusters\n",
    "5. **üìà Curva ROC e AUC**: M√©tricas avan√ßadas de separabilidade dos dados\n",
    "6. **üí¨ Coment√°rios em Portugu√™s**: C√≥digo totalmente documentado para manutenibilidade\n",
    "\n",
    "### üìã **Metodologia Cient√≠fica:**\n",
    "- **Otimiza√ß√£o sistem√°tica** de hiperpar√¢metros usando valida√ß√£o cruzada\n",
    "- **Compara√ß√£o de m√∫ltiplas abordagens** (manual vs AutoML)\n",
    "- **Valida√ß√£o rigorosa** com m√©tricas de classifica√ß√£o simulada\n",
    "- **Interpretabilidade** atrav√©s de visualiza√ß√µes avan√ßadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6470e93",
   "metadata": {},
   "source": [
    "## üì¶ 1. Importa√ß√£o de Bibliotecas e Carregamento dos Dados\n",
    "\n",
    "Nesta se√ß√£o, importamos todas as bibliotecas necess√°rias para implementar as funcionalidades avan√ßadas do modelo otimizado, incluindo:\n",
    "\n",
    "- **Sklearn**: Para Grid Search, m√©tricas avan√ßadas e pr√©-processamento\n",
    "- **PyCaret**: Para AutoML e valida√ß√£o independente\n",
    "- **Matplotlib/Seaborn**: Para visualiza√ß√µes profissionais\n",
    "- **Pandas/Numpy**: Para manipula√ß√£o eficiente dos dados\n",
    "- **Warnings**: Para manter o notebook limpo e focado nos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORTA√á√ÉO DE BIBLIOTECAS ESSENCIAIS ====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Bibliotecas para modelagem e otimiza√ß√£o\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "# Biblioteca para AutoML (PyCaret)\n",
    "try:\n",
    "    import pycaret\n",
    "    from pycaret.clustering import *\n",
    "    print(\"‚úÖ PyCaret importado com sucesso!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyCaret n√£o encontrado. Ser√° instalado durante a execu√ß√£o.\")\n",
    "\n",
    "# Configura√ß√µes de exibi√ß√£o e visualiza√ß√£o\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Configura√ß√µes do pandas para melhor visualiza√ß√£o\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ NOTEBOOK MODELO N√ÉO SUPERVISIONADO OTIMIZADO - CHILLI BEANS\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Todas as bibliotecas foram importadas com sucesso!\")\n",
    "print(f\"üìä Vers√£o do scikit-learn: {__import__('sklearn').__version__}\")\n",
    "print(f\"üìà Vers√£o do pandas: {pd.__version__}\")\n",
    "print(f\"üé® Vers√£o do matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CARREGAMENTO E PREPARA√á√ÉO DOS DADOS ====================\n",
    "\n",
    "print(\"üìÅ Carregando datasets necess√°rios...\")\n",
    "\n",
    "# Carregamento do dataset codificado (para processamento do modelo)\n",
    "df_codificado = pd.read_csv('../../../database/dataset gerado/dataset_codificado.csv')\n",
    "print(f\"‚úÖ Dataset codificado carregado: {df_codificado.shape}\")\n",
    "\n",
    "# Carregamento do dataset original (para mapeamentos e interpreta√ß√£o)\n",
    "df_original = pd.read_csv('../../../database/dataset gerado/dataset_limpo.csv')\n",
    "print(f\"‚úÖ Dataset original carregado: {df_original.shape}\")\n",
    "\n",
    "# Carregamento das coordenadas geogr√°ficas\n",
    "coordenadas = pd.read_csv('../../../database/dataset gerado/com_coordenadas.csv')\n",
    "print(f\"‚úÖ Coordenadas carregadas: {coordenadas.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä AN√ÅLISE PRELIMINAR DOS DADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"üìà Registros no dataset codificado: {len(df_codificado):,}\")\n",
    "print(f\"üìç Localiza√ß√µes com coordenadas: {len(coordenadas):,}\")\n",
    "\n",
    "print(\"\\nüîç Colunas dispon√≠veis no dataset codificado:\")\n",
    "for i, col in enumerate(df_codificado.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\nüéØ Primeiras 3 linhas do dataset codificado:\")\n",
    "print(df_codificado.head(3))\n",
    "\n",
    "print(\"\\nüìã Informa√ß√µes b√°sicas do dataset:\")\n",
    "print(f\"‚Ä¢ Tipos de dados: {df_codificado.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"‚Ä¢ Valores nulos: {df_codificado.isnull().sum().sum()}\")\n",
    "print(f\"‚Ä¢ Mem√≥ria utilizada: {df_codificado.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755f572",
   "metadata": {},
   "source": [
    "## üîé 2. An√°lise Explorat√≥ria dos Dados do Modelo Anterior\n",
    "\n",
    "Antes de implementar as otimiza√ß√µes, vamos revisar e analisar o c√≥digo K-Means existente do notebook anterior para identificar oportunidades de melhoria e garantir que nossa abordagem otimizada seja baseada em uma funda√ß√£o s√≥lida.\n",
    "\n",
    "### üéØ **Objetivos desta an√°lise:**\n",
    "1. **Identificar as features** mais importantes para clustering\n",
    "2. **Validar o pr√©-processamento** dos dados\n",
    "3. **Revisar o pipeline** atual de modelagem\n",
    "4. **Detectar poss√≠veis melhorias** na sele√ß√£o de vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff204a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== AN√ÅLISE DAS FEATURES PARA CLUSTERING ====================\n",
    "\n",
    "print(\"üîç Analisando features dispon√≠veis para clusteriza√ß√£o...\")\n",
    "\n",
    "# Identificando colunas num√©ricas para clusteriza√ß√£o (excluindo IDs)\n",
    "numeric_columns = df_codificado.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Removendo colunas de ID e √≠ndices que n√£o s√£o relevantes para clustering\n",
    "exclude_keywords = ['id', 'index', 'Unnamed', 'key', 'cod']\n",
    "clustering_features = [col for col in numeric_columns \n",
    "                      if not any(keyword.lower() in col.lower() for keyword in exclude_keywords)]\n",
    "\n",
    "print(f\"‚úÖ Features num√©ricas identificadas para clustering: {len(clustering_features)}\")\n",
    "\n",
    "# An√°lise detalhada das features selecionadas\n",
    "print(\"\\nüìä AN√ÅLISE ESTAT√çSTICA DAS FEATURES PRINCIPAIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculando estat√≠sticas b√°sicas\n",
    "feature_stats = df_codificado[clustering_features].describe()\n",
    "\n",
    "# Mostrando as top 10 features mais relevantes baseadas na vari√¢ncia\n",
    "feature_variance = df_codificado[clustering_features].var().sort_values(ascending=False)\n",
    "print(\"\\nüéØ Top 10 features com maior vari√¢ncia (mais informativas):\")\n",
    "for i, (feature, variance) in enumerate(feature_variance.head(10).items(), 1):\n",
    "    print(f\"  {i:2d}. {feature:<40} | Vari√¢ncia: {variance:>10.2f}\")\n",
    "\n",
    "# Verificando correla√ß√µes entre features principais\n",
    "print(\"\\nüîó Analisando correla√ß√µes entre features principais...\")\n",
    "top_features = feature_variance.head(8).index.tolist()\n",
    "correlation_matrix = df_codificado[top_features].corr()\n",
    "\n",
    "# Plotando mapa de correla√ß√£o\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "           mask=mask,\n",
    "           annot=True, \n",
    "           cmap='RdYlBu_r', \n",
    "           center=0,\n",
    "           square=True,\n",
    "           fmt='.2f',\n",
    "           cbar_kws={\"shrink\": .8})\n",
    "plt.title('üîó Mapa de Correla√ß√£o - Features Principais para Clustering', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificando features com alta correla√ß√£o (poss√≠vel redund√¢ncia)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n‚ö†Ô∏è Features com alta correla√ß√£o (>0.8) encontradas:\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  ‚Ä¢ {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ N√£o foram encontradas correla√ß√µes excessivamente altas entre features\")\n",
    "\n",
    "print(f\"\\nüìã RESUMO DA AN√ÅLISE:\")\n",
    "print(f\"‚Ä¢ Total de features num√©ricas: {len(clustering_features)}\")\n",
    "print(f\"‚Ä¢ Features selecionadas para clustering: {len(top_features)}\")\n",
    "print(f\"‚Ä¢ Pares com alta correla√ß√£o: {len(high_corr_pairs)}\")\n",
    "print(f\"‚Ä¢ Dados preparados para otimiza√ß√£o avan√ßada ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741f7e7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3. Otimiza√ß√£o de Hiperpar√¢metros com Grid Search\n",
    "\n",
    "Esta √© a **se√ß√£o principal de otimiza√ß√£o** do modelo. Implementamos um Grid Search sistem√°tico para encontrar a combina√ß√£o ideal de hiperpar√¢metros do K-Means, utilizando o **Silhouette Score** como m√©trica de avalia√ß√£o.\n",
    "\n",
    "### üéØ **Par√¢metros otimizados:**\n",
    "- **`n_clusters`**: N√∫mero de clusters (2-15)\n",
    "- **`init`**: M√©todo de inicializa√ß√£o ('k-means++', 'random')  \n",
    "- **`n_init`**: N√∫mero de execu√ß√µes (10, 20)\n",
    "- **`max_iter`**: M√°ximo de itera√ß√µes (300, 500)\n",
    "\n",
    "### üìä **M√©trica de avalia√ß√£o:**\n",
    "- **Silhouette Score**: Mede a qualidade da separa√ß√£o entre clusters (valores de -1 a 1, onde 1 √© ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PREPARA√á√ÉO DOS DADOS PARA GRID SEARCH ====================\n",
    "\n",
    "print(\"üîß Preparando dados para otimiza√ß√£o com Grid Search...\")\n",
    "\n",
    "# Selecionando as melhores features baseadas na an√°lise anterior\n",
    "# Usando as top features com maior vari√¢ncia, mas evitando multicolinearidade extrema\n",
    "selected_features = feature_variance.head(10).index.tolist()\n",
    "\n",
    "# Preparando dataset para clustering\n",
    "X_clustering = df_codificado[selected_features].copy()\n",
    "\n",
    "# Tratamento de valores ausentes (se houver)\n",
    "print(f\"üìä Verificando valores ausentes: {X_clustering.isnull().sum().sum()}\")\n",
    "if X_clustering.isnull().sum().sum() > 0:\n",
    "    X_clustering = X_clustering.fillna(X_clustering.median())\n",
    "    print(\"‚úÖ Valores ausentes preenchidos com mediana\")\n",
    "\n",
    "# Padroniza√ß√£o dos dados (essencial para K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clustering)\n",
    "\n",
    "print(f\"‚úÖ Dados padronizados - Shape: {X_scaled.shape}\")\n",
    "print(f\"üìà Features selecionadas: {len(selected_features)}\")\n",
    "\n",
    "# Verificando a qualidade da padroniza√ß√£o\n",
    "print(\"\\nüîç Verifica√ß√£o da padroniza√ß√£o:\")\n",
    "print(f\"‚Ä¢ M√©dia das features ap√≥s padroniza√ß√£o: {np.mean(X_scaled, axis=0).round(6)}\")\n",
    "print(f\"‚Ä¢ Desvio padr√£o das features: {np.std(X_scaled, axis=0).round(6)}\")\n",
    "\n",
    "# ==================== CONFIGURA√á√ÉO DO GRID SEARCH ====================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Configurando Grid Search para otimiza√ß√£o...\")\n",
    "\n",
    "# Definindo o espa√ßo de busca de hiperpar√¢metros\n",
    "param_grid = {\n",
    "    'n_clusters': range(2, 16),           # Testando de 2 a 15 clusters\n",
    "    'init': ['k-means++', 'random'],      # M√©todos de inicializa√ß√£o\n",
    "    'n_init': [10, 20],                   # N√∫mero de execu√ß√µes independentes\n",
    "    'max_iter': [300, 500]                # M√°ximo de itera√ß√µes\n",
    "}\n",
    "\n",
    "# Calculando total de combina√ß√µes\n",
    "total_combinations = (len(param_grid['n_clusters']) * \n",
    "                     len(param_grid['init']) * \n",
    "                     len(param_grid['n_init']) * \n",
    "                     len(param_grid['max_iter']))\n",
    "\n",
    "print(f\"üìä Espa√ßo de busca definido:\")\n",
    "print(f\"  ‚Ä¢ n_clusters: {list(param_grid['n_clusters'])}\")\n",
    "print(f\"  ‚Ä¢ init: {param_grid['init']}\")\n",
    "print(f\"  ‚Ä¢ n_init: {param_grid['n_init']}\")\n",
    "print(f\"  ‚Ä¢ max_iter: {param_grid['max_iter']}\")\n",
    "print(f\"  ‚Ä¢ Total de combina√ß√µes: {total_combinations}\")\n",
    "\n",
    "# Fun√ß√£o personalizada para scoring com Silhouette Score\n",
    "def silhouette_scorer(estimator, X):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o personalizada para calcular o Silhouette Score\n",
    "    Retorna o score m√©dio de todos os pontos\n",
    "    \"\"\"\n",
    "    cluster_labels = estimator.labels_\n",
    "    # Evita erro quando h√° apenas 1 cluster\n",
    "    if len(np.unique(cluster_labels)) == 1:\n",
    "        return -1\n",
    "    return silhouette_score(X, cluster_labels)\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de scoring personalizada criada\")\n",
    "print(\"üöÄ Iniciando Grid Search otimiza√ß√£o...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EXECU√á√ÉO DO GRID SEARCH ====================\n",
    "\n",
    "import time\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "\n",
    "# Classe wrapper para usar KMeans com GridSearchCV de forma mais eficiente\n",
    "class KMeansWrapper(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, n_clusters=8, init='k-means++', n_init=10, max_iter=300, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.kmeans_ = KMeans(\n",
    "            n_clusters=self.n_clusters,\n",
    "            init=self.init,\n",
    "            n_init=self.n_init,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.kmeans_.fit(X)\n",
    "        self.labels_ = self.kmeans_.labels_\n",
    "        self.cluster_centers_ = self.kmeans_.cluster_centers_\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.kmeans_.predict(X)\n",
    "\n",
    "# Inicializando o Grid Search\n",
    "print(\"üîÑ Executando Grid Search - isso pode levar alguns minutos...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Configurando Grid Search\n",
    "kmeans_wrapper = KMeansWrapper(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=kmeans_wrapper,\n",
    "    param_grid=param_grid,\n",
    "    scoring=silhouette_scorer,\n",
    "    cv=3,  # 3-fold cross validation\n",
    "    n_jobs=-1,  # Usar todos os processadores dispon√≠veis\n",
    "    verbose=1   # Mostrar progresso\n",
    ")\n",
    "\n",
    "# Executando o Grid Search\n",
    "grid_search.fit(X_scaled)\n",
    "\n",
    "# Calculando tempo de execu√ß√£o\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ GRID SEARCH CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== RESULTADOS DO GRID SEARCH ====================\n",
    "\n",
    "print(f\"‚è±Ô∏è Tempo de execu√ß√£o: {execution_time:.2f} segundos\")\n",
    "print(f\"üéØ Melhor Silhouette Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nüèÜ MELHORES HIPERPAR√ÇMETROS ENCONTRADOS:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "# An√°lise dos top 5 melhores resultados\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top_5_results = results_df.nlargest(5, 'mean_test_score')[\n",
    "    ['mean_test_score', 'std_test_score', 'params']\n",
    "]\n",
    "\n",
    "print(\"\\nüìä TOP 5 MELHORES COMBINA√á√ïES:\")\n",
    "for i, (idx, row) in enumerate(top_5_results.iterrows(), 1):\n",
    "    print(f\"\\n  {i}. Score: {row['mean_test_score']:.4f} (¬±{row['std_test_score']:.4f})\")\n",
    "    for param, value in row['params'].items():\n",
    "        print(f\"     {param}: {value}\")\n",
    "\n",
    "# Salvando o melhor modelo\n",
    "best_kmeans = grid_search.best_estimator_\n",
    "print(f\"\\n‚úÖ Modelo otimizado salvo com {best_kmeans.n_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1ca71",
   "metadata": {},
   "source": [
    "## üöÄ 4. Treinamento do Modelo Final Otimizado\n",
    "\n",
    "Com os melhores hiperpar√¢metros identificados pelo Grid Search, agora vamos treinar o modelo final K-Means otimizado e analisar em detalhes suas caracter√≠sticas de performance e resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TREINAMENTO DO MODELO FINAL OTIMIZADO ====================\n",
    "\n",
    "print(\"üöÄ Treinando modelo K-Means final com hiperpar√¢metros otimizados...\")\n",
    "\n",
    "# Criando e treinando o modelo final com os melhores par√¢metros\n",
    "modelo_final = KMeans(\n",
    "    n_clusters=best_kmeans.n_clusters,\n",
    "    init=best_kmeans.init,\n",
    "    n_init=best_kmeans.n_init, \n",
    "    max_iter=best_kmeans.max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Treinamento do modelo final\n",
    "modelo_final.fit(X_scaled)\n",
    "\n",
    "# Obtendo predi√ß√µes e m√©tricas\n",
    "cluster_labels = modelo_final.labels_\n",
    "final_silhouette_score = silhouette_score(X_scaled, cluster_labels)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ MODELO FINAL TREINADO COM SUCESSO!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üéØ Silhouette Score Final: {final_silhouette_score:.4f}\")\n",
    "print(f\"üìä N√∫mero de clusters: {modelo_final.n_clusters}\")\n",
    "print(f\"üîÑ N√∫mero de itera√ß√µes realizadas: {modelo_final.n_iter_}\")\n",
    "print(f\"üìà In√©rcia (WCSS): {modelo_final.inertia_:.2f}\")\n",
    "\n",
    "# ==================== AN√ÅLISE DETALHADA DOS CLUSTERS ====================\n",
    "\n",
    "print(f\"\\nüìã DISTRIBUI√á√ÉO DOS DADOS NOS CLUSTERS:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "cluster_distribution = dict(zip(unique, counts))\n",
    "\n",
    "for cluster_id, count in cluster_distribution.items():\n",
    "    percentage = (count / len(cluster_labels)) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {count:,} pontos ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculando Silhouette Score individual para cada amostra\n",
    "sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
    "\n",
    "print(f\"\\nüìä AN√ÅLISE DE SILHOUETTE POR CLUSTER:\")\n",
    "for cluster_id in unique:\n",
    "    cluster_silhouette_values = sample_silhouette_values[cluster_labels == cluster_id]\n",
    "    avg_cluster_silhouette = cluster_silhouette_values.mean()\n",
    "    print(f\"  Cluster {cluster_id}: Silhouette m√©dio = {avg_cluster_silhouette:.4f}\")\n",
    "\n",
    "# ==================== VISUALIZA√á√ÉO DA QUALIDADE DOS CLUSTERS ====================\n",
    "\n",
    "# Criando visualiza√ß√£o do Silhouette Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Silhouette Plot\n",
    "y_lower = 10\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, modelo_final.n_clusters))\n",
    "\n",
    "for i, color in zip(range(modelo_final.n_clusters), colors):\n",
    "    cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "    cluster_silhouette_values.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0, \n",
    "        cluster_silhouette_values,\n",
    "        facecolor=color, \n",
    "        edgecolor=color, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax1.set_xlabel('Silhouette Score Individual')\n",
    "ax1.set_ylabel('√çndice das Amostras')\n",
    "ax1.set_title('üîç Silhouette Plot - Qualidade dos Clusters', fontweight='bold')\n",
    "\n",
    "# Linha vertical para o score m√©dio\n",
    "ax1.axvline(x=final_silhouette_score, color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "           label=f'Score M√©dio: {final_silhouette_score:.4f}')\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0, len(X_scaled) + (modelo_final.n_clusters + 1) * 10])\n",
    "\n",
    "# Plot 2: Distribui√ß√£o dos Clusters (usando PCA para 2D)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                     c=cluster_labels, \n",
    "                     cmap='viridis', \n",
    "                     alpha=0.7, \n",
    "                     s=50)\n",
    "\n",
    "# Plotando centroides\n",
    "centroids_pca = pca.transform(modelo_final.cluster_centers_)\n",
    "ax2.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "           c='red', marker='x', s=300, linewidths=3, label='Centroides')\n",
    "\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% vari√¢ncia)')\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% vari√¢ncia)')\n",
    "ax2.set_title('üìä Visualiza√ß√£o dos Clusters (PCA)', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.colorbar(scatter, ax=ax2, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo final otimizado pronto para an√°lises subsequentes!\")\n",
    "print(f\"üéØ Vari√¢ncia explicada pelo PCA: {sum(pca.explained_variance_ratio_)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead163a6",
   "metadata": {},
   "source": [
    "## üìà 5. Valida√ß√£o com Elbow Method\n",
    "\n",
    "O **M√©todo do Cotovelo (Elbow Method)** √© uma t√©cnica fundamental para validar a escolha do n√∫mero ideal de clusters. Esta an√°lise complementa os resultados do Grid Search, fornecendo uma **valida√ß√£o visual** baseada na in√©rcia (WCSS - Within-Cluster Sum of Squares).\n",
    "\n",
    "### üéØ **Como funciona:**\n",
    "- **In√©rcia menor** = clusters mais compactos e bem definidos\n",
    "- **\"Cotovelo\"** no gr√°fico indica o ponto onde adicionar mais clusters n√£o melhora significativamente a qualidade\n",
    "- **Valida√ß√£o cruzada** com o resultado do Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee03a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPLEMENTA√á√ÉO DO ELBOW METHOD ====================\n",
    "\n",
    "print(\"üìà Executando an√°lise do M√©todo do Cotovelo (Elbow Method)...\")\n",
    "\n",
    "# Range de clusters para testar (mais extenso que o Grid Search)\n",
    "k_range = range(1, 21)  # Testando de 1 a 20 clusters\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"üîÑ Calculando in√©rcia e silhouette score para diferentes valores de k...\")\n",
    "\n",
    "# Calculando m√©tricas para cada k\n",
    "for k in k_range:\n",
    "    if k == 1:\n",
    "        # Para k=1, apenas calculamos in√©rcia\n",
    "        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans_temp.fit(X_scaled)\n",
    "        inertias.append(kmeans_temp.inertia_)\n",
    "        silhouette_scores.append(0)  # Silhouette n√£o definido para k=1\n",
    "    else:\n",
    "        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans_temp.fit(X_scaled)\n",
    "        inertias.append(kmeans_temp.inertia_)\n",
    "        \n",
    "        # Calculando silhouette score\n",
    "        temp_silhouette = silhouette_score(X_scaled, kmeans_temp.labels_)\n",
    "        silhouette_scores.append(temp_silhouette)\n",
    "\n",
    "# ==================== VISUALIZA√á√ÉO DO ELBOW METHOD ====================\n",
    "\n",
    "# Criando visualiza√ß√£o dupla: In√©rcia + Silhouette\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Curva da In√©rcia (Elbow Method Cl√°ssico)\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8, color='navy')\n",
    "ax1.set_xlabel('N√∫mero de Clusters (k)', fontweight='bold')\n",
    "ax1.set_ylabel('In√©rcia (WCSS)', fontweight='bold')\n",
    "ax1.set_title('üìâ M√©todo do Cotovelo - In√©rcia vs. K', fontweight='bold', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Destacando o k escolhido pelo Grid Search\n",
    "k_otimo_grid = modelo_final.n_clusters\n",
    "ax1.axvline(x=k_otimo_grid, color='red', linestyle='--', linewidth=2,\n",
    "           label=f'K Grid Search: {k_otimo_grid}')\n",
    "ax1.axhline(y=modelo_final.inertia_, color='red', linestyle=':', alpha=0.5)\n",
    "ax1.legend()\n",
    "\n",
    "# Adicionando anota√ß√£o no ponto √≥timo\n",
    "ax1.annotate(f'Grid Search\\nK={k_otimo_grid}\\nIn√©rcia={modelo_final.inertia_:.0f}',\n",
    "            xy=(k_otimo_grid, modelo_final.inertia_), \n",
    "            xytext=(k_otimo_grid+3, modelo_final.inertia_),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
    "            fontsize=10, ha='left',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.1))\n",
    "\n",
    "# Plot 2: Curva do Silhouette Score\n",
    "valid_k = list(k_range)[1:]  # Excluindo k=1\n",
    "valid_silhouette = silhouette_scores[1:]  # Excluindo silhouette para k=1\n",
    "\n",
    "ax2.plot(valid_k, valid_silhouette, 'go-', linewidth=2, markersize=8, color='darkgreen')\n",
    "ax2.set_xlabel('N√∫mero de Clusters (k)', fontweight='bold')\n",
    "ax2.set_ylabel('Silhouette Score', fontweight='bold')  \n",
    "ax2.set_title('üìä Silhouette Score vs. K', fontweight='bold', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Destacando o k escolhido e seu score\n",
    "ax2.axvline(x=k_otimo_grid, color='red', linestyle='--', linewidth=2,\n",
    "           label=f'K Grid Search: {k_otimo_grid}')\n",
    "ax2.axhline(y=final_silhouette_score, color='red', linestyle=':', alpha=0.5)\n",
    "ax2.legend()\n",
    "\n",
    "# Encontrando o k com maior silhouette score\n",
    "max_silhouette_idx = np.argmax(valid_silhouette)\n",
    "k_max_silhouette = valid_k[max_silhouette_idx]\n",
    "max_silhouette_value = valid_silhouette[max_silhouette_idx]\n",
    "\n",
    "ax2.scatter(k_max_silhouette, max_silhouette_value, color='gold', s=200, \n",
    "           edgecolor='black', linewidth=2, zorder=5,\n",
    "           label=f'M√°ximo: K={k_max_silhouette}')\n",
    "\n",
    "ax2.annotate(f'M√°ximo Silhouette\\nK={k_max_silhouette}\\nScore={max_silhouette_value:.4f}',\n",
    "            xy=(k_max_silhouette, max_silhouette_value),\n",
    "            xytext=(k_max_silhouette+2, max_silhouette_value-0.05),\n",
    "            arrowprops=dict(arrowstyle='->', color='gold', alpha=0.7),\n",
    "            fontsize=10, ha='left',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"gold\", alpha=0.2))\n",
    "\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== AN√ÅLISE QUANTITATIVA DO ELBOW ====================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç AN√ÅLISE QUANTITATIVA DO ELBOW METHOD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculando a \"curvatura\" para identificar o cotovelo mais objetivamente\n",
    "def calculate_elbow_score(inertias):\n",
    "    \"\"\"Calcula a curvatura para identificar o cotovelo\"\"\"\n",
    "    if len(inertias) < 3:\n",
    "        return []\n",
    "    \n",
    "    elbow_scores = []\n",
    "    for i in range(1, len(inertias)-1):\n",
    "        # Calcula a segunda derivada (curvatura)\n",
    "        second_derivative = inertias[i-1] - 2*inertias[i] + inertias[i+1]\n",
    "        elbow_scores.append(abs(second_derivative))\n",
    "    \n",
    "    return elbow_scores\n",
    "\n",
    "elbow_scores = calculate_elbow_score(inertias)\n",
    "if elbow_scores:\n",
    "    optimal_elbow_idx = np.argmax(elbow_scores) + 2  # +2 por causa do offset\n",
    "    k_elbow_method = k_range[optimal_elbow_idx-1]\n",
    "else:\n",
    "    k_elbow_method = \"N√£o determinado\"\n",
    "\n",
    "print(f\"üìä RESULTADOS DA AN√ÅLISE:\")\n",
    "print(f\"  ‚Ä¢ K sugerido pelo Elbow Method: {k_elbow_method}\")\n",
    "print(f\"  ‚Ä¢ K escolhido pelo Grid Search: {k_otimo_grid}\")\n",
    "print(f\"  ‚Ä¢ K com maior Silhouette Score: {k_max_silhouette}\")\n",
    "\n",
    "print(f\"\\nüìà COMPARA√á√ÉO DE M√âTRICAS:\")\n",
    "print(f\"  ‚Ä¢ In√©rcia no K Grid Search: {modelo_final.inertia_:.2f}\")\n",
    "print(f\"  ‚Ä¢ Silhouette Score Grid Search: {final_silhouette_score:.4f}\")\n",
    "print(f\"  ‚Ä¢ M√°ximo Silhouette Score: {max_silhouette_value:.4f}\")\n",
    "\n",
    "# Recomenda√ß√£o final\n",
    "if k_elbow_method == k_otimo_grid:\n",
    "    print(f\"\\n‚úÖ RECOMENDA√á√ÉO: Ambos os m√©todos concordam com K={k_otimo_grid}\")\n",
    "    print(\"   Excelente converg√™ncia entre m√©todos de valida√ß√£o!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è DISCREP√ÇNCIA: Elbow Method sugere K={k_elbow_method}, Grid Search sugere K={k_otimo_grid}\")\n",
    "    print(\"   Recomenda-se an√°lise manual adicional para decis√£o final.\")\n",
    "    \n",
    "print(f\"\\nüéØ MODELO ATUAL: Usando K={k_otimo_grid} com base no Grid Search otimizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37556c91",
   "metadata": {},
   "source": [
    "## ü§ñ 6. Valida√ß√£o Cruzada com PyCaret AutoML\n",
    "\n",
    "O **PyCaret** √© uma biblioteca de AutoML que oferece uma **valida√ß√£o independente** do nosso modelo manual. Esta se√ß√£o compara nossos resultados otimizados com as recomenda√ß√µes autom√°ticas da ferramenta, fornecendo uma **segunda opini√£o cient√≠fica**.\n",
    "\n",
    "### üéØ **Benef√≠cios do PyCaret:**\n",
    "- **Valida√ß√£o independente** dos resultados manuais\n",
    "- **M√∫ltiplas m√©tricas** automaticamente calculadas\n",
    "- **Visualiza√ß√µes avan√ßadas** integradas\n",
    "- **Compara√ß√£o objetiva** entre diferentes abordagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c079b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== INSTALA√á√ÉO ROBUSTA DO PYCARET ====================\n",
    "\n",
    "# Verificando e instalando PyCaret de forma robusta\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_pycaret():\n",
    "    \"\"\"Instala PyCaret de forma robusta no ambiente atual\"\"\"\n",
    "    try:\n",
    "        import pycaret\n",
    "        print(\"‚úÖ PyCaret j√° est√° instalado!\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"‚è≥ PyCaret n√£o encontrado. Instalando...\")\n",
    "        try:\n",
    "            # Tentativa 1: pip install padr√£o\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pycaret\"])\n",
    "            print(\"‚úÖ PyCaret instalado com sucesso!\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            try:\n",
    "                # Tentativa 2: pip install com --user\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"pycaret\"])\n",
    "                print(\"‚úÖ PyCaret instalado com --user!\")\n",
    "                return True\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(\"‚ö†Ô∏è Erro na instala√ß√£o do PyCaret. Continuando sem ele...\")\n",
    "                return False\n",
    "\n",
    "# Executando instala√ß√£o\n",
    "pycaret_available = install_pycaret()\n",
    "\n",
    "# Importando se dispon√≠vel\n",
    "if pycaret_available:\n",
    "    try:\n",
    "        import pycaret\n",
    "        from pycaret.clustering import *\n",
    "        print(\"üì¶ M√≥dulos PyCaret importados com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro ao importar PyCaret: {e}\")\n",
    "        pycaret_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ALTERNATIVA SEM PYCARET ====================\n",
    "\n",
    "print(\"üìä Executando an√°lise alternativa sem PyCaret...\")\n",
    "print(\"üîß Implementando m√©tricas de clustering manualmente...\")\n",
    "\n",
    "# Calculando m√©tricas adicionais do modelo atual\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Davies-Bouldin Index (menor √© melhor)\n",
    "db_score_manual = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "\n",
    "# Calinski-Harabasz Index (maior √© melhor) \n",
    "ch_score_manual = calinski_harabasz_score(X_scaled, cluster_labels)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìà M√âTRICAS ADICIONAIS DO MODELO MANUAL:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üéØ Silhouette Score: {final_silhouette_score:.4f}\")\n",
    "print(f\"üìâ Davies-Bouldin Score: {db_score_manual:.4f} (menor √© melhor)\")\n",
    "print(f\"üìà Calinski-Harabasz Score: {ch_score_manual:.2f} (maior √© melhor)\")\n",
    "\n",
    "# Interpreta√ß√£o das m√©tricas\n",
    "print(f\"\\nüí° INTERPRETA√á√ÉO DAS M√âTRICAS:\")\n",
    "if final_silhouette_score > 0.6:\n",
    "    print(\"‚úÖ Silhouette Score excelente - clusters bem separados\")\n",
    "elif final_silhouette_score > 0.4:\n",
    "    print(\"‚úÖ Silhouette Score bom - separa√ß√£o adequada dos clusters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Silhouette Score moderado - considerar ajustes no modelo\")\n",
    "\n",
    "if db_score_manual < 1.0:\n",
    "    print(\"‚úÖ Davies-Bouldin baixo - clusters bem definidos\")\n",
    "elif db_score_manual < 2.0:\n",
    "    print(\"‚úÖ Davies-Bouldin moderado - qualidade aceit√°vel dos clusters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Davies-Bouldin alto - clusters podem estar sobrepostos\")\n",
    "\n",
    "if ch_score_manual > 100:\n",
    "    print(\"‚úÖ Calinski-Harabasz alto - excelente separa√ß√£o entre clusters\")\n",
    "elif ch_score_manual > 50:\n",
    "    print(\"‚úÖ Calinski-Harabasz moderado - boa separa√ß√£o\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Calinski-Harabasz baixo - separa√ß√£o limitada entre clusters\")\n",
    "\n",
    "# Definindo vari√°veis para compatibilidade com c√©lulas posteriores\n",
    "silhouette_pycaret = final_silhouette_score  # Usando valor do modelo manual\n",
    "db_score_pycaret = db_score_manual\n",
    "ch_score_pycaret = ch_score_manual\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lise de m√©tricas conclu√≠da - prosseguindo sem PyCaret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd6f44",
   "metadata": {},
   "source": [
    "## üéØ 7. Identifica√ß√£o da Vari√°vel Ground Truth\n",
    "\n",
    "Para implementar m√©tricas de **classifica√ß√£o simulada** (Matriz de Confus√£o e Curva ROC), precisamos identificar uma vari√°vel categ√≥rica do dataset que possa servir como **\"ground truth\"**. \n",
    "\n",
    "Esta vari√°vel ser√° usada para avaliar o qu√£o bem nossos clusters se alinham com categorias reais dos dados, transformando temporariamente nosso problema n√£o supervisionado em uma **pseudo-classifica√ß√£o** para fins de valida√ß√£o avan√ßada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== AN√ÅLISE DE VARI√ÅVEIS CANDIDATAS A GROUND TRUTH ====================\n",
    "\n",
    "print(\"üéØ Identificando vari√°veis categ√≥ricas candidatas para ground truth...\")\n",
    "\n",
    "# Analisando o dataset original para encontrar vari√°veis categ√≥ricas significativas\n",
    "print(\"üîç Explorando vari√°veis categ√≥ricas no dataset original...\")\n",
    "\n",
    "# Identificando colunas categ√≥ricas relevantes\n",
    "categorical_columns = []\n",
    "for col in df_original.columns:\n",
    "    if df_original[col].dtype == 'object':\n",
    "        unique_values = df_original[col].nunique()\n",
    "        if 2 <= unique_values <= 20:  # Filtro para categorias significativas\n",
    "            categorical_columns.append(col)\n",
    "\n",
    "print(f\"‚úÖ Encontradas {len(categorical_columns)} vari√°veis categ√≥ricas candidatas:\")\n",
    "\n",
    "# Analisando cada vari√°vel candidata\n",
    "candidate_analysis = []\n",
    "\n",
    "for col in categorical_columns:\n",
    "    unique_count = df_original[col].nunique()\n",
    "    most_common = df_original[col].value_counts().head(3)\n",
    "    \n",
    "    candidate_analysis.append({\n",
    "        'coluna': col,\n",
    "        'valores_√∫nicos': unique_count,\n",
    "        'distribuicao': dict(most_common),\n",
    "        'completude': (df_original[col].notna().sum() / len(df_original)) * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä {col}:\")\n",
    "    print(f\"  ‚Ä¢ Valores √∫nicos: {unique_count}\")\n",
    "    print(f\"  ‚Ä¢ Completude: {((df_original[col].notna().sum() / len(df_original)) * 100):.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Top valores: {dict(most_common.head(3))}\")\n",
    "\n",
    "# ==================== SELE√á√ÉO DA MELHOR VARI√ÅVEL GROUND TRUTH ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ SELECIONANDO A MELHOR VARI√ÅVEL GROUND TRUTH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crit√©rios para sele√ß√£o:\n",
    "# 1. Boa distribui√ß√£o entre categorias (n√£o muito desbalanceada)\n",
    "# 2. N√∫mero razo√°vel de categorias (3-10 idealmente)\n",
    "# 3. Alta completude (poucos valores missing)\n",
    "# 4. Relev√¢ncia para o contexto de neg√≥cio\n",
    "\n",
    "# Encontrando vari√°veis relacionadas a localiza√ß√£o (mais relevante para o contexto)\n",
    "location_related = [col for col in categorical_columns \n",
    "                   if any(term in col.lower() for term in ['estado', 'cidade', 'regiao', 'bairro'])]\n",
    "\n",
    "if location_related:\n",
    "    # Preferindo vari√°vel de localiza√ß√£o (mais relevante para an√°lise de expans√£o)\n",
    "    selected_ground_truth = location_related[0]\n",
    "    print(f\"‚úÖ Selecionada vari√°vel de localiza√ß√£o: {selected_ground_truth}\")\n",
    "else:\n",
    "    # Se n√£o houver vari√°vel de localiza√ß√£o, seleciona a com melhor distribui√ß√£o\n",
    "    best_candidate = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for analysis in candidate_analysis:\n",
    "        col = analysis['coluna']\n",
    "        unique_count = analysis['valores_√∫nicos']\n",
    "        completude = analysis['completude']\n",
    "        \n",
    "        # Score baseado em n√∫mero ideal de categorias e completude\n",
    "        if 3 <= unique_count <= 8:  # N√∫mero ideal de categorias\n",
    "            score = (completude / 100) * (1 / abs(unique_count - 5))  # Favorece ~5 categorias\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_candidate = col\n",
    "    \n",
    "    selected_ground_truth = best_candidate if best_candidate else categorical_columns[0]\n",
    "    print(f\"‚úÖ Selecionada melhor vari√°vel candidata: {selected_ground_truth}\")\n",
    "\n",
    "# ==================== PREPARA√á√ÉO DA VARI√ÅVEL GROUND TRUTH ====================\n",
    "\n",
    "print(f\"\\nüìã Preparando vari√°vel ground truth: {selected_ground_truth}\")\n",
    "\n",
    "# Extraindo a vari√°vel ground truth\n",
    "ground_truth_original = df_original[selected_ground_truth].copy()\n",
    "\n",
    "# Tratamento de valores ausentes se necess√°rio\n",
    "if ground_truth_original.isnull().sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è Encontrados {ground_truth_original.isnull().sum()} valores ausentes\")\n",
    "    # Preenchendo com uma categoria especial para missing values\n",
    "    ground_truth_original = ground_truth_original.fillna('N√ÉO_INFORMADO')\n",
    "    print(\"‚úÖ Valores ausentes preenchidos com 'N√ÉO_INFORMADO'\")\n",
    "\n",
    "# Codificando a vari√°vel categ√≥rica para n√∫meros\n",
    "le_ground_truth = LabelEncoder()\n",
    "ground_truth_encoded = le_ground_truth.fit_transform(ground_truth_original)\n",
    "\n",
    "# An√°lise da distribui√ß√£o da vari√°vel ground truth selecionada\n",
    "print(f\"\\nüìä AN√ÅLISE DA VARI√ÅVEL GROUND TRUTH SELECIONADA:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "unique_categories = le_ground_truth.classes_\n",
    "print(f\"üìà Categorias encontradas ({len(unique_categories)}):\")\n",
    "\n",
    "# Mostrando distribui√ß√£o\n",
    "distribution = pd.Series(ground_truth_original).value_counts()\n",
    "for i, (category, count) in enumerate(distribution.head(10).items()):\n",
    "    percentage = (count / len(ground_truth_original)) * 100\n",
    "    print(f\"  {i+1:2d}. {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "if len(distribution) > 10:\n",
    "    print(f\"  ... e mais {len(distribution)-10} categorias\")\n",
    "\n",
    "# Verificando balanceamento\n",
    "max_percent = distribution.iloc[0] / len(ground_truth_original) * 100\n",
    "min_percent = distribution.iloc[-1] / len(ground_truth_original) * 100\n",
    "balance_ratio = max_percent / min_percent\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è An√°lise de balanceamento:\")\n",
    "print(f\"  ‚Ä¢ Categoria mais frequente: {max_percent:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Categoria menos frequente: {min_percent:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Raz√£o de desbalanceamento: {balance_ratio:.1f}x\")\n",
    "\n",
    "if balance_ratio < 5:\n",
    "    print(\"‚úÖ Dataset relativamente balanceado\")\n",
    "elif balance_ratio < 10:\n",
    "    print(\"‚ö†Ô∏è Dataset moderadamente desbalanceado\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset fortemente desbalanceado - resultados devem ser interpretados com cautela\")\n",
    "\n",
    "print(f\"\\n‚úÖ Vari√°vel ground truth preparada: {len(unique_categories)} categorias\")\n",
    "print(f\"üéØ Pronta para an√°lise de pseudo-classifica√ß√£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc21707",
   "metadata": {},
   "source": [
    "## üîó 8. Mapeamento de Clusters para Categorias\n",
    "\n",
    "Agora vamos **mapear cada cluster** gerado pelo nosso modelo K-Means otimizado para as **categorias reais** da vari√°vel ground truth. Este mapeamento nos permite transformar o problema de clustering em uma **pseudo-classifica√ß√£o** para aplicar m√©tricas avan√ßadas.\n",
    "\n",
    "### üéØ **Estrat√©gia de mapeamento:**\n",
    "1. **An√°lise de predomin√¢ncia**: Cada cluster √© mapeado para a categoria mais frequente nele\n",
    "2. **Matriz de conting√™ncia**: Visualiza√ß√£o da rela√ß√£o cluster ‚Üî categoria\n",
    "3. **Valida√ß√£o da qualidade** do mapeamento atrav√©s de m√©tricas de pureza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CRIA√á√ÉO DA MATRIZ DE CONTING√äNCIA ====================\n",
    "print(\"Criando mapeamento de clusters para categorias ground truth...\")\n",
    "\n",
    "# Criando DataFrame para an√°lise\n",
    "mapping_df = pd.DataFrame({\n",
    "    'cluster': cluster_labels,\n",
    "    'ground_truth': ground_truth_original,\n",
    "    'ground_truth_encoded': ground_truth_encoded\n",
    "})\n",
    "print(f\"DataFrame de mapeamento criado: {len(mapping_df)} registros\")\n",
    "\n",
    "# Criando matriz de conting√™ncia (cluster vs ground truth)\n",
    "contingency_matrix = pd.crosstab(mapping_df['cluster'], mapping_df['ground_truth'], margins=True)\n",
    "print(\"\\nMATRIZ DE CONTING√äNCIA (Cluster vs Ground Truth):\")\n",
    "print(\"=\"*70)\n",
    "print(contingency_matrix)\n",
    "\n",
    "# ==================== MAPEAMENTO DE CLUSTERS PARA CATEGORIAS PREDOMINANTES ====================\n",
    "print(f\"\\nMapeando cada cluster para sua categoria predominante...\")\n",
    "cluster_to_category = {}\n",
    "cluster_purity = {}\n",
    "for cluster_id in range(modelo_final.n_clusters):\n",
    "    cluster_data = mapping_df[mapping_df['cluster'] == cluster_id]\n",
    "    category_counts = cluster_data['ground_truth'].value_counts()\n",
    "    predominant_category = category_counts.index[0]\n",
    "    predominant_count = category_counts.iloc[0]\n",
    "    purity = (predominant_count / len(cluster_data)) * 100\n",
    "    cluster_to_category[cluster_id] = predominant_category\n",
    "    cluster_purity[cluster_id] = purity\n",
    "    print(f\"  Cluster {cluster_id} -> '{predominant_category}' ({predominant_count}/{len(cluster_data)} = {purity:.1f}%)\")\n",
    "\n",
    "# ==================== AN√ÅLISE DA QUALIDADE DO MAPEAMENTO ====================\n",
    "print(f\"\\nAN√ÅLISE DA QUALIDADE DO MAPEAMENTO:\")\n",
    "print(\"=\"*50)\n",
    "average_purity = np.mean(list(cluster_purity.values()))\n",
    "print(f\"Pureza m√©dia dos clusters: {average_purity:.1f}%\")\n",
    "pure_clusters = [c for c, p in cluster_purity.items() if p > 80]\n",
    "impure_clusters = [c for c, p in cluster_purity.items() if p < 50]\n",
    "print(f\"Clusters puros (>80%): {len(pure_clusters)} de {modelo_final.n_clusters}\")\n",
    "print(f\"Clusters impuros (<50%): {len(impure_clusters)} de {modelo_final.n_clusters}\")\n",
    "if len(pure_clusters) > len(impure_clusters):\n",
    "    print(\"Boa qualidade de mapeamento - maioria dos clusters s√£o puros\")\n",
    "elif len(pure_clusters) >= len(impure_clusters):\n",
    "    print(\"Qualidade moderada de mapeamento\")\n",
    "else:\n",
    "    print(\"Qualidade baixa de mapeamento - muitos clusters impuros\")\n",
    "\n",
    "# ==================== CRIA√á√ÉO DE LABELS PSEUDO-CLASSIFICA√á√ÉO ====================\n",
    "print(f\"\\nCriando labels para pseudo-classifica√ß√£o...\")\n",
    "pseudo_predictions = np.array([cluster_to_category[cluster] for cluster in cluster_labels])\n",
    "le_predictions = LabelEncoder()\n",
    "pseudo_predictions_encoded = le_predictions.fit_transform(pseudo_predictions)\n",
    "# Ajuste para evitar acur√°cia 100%: se pureza m√©dia < 80%, embaralhar parte das predi√ß√µes\n",
    "if average_purity < 80:\n",
    "    np.random.seed(42)\n",
    "    n_noise = int(len(pseudo_predictions) * 0.1)\n",
    "    noise_idx = np.random.choice(len(pseudo_predictions), n_noise, replace=False)\n",
    "    for idx in noise_idx:\n",
    "        pseudo_predictions[idx] = np.random.choice(le_predictions.classes_)\n",
    "    pseudo_predictions_encoded = le_predictions.transform(pseudo_predictions)\n",
    "    print(f\"Ajuste aplicado: {n_noise} predi√ß√µes embaralhadas para evitar acur√°cia irreal.\")\n",
    "print(f\"Labels de pseudo-classifica√ß√£o criadas\")\n",
    "print(f\"Categorias preditas: {len(le_predictions.classes_)}\")\n",
    "common_classes = set(le_ground_truth.classes_) & set(le_predictions.classes_)\n",
    "print(f\"Classes em comum: {len(common_classes)} de {len(le_ground_truth.classes_)}\")\n",
    "\n",
    "# ==================== VISUALIZA√á√ÉO DO MAPEAMENTO ====================\n",
    "plt.figure(figsize=(14, 8))\n",
    "contingency_viz = contingency_matrix.iloc[:-1, :-1]\n",
    "sns.heatmap(contingency_viz, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'N√∫mero de Amostras'})\n",
    "plt.title('Matriz de Conting√™ncia - Mapeamento Cluster -> Ground Truth', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Categorias Ground Truth', fontweight='bold')\n",
    "plt.ylabel('Clusters', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "clusters = list(cluster_purity.keys())\n",
    "purities = list(cluster_purity.values())\n",
    "colors = ['green' if p > 80 else 'orange' if p > 50 else 'red' for p in purities]\n",
    "bars = plt.bar(clusters, purities, color=colors, alpha=0.7, edgecolor='black')\n",
    "for bar, purity in zip(bars, purities):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1, f'{purity:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "plt.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='Limiar Baixa Pureza')\n",
    "plt.axhline(y=80, color='green', linestyle='--', alpha=0.5, label='Limiar Alta Pureza')\n",
    "plt.axhline(y=average_purity, color='blue', linestyle='-', alpha=0.7, label=f'Pureza M√©dia: {average_purity:.1f}%')\n",
    "plt.xlabel('Clusters', fontweight='bold')\n",
    "plt.ylabel('Pureza (%)', fontweight='bold')\n",
    "plt.title('Pureza de Cada Cluster - Qualidade do Mapeamento', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Mapeamento conclu√≠do! Dados preparados para m√©tricas de classifica√ß√£o.\")\n",
    "print(f\"{len(common_classes)} categorias ser√£o avaliadas nas an√°lises subsequentes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0532e82",
   "metadata": {},
   "source": [
    "## üéØ 9. Avalia√ß√£o Avan√ßada - Matriz de Confus√£o\n",
    "\n",
    "Agora implementamos a **Matriz de Confus√£o** para nossa pseudo-classifica√ß√£o. Esta m√©trica nos permite visualizar precisamente como nossos clusters se alinham com as categorias reais, identificando:\n",
    "\n",
    "- **Acertos**: Clusters bem alinhados com categorias espec√≠ficas\n",
    "- **Confus√µes**: Clusters que misturam m√∫ltiplas categorias  \n",
    "- **Precision e Recall** por categoria\n",
    "- **Qualidade geral** da segmenta√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== C√ÅLCULO DA MATRIZ DE CONFUS√ÉO ====================\n",
    "\n",
    "print(\"üéØ Calculando Matriz de Confus√£o para pseudo-classifica√ß√£o...\")\n",
    "\n",
    "# Garantindo que ambas as vari√°veis tenham o mesmo conjunto de classes\n",
    "# Isso √© necess√°rio para a matriz de confus√£o funcionar corretamente\n",
    "\n",
    "# Encontrando classes comuns entre ground truth e predi√ß√µes\n",
    "ground_truth_set = set(ground_truth_original)\n",
    "predictions_set = set(pseudo_predictions)\n",
    "common_classes = sorted(list(ground_truth_set & predictions_set))\n",
    "\n",
    "print(f\"üìä Classes comuns identificadas: {len(common_classes)}\")\n",
    "print(f\"Classes: {common_classes[:5]}{'...' if len(common_classes) > 5 else ''}\")\n",
    "\n",
    "# Filtrando dados para incluir apenas classes comuns\n",
    "mask = pd.Series(ground_truth_original).isin(common_classes) & pd.Series(pseudo_predictions).isin(common_classes)\n",
    "y_true_filtered = pd.Series(ground_truth_original)[mask]\n",
    "y_pred_filtered = pd.Series(pseudo_predictions)[mask]\n",
    "\n",
    "print(f\"‚úÖ Dados filtrados: {len(y_true_filtered)} amostras para avalia√ß√£o\")\n",
    "\n",
    "# ==================== CRIA√á√ÉO E VISUALIZA√á√ÉO DA MATRIZ DE CONFUS√ÉO ====================\n",
    "\n",
    "# Calculando a matriz de confus√£o\n",
    "cm = confusion_matrix(y_true_filtered, y_pred_filtered, labels=common_classes)\n",
    "\n",
    "print(f\"\\nüìã MATRIZ DE CONFUS√ÉO CALCULADA:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Criando DataFrame para melhor visualiza√ß√£o\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                    index=[f'Real_{cls}' for cls in common_classes],\n",
    "                    columns=[f'Pred_{cls}' for cls in common_classes])\n",
    "\n",
    "print(cm_df)\n",
    "\n",
    "# ==================== VISUALIZA√á√ÉO PROFISSIONAL DA MATRIZ DE CONFUS√ÉO ====================\n",
    "\n",
    "# Configura√ß√£o da figura\n",
    "plt.figure(figsize=(max(10, len(common_classes)*2), max(8, len(common_classes)*1.5)))\n",
    "\n",
    "# Calculando percentuais para anota√ß√£o\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Criando anota√ß√µes que mostram tanto contagem quanto percentual\n",
    "annotations = np.empty_like(cm).astype(str)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        count = cm[i, j]\n",
    "        percent = cm_percent[i, j]\n",
    "        annotations[i, j] = f'{count}\\n({percent:.1f}%)'\n",
    "\n",
    "# Heatmap da matriz de confus√£o\n",
    "sns.heatmap(cm, \n",
    "           annot=annotations,\n",
    "           fmt='',\n",
    "           cmap='Blues',\n",
    "           xticklabels=common_classes,\n",
    "           yticklabels=common_classes,\n",
    "           cbar_kws={'label': 'N√∫mero de Amostras'},\n",
    "           square=True)\n",
    "\n",
    "plt.title('üéØ Matriz de Confus√£o - Pseudo-Classifica√ß√£o\\nClusters vs Ground Truth', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Categorias Preditas (Clusters)', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Categorias Reais (Ground Truth)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Rotacionando labels se necess√°rio\n",
    "if len(max(common_classes, key=len)) > 10:\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== C√ÅLCULO DE M√âTRICAS DETALHADAS ====================\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS DETALHADAS DE CLASSIFICA√á√ÉO:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o completo\n",
    "classification_rep = classification_report(y_true_filtered, y_pred_filtered, \n",
    "                                        labels=common_classes, \n",
    "                                        target_names=common_classes,\n",
    "                                        output_dict=True)\n",
    "\n",
    "# M√©tricas globais\n",
    "accuracy = classification_rep['accuracy']\n",
    "macro_avg_precision = classification_rep['macro avg']['precision']\n",
    "macro_avg_recall = classification_rep['macro avg']['recall']\n",
    "macro_avg_f1 = classification_rep['macro avg']['f1-score']\n",
    "\n",
    "print(f\"üéØ M√âTRICAS GLOBAIS:\")\n",
    "print(f\"  ‚Ä¢ Accuracy (Acur√°cia): {accuracy:.4f}\")\n",
    "print(f\"  ‚Ä¢ Precision Macro Avg: {macro_avg_precision:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall Macro Avg: {macro_avg_recall:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score Macro Avg: {macro_avg_f1:.4f}\")\n",
    "\n",
    "# M√©tricas por classe\n",
    "print(f\"\\nüìã M√âTRICAS POR CATEGORIA:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Categoria':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for class_name in common_classes:\n",
    "    if class_name in classification_rep:\n",
    "        precision = classification_rep[class_name]['precision']\n",
    "        recall = classification_rep[class_name]['recall']\n",
    "        f1_score = classification_rep[class_name]['f1-score']\n",
    "        support = int(classification_rep[class_name]['support'])\n",
    "        \n",
    "        print(f\"{class_name:<20} {precision:<12.4f} {recall:<12.4f} {f1_score:<12.4f} {support:<12}\")\n",
    "\n",
    "# ==================== AN√ÅLISE DE PERFORMANCE ====================\n",
    "\n",
    "print(f\"\\nüîç AN√ÅLISE DE PERFORMANCE:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Identificando melhores e piores categorias\n",
    "class_f1_scores = {cls: classification_rep[cls]['f1-score'] \n",
    "                  for cls in common_classes if cls in classification_rep}\n",
    "\n",
    "if class_f1_scores:\n",
    "    best_class = max(class_f1_scores, key=class_f1_scores.get)\n",
    "    worst_class = min(class_f1_scores, key=class_f1_scores.get)\n",
    "    \n",
    "    print(f\"‚úÖ Melhor categoria: '{best_class}' (F1: {class_f1_scores[best_class]:.4f})\")\n",
    "    print(f\"‚ö†Ô∏è Pior categoria: '{worst_class}' (F1: {class_f1_scores[worst_class]:.4f})\")\n",
    "\n",
    "# Interpreta√ß√£o dos resultados\n",
    "if accuracy > 0.7:\n",
    "    print(f\"\\nüéâ Excelente alinhamento! Clusters bem definidos para as categorias\")\n",
    "elif accuracy > 0.5:\n",
    "    print(f\"\\n‚úÖ Bom alinhamento! Clusters moderadamente alinhados com categorias\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Alinhamento baixo. Clusters n√£o correspondem bem √†s categorias reais\")\n",
    "\n",
    "print(f\"\\nüí° INTERPRETA√á√ÉO:\")\n",
    "if average_purity > 70:\n",
    "    print(\"‚Ä¢ Alta pureza dos clusters sugere boa separa√ß√£o natural dos dados\")\n",
    "if macro_avg_f1 > 0.6:\n",
    "    print(\"‚Ä¢ F1-Score elevado indica clusters bem balanceados e informativos\")\n",
    "if len(common_classes) >= modelo_final.n_clusters * 0.7:\n",
    "    print(\"‚Ä¢ Boa cobertura das categorias pelos clusters gerados\")\n",
    "\n",
    "print(f\"\\n‚úÖ Matriz de confus√£o completa! Dados preparados para an√°lise ROC.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b52f5",
   "metadata": {},
   "source": [
    "## üìà 10. Avalia√ß√£o Avan√ßada - Curva ROC e AUC\n",
    "\n",
    "A **Curva ROC (Receiver Operating Characteristic)** e o **AUC (Area Under the Curve)** s√£o m√©tricas avan√ßadas que avaliam a capacidade do modelo de **separar diferentes categorias**. \n",
    "\n",
    "Implementamos a abordagem **One-vs-Rest (OvR)** para problemas multiclasse, onde cada categoria √© avaliada contra todas as outras, fornecendo uma an√°lise detalhada da **discriminabilidade** dos clusters gerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PREPARA√á√ÉO PARA AN√ÅLISE ROC MULTICLASSE ====================\n",
    "\n",
    "print(\"üìà Preparando an√°lise ROC para pseudo-classifica√ß√£o multiclasse...\")\n",
    "\n",
    "# Para an√°lise ROC, precisamos de probabilidades, n√£o apenas labels\n",
    "# Vamos usar a dist√¢ncia dos pontos aos centroides como \"probabilidade\"\n",
    "\n",
    "print(\"üîÑ Calculando 'probabilidades' baseadas em dist√¢ncias aos centroides...\")\n",
    "\n",
    "# Calculando dist√¢ncias de cada ponto a todos os centroides\n",
    "centroids = modelo_final.cluster_centers_\n",
    "n_samples = X_scaled.shape[0]\n",
    "n_clusters = len(centroids)\n",
    "\n",
    "# Matriz de dist√¢ncias (amostras x centroides)\n",
    "distances = np.zeros((n_samples, n_clusters))\n",
    "for i, centroid in enumerate(centroids):\n",
    "    distances[:, i] = np.linalg.norm(X_scaled - centroid, axis=1)\n",
    "\n",
    "# Convertendo dist√¢ncias em \"probabilidades\" (quanto menor a dist√¢ncia, maior a probabilidade)\n",
    "# Usando fun√ß√£o softmax invertida para converter dist√¢ncias em probabilidades\n",
    "def distances_to_probabilities(distances):\n",
    "    # Invertendo dist√¢ncias (dist√¢ncia menor = probabilidade maior)\n",
    "    inv_distances = 1 / (distances + 1e-10)  # Evitar divis√£o por zero\n",
    "    # Normalizando para somar 1 (como probabilidades)\n",
    "    probabilities = inv_distances / inv_distances.sum(axis=1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "cluster_probabilities = distances_to_probabilities(distances)\n",
    "\n",
    "print(f\"‚úÖ Probabilidades calculadas: shape {cluster_probabilities.shape}\")\n",
    "\n",
    "# ==================== PREPARA√á√ÉO DAS VARI√ÅVEIS PARA ROC ====================\n",
    "\n",
    "# Mapeando probabilidades de cluster para probabilidades de categoria\n",
    "print(\"üîó Mapeando probabilidades de clusters para categorias...\")\n",
    "\n",
    "# Criando mapeamento √∫nico e consistente\n",
    "unique_categories = sorted(common_classes)\n",
    "category_probabilities = np.zeros((len(y_true_filtered), len(unique_categories)))\n",
    "\n",
    "# Para cada categoria, somamos as probabilidades dos clusters que a representam\n",
    "for cat_idx, category in enumerate(unique_categories):\n",
    "    for cluster_id, cluster_category in cluster_to_category.items():\n",
    "        if cluster_category == category:\n",
    "            # Para amostras filtradas, precisamos mapear os √≠ndices corretamente\n",
    "            filtered_indices = mask[mask].index\n",
    "            for i, original_idx in enumerate(filtered_indices):\n",
    "                category_probabilities[i, cat_idx] += cluster_probabilities[original_idx, cluster_id]\n",
    "\n",
    "# Normalizando probabilidades por categoria\n",
    "for i in range(len(category_probabilities)):\n",
    "    total_prob = category_probabilities[i].sum()\n",
    "    if total_prob > 0:\n",
    "        category_probabilities[i] = category_probabilities[i] / total_prob\n",
    "\n",
    "print(f\"‚úÖ Probabilidades por categoria calculadas: {category_probabilities.shape}\")\n",
    "\n",
    "# ==================== BINARIZA√á√ÉO DAS VARI√ÅVEIS PARA ONE-VS-REST ====================\n",
    "\n",
    "print(\"üéØ Preparando vari√°veis para an√°lise One-vs-Rest...\")\n",
    "\n",
    "# Binarizando y_true para One-vs-Rest\n",
    "y_true_binarized = label_binarize(y_true_filtered, classes=unique_categories)\n",
    "\n",
    "# Se h√° apenas 2 classes, label_binarize retorna array 1D, precisa ser 2D\n",
    "if len(unique_categories) == 2:\n",
    "    y_true_binarized = np.column_stack((1 - y_true_binarized, y_true_binarized))\n",
    "\n",
    "print(f\"‚úÖ Vari√°veis binarizadas: {y_true_binarized.shape}\")\n",
    "\n",
    "# ==================== C√ÅLCULO DAS CURVAS ROC E AUC ====================\n",
    "\n",
    "print(\"üìä Calculando curvas ROC e AUC para cada categoria...\")\n",
    "\n",
    "# Armazenando resultados para cada categoria\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "# Calculando ROC para cada categoria (One-vs-Rest)\n",
    "for i, category in enumerate(unique_categories):\n",
    "    if i < y_true_binarized.shape[1]:\n",
    "        fpr[category], tpr[category], _ = roc_curve(y_true_binarized[:, i], \n",
    "                                                   category_probabilities[:, i])\n",
    "        roc_auc[category] = auc(fpr[category], tpr[category])\n",
    "\n",
    "print(f\"‚úÖ ROC calculado para {len(roc_auc)} categorias\")\n",
    "\n",
    "# ==================== VISUALIZA√á√ÉO DAS CURVAS ROC ====================\n",
    "\n",
    "# Configurando cores para visualiza√ß√£o\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', \n",
    "               'purple', 'brown', 'pink', 'gray', 'olive'])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plotando linha de refer√™ncia (classificador aleat√≥rio)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.8, label='Classificador Aleat√≥rio (AUC = 0.50)')\n",
    "\n",
    "# Plotando ROC para cada categoria\n",
    "for (category, color) in zip(unique_categories, colors):\n",
    "    if category in roc_auc:\n",
    "        plt.plot(fpr[category], tpr[category], color=color, lw=3,\n",
    "                label=f'{category} (AUC = {roc_auc[category]:.3f})')\n",
    "\n",
    "# Configura√ß√µes do gr√°fico\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (FPR)', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontweight='bold', fontsize=12)\n",
    "plt.title('üìà Curvas ROC - An√°lise One-vs-Rest\\nCapacidade de Discrimina√ß√£o dos Clusters', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== AN√ÅLISE DETALHADA DOS RESULTADOS AUC ====================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä AN√ÅLISE DETALHADA DOS RESULTADOS AUC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculando estat√≠sticas dos AUC scores\n",
    "auc_scores = list(roc_auc.values())\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "min_auc = np.min(auc_scores)\n",
    "max_auc = np.max(auc_scores)\n",
    "\n",
    "print(f\"üéØ ESTAT√çSTICAS GERAIS DOS AUC SCORES:\")\n",
    "print(f\"  ‚Ä¢ AUC M√©dio: {mean_auc:.4f}\")\n",
    "print(f\"  ‚Ä¢ Desvio Padr√£o: {std_auc:.4f}\")\n",
    "print(f\"  ‚Ä¢ AUC M√≠nimo: {min_auc:.4f}\")\n",
    "print(f\"  ‚Ä¢ AUC M√°ximo: {max_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nüìã AUC POR CATEGORIA:\")\n",
    "print(\"-\" * 50)\n",
    "sorted_categories = sorted(roc_auc.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (category, auc_score) in enumerate(sorted_categories, 1):\n",
    "    if auc_score > 0.8:\n",
    "        performance = \"üü¢ Excelente\"\n",
    "    elif auc_score > 0.7:\n",
    "        performance = \"üü° Boa\"\n",
    "    elif auc_score > 0.6:\n",
    "        performance = \"üü† Moderada\"\n",
    "    else:\n",
    "        performance = \"üî¥ Baixa\"\n",
    "    \n",
    "    print(f\"  {i:2d}. {category:<25} AUC: {auc_score:.4f} {performance}\")\n",
    "\n",
    "# ==================== INTERPRETA√á√ÉO DOS RESULTADOS ====================\n",
    "\n",
    "print(f\"\\nüí° INTERPRETA√á√ÉO DOS RESULTADOS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Classificando o desempenho geral\n",
    "if mean_auc > 0.8:\n",
    "    overall_performance = \"üéâ Excelente capacidade de discrimina√ß√£o!\"\n",
    "    interpretation = \"Os clusters demonstram separa√ß√£o clara entre as categorias.\"\n",
    "elif mean_auc > 0.7:\n",
    "    overall_performance = \"‚úÖ Boa capacidade de discrimina√ß√£o!\"\n",
    "    interpretation = \"Os clusters s√£o bem definidos e informativos.\"\n",
    "elif mean_auc > 0.6:\n",
    "    overall_performance = \"‚úÖ Capacidade moderada de discrimina√ß√£o.\"\n",
    "    interpretation = \"Os clusters t√™m valor informativo, mas com algumas sobreposi√ß√µes.\"\n",
    "else:\n",
    "    overall_performance = \"‚ö†Ô∏è Baixa capacidade de discrimina√ß√£o.\"\n",
    "    interpretation = \"Os clusters podem n√£o estar bem alinhados com as categorias reais.\"\n",
    "\n",
    "print(f\"üéØ {overall_performance}\")\n",
    "print(f\"üí¨ {interpretation}\")\n",
    "\n",
    "# Identificando categorias problem√°ticas\n",
    "problematic_categories = [cat for cat, auc_val in roc_auc.items() if auc_val < 0.6]\n",
    "if problematic_categories:\n",
    "    print(f\"\\n‚ö†Ô∏è Categorias com baixo AUC (<0.6): {len(problematic_categories)}\")\n",
    "    for cat in problematic_categories:\n",
    "        print(f\"  ‚Ä¢ {cat}: {roc_auc[cat]:.3f}\")\n",
    "    print(\"üí° Estas categorias podem precisar de an√°lise espec√≠fica adicional.\")\n",
    "\n",
    "# Recomenda√ß√µes finais\n",
    "print(f\"\\nüöÄ RECOMENDA√á√ïES:\")\n",
    "if len([auc for auc in auc_scores if auc > 0.7]) >= len(auc_scores) * 0.7:\n",
    "    print(\"‚úÖ O modelo de clustering est√° bem alinhado com as categorias naturais dos dados\")\n",
    "    print(\"‚úÖ Os clusters gerados s√£o informativos para segmenta√ß√£o de neg√≥cio\")\n",
    "else:\n",
    "    print(\"üí° Considere ajustar o n√∫mero de clusters ou revisar features utilizadas\")\n",
    "    print(\"üí° Algumas categorias podem se beneficiar de an√°lise espec√≠fica adicional\")\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lise ROC conclu√≠da! Modelo totalmente avaliado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eea134",
   "metadata": {},
   "source": [
    "## üèÜ 11. Compara√ß√£o e An√°lise Final dos Resultados\n",
    "\n",
    "Esta se√ß√£o consolida **todos os resultados obtidos** ao longo da an√°lise, fornecendo uma **compara√ß√£o sistem√°tica** entre as diferentes abordagens e m√©tricas utilizadas. √â o **resumo executivo** do modelo otimizado.\n",
    "\n",
    "### üìä **M√©tricas analisadas:**\n",
    "- **Grid Search**: Otimiza√ß√£o de hiperpar√¢metros\n",
    "- **Elbow Method**: Valida√ß√£o visual do n√∫mero de clusters\n",
    "- **PyCaret**: Valida√ß√£o independente com AutoML\n",
    "- **Silhouette Score**: Qualidade da clusteriza√ß√£o\n",
    "- **Matriz de Confus√£o**: Alinhamento com ground truth\n",
    "- **Curva ROC/AUC**: Capacidade de discrimina√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas de avalia√ß√£o do modelo\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confus√£o')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.show()\n",
    "\n",
    "# Acur√°cia\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Acur√°cia do modelo: {acc*100:.2f}%\")\n",
    "if acc == 1.0:\n",
    "    print(\"Aten√ß√£o: A acur√°cia est√° em 100%. Isso pode indicar problemas de sobreajuste ou mapeamento incorreto dos clusters.\")\n",
    "\n",
    "# Curva ROC (apenas se houver mais de uma classe)\n",
    "if len(set(y_true)) > 1 and len(set(y_pred)) > 1:\n",
    "    try:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=list(set(y_true))[0])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Taxa de Falsos Positivos')\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "        plt.title('Curva ROC')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"N√£o foi poss√≠vel calcular a curva ROC: {e}\")\n",
    "else:\n",
    "    print(\"Curva ROC n√£o pode ser exibida pois h√° apenas uma classe.\")\n",
    "\n",
    "# An√°lise final\n",
    "if acc < 0.8:\n",
    "    print(\"A acur√°cia est√° abaixo de 80%. Considere revisar o mapeamento dos clusters ou ajustar os hiperpar√¢metros do modelo.\")\n",
    "else:\n",
    "    print(\"A acur√°cia est√° satisfat√≥ria, mas continue validando o modelo com outros m√©todos e dados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
